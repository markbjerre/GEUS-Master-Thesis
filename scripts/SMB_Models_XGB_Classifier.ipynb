{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Mass Balance: XGB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTbsuMgO3ery"
   },
   "source": [
    "Import relevant Libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyarrow) (1.22.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (5.13.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from plotly) (8.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: feature_engine in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (1.2.1)\n",
      "Requirement already satisfied: pandas>=1.0.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (1.3.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (1.5.4)\n",
      "Requirement already satisfied: statsmodels>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (0.13.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas>=1.0.3->feature_engine) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas>=1.0.3->feature_engine) (2020.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn>=1.0.0->feature_engine) (2.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from statsmodels>=0.11.1->feature_engine) (23.0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ERROR: unknown command \"update\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow\n",
    "%pip install plotly\n",
    "%pip install feature_engine\n",
    "%pip update pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yJLqdDNx3j-Y"
   },
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math as math\n",
    "import datetime\n",
    "from scipy import stats\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set WD\n",
    "import os\n",
    "from pyrsistent import v\n",
    "os.getcwd()\n",
    "#os.chdir('/Users/asgerlyngeholst-hansen/Desktop/GEUS-Master-Thesis/')\n",
    "#os.chdir('C:\\\\Users\\\\nifu18ab\\\\Desktop\\\\GEUS-Master-Thesis')\n",
    "#os.chdir('/Users/nilsfulde/Desktop/GEUS-Master-Thesis')\n",
    "os.chdir('/Users/nilsfulde/Desktop/Master_Thesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load KM_L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/promice/preprocessed/daily/SCO_L.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA values in Melting season as blank\n",
    "df['Melting Season'] = df['Melting Season'].fillna('No Ablation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all NA values if training should be done only on ablation\n",
    "#df = df.dropna()\n",
    "\n",
    "# Remove only values from Wind direction if training should be done on entire dataset\n",
    "df = df[df['Wind from direction (upper boom)'].notna()] \n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit retrieve the indexes for all melting season categories and remove column from df\n",
    "for category in ['pre', 'beginning', 'middle', 'end']:\n",
    "    exec(f\"indices_{category} = df[df['Melting Season'] == '{category}'].index\")\n",
    "\n",
    "# This bit retrieve the indexes for Ablation periods\n",
    "for category in [True]:\n",
    "    exec(f\"indices_ablation = df[df['Ablation'] == {category}].index\")\n",
    "\n",
    "indices_all = df[df['Ablation'].isin([True, False])].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "# Creating a instance of label Encoder.\n",
    "le = LabelEncoder()\n",
    " \n",
    "# Using .fit_transform function to fit label\n",
    "# encoder and return encoded label\n",
    "df['Melting Season'] = le.fit_transform(df['Melting Season'])\n",
    "label_dict = dict(zip(le.classes_, le.transform(le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to include only relevant features\n",
    "exclude_list = [                                                    'index', # excluded because of unimportant information\n",
    "                                                                     'stid', # excluded because of unimportant information\n",
    "                                                               'Unnamed: 0', # Old Index\n",
    "                                                                 'Ablation',\n",
    "                                                          # 'Melting Season', # We drop melting season later \n",
    "                                              'year', 'month','day', 'hour', # Drop because of sin & cos values\n",
    "                                                                 'subgroup',\n",
    "                'Surface height from combined measurements',\n",
    "                'Surface height from combined measurements DELTA'\n",
    "               ]\n",
    "\n",
    "df = df[[column for column in df.columns if column not in exclude_list]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to change the datetime variables into a form that a ML models can understand\n",
    "  # https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "  # https://feature-engine.trainindata.com/en/1.3.x/user_guide/creation/CyclicalFeatures.html\n",
    "    \n",
    "    \n",
    "# This bit to split Datetime column into year, month, day, hour columns\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "df['Datetime'] = df['Datetime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "#Create new columns\n",
    "#df['year'] = df['Datetime'].dt.year \n",
    "df['month'] = df['Datetime'].dt.month\n",
    "#df['day'] = df['Datetime'].dt.day\n",
    "#df['hour'] = df['Datetime'].dt.hour\n",
    "\n",
    "#Drop the datetime column\n",
    "df.drop(columns=['Datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to change the datetime variables into a form that a ML models can understand (continued)\n",
    "  # https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "  # https://feature-engine.trainindata.com/en/1.3.x/user_guide/creation/CyclicalFeatures.html \n",
    "\n",
    "# Use CyclicalFeatures Package to convert year, month, day & hour\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "\n",
    "cyclical = CyclicalFeatures(variables=None, drop_original=True)\n",
    "\n",
    "# with year\n",
    "#cyclical = cyclical.fit_transform(df[['month', 'year', 'Wind from direction (upper boom)']]) \n",
    "\n",
    "# without year\n",
    "cyclical = cyclical.fit_transform(df[['month', 'Wind from direction (upper boom)']]) \n",
    "df = pd.merge(df, cyclical, left_index=True, right_index=True)\n",
    "\n",
    "df.drop(columns=['month', 'Wind from direction (upper boom)'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to convert all numerical values into the same scale \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler()]\n",
    "\n",
    "# with year\n",
    "#num_cols = df.select_dtypes(include=['int64','float64']).drop(['Surface height from combined measurements', 'Surface height from combined measurements DELTA','month_sin','month_cos','year_sin','year_cos','Wind from direction (upper boom)_sin', 'Wind from direction (upper boom)_cos', 'Melting Season'], axis=1).columns #select numerical columns except y and cyclical feature\n",
    "\n",
    "# without year\n",
    "num_cols = df.select_dtypes(include=['int64','float64']).drop(['month_sin','month_cos','Wind from direction (upper boom)_sin', 'Wind from direction (upper boom)_cos', 'Melting Season'], axis=1).columns #select numerical columns except y and cyclical feature\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#fit and transform numerical columns\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to split into Train & Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# split the data into training and testing sets while maintaining the subgroup distribution\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(df, df['Melting Season']):\n",
    "    train = df.loc[train_index]\n",
    "    test = df.loc[test_index]\n",
    "\n",
    "# separate the features and target variables for the train and test sets\n",
    "X_train = train.drop(['Melting Season'], axis=1)\n",
    "y_train = train['Melting Season']\n",
    "X_test = test.drop(['Melting Season'], axis=1)\n",
    "y_test = test['Melting Season']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'gamma': 0, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100}\n",
      "Accuracy score on test set:  0.93519882179676\n",
      "F1 score on test set:  0.9340593798367753\n",
      "Confusion matrix on test set: \n",
      "             No Ablation  beginning  end  middle  pre\n",
      "No Ablation          524          0    4       8    1\n",
      "beginning              0         35    0      21    0\n",
      "end                   15          0   35       8    0\n",
      "middle                 1          5    7     244    0\n",
      "pre                    2         10    0       6  432\n"
     ]
    }
   ],
   "source": [
    "# This bit to find best XBG Regressor model for when Ablation == True and for both target variables\n",
    "\n",
    "indices = eval(f\"indices_all\")  # Get the indices for the current category\n",
    "\n",
    "# Find common indexes\n",
    "X_Train_valid_indices = indices.intersection(X_train.index)\n",
    "y_Train_valid_indices = indices.intersection(y_train.index)\n",
    "X_Test_valid_indices = indices.intersection(X_test.index)\n",
    "y_Test_valid_indices = indices.intersection(y_test.index)\n",
    "\n",
    "# filter out by indeces\n",
    "X_train_category = X_train.loc[X_Train_valid_indices]  \n",
    "y_train_category = y_train.loc[y_Train_valid_indices] \n",
    "X_test_category = X_test.loc[X_Test_valid_indices]  \n",
    "y_test_category = y_test.loc[y_Test_valid_indices]\n",
    "\n",
    "# Define the XGBClassifier model\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=5)\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3], #[3, 4, 5],\n",
    "    'learning_rate': [0.01], #[0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100], #[100, 200, 500],\n",
    "    'min_child_weight': [1], #[1, 3, 5],\n",
    "    'gamma': [0], #[0, 0.1, 0.3],\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid = param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train_category, y_train_category)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score on the test set\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "y_pred_category = grid_search.predict(X_test_category)\n",
    "print(\"Accuracy score on test set: \", accuracy_score(y_test_category, y_pred_category))\n",
    "print(\"F1 score on test set: \", f1_score(y_test_category, y_pred_category, average='weighted'))\n",
    "\n",
    "labels = [k for k, v in sorted(label_dict.items(), key=lambda item: item[1])]\n",
    "cm = confusion_matrix(y_test_category, y_pred_category)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(\"Confusion matrix on test set: \")\n",
    "print(cm_df)\n",
    "\n",
    "\n",
    "# Save the best model parameters as 'model' variable\n",
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air pressure (upper boom)</th>\n",
       "      <th>Air temperature (upper boom)</th>\n",
       "      <th>Relative humidity (upper boom) - corrected</th>\n",
       "      <th>Specific humidity (upper boom)</th>\n",
       "      <th>Wind speed (upper boom)</th>\n",
       "      <th>Downwelling shortwave radiation - corrected</th>\n",
       "      <th>Upwelling shortwave radiation - corrected</th>\n",
       "      <th>Downwelling longwave radiation</th>\n",
       "      <th>Upwelling longwave radiation</th>\n",
       "      <th>Surface temperature</th>\n",
       "      <th>Latent heat flux (upper boom)</th>\n",
       "      <th>Sensible heat flux (upper boom)</th>\n",
       "      <th>Albedo</th>\n",
       "      <th>Cloud cover</th>\n",
       "      <th>Melting Season</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>Wind from direction (upper boom)_sin</th>\n",
       "      <th>Wind from direction (upper boom)_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.487301</td>\n",
       "      <td>0.823685</td>\n",
       "      <td>0.632328</td>\n",
       "      <td>0.679327</td>\n",
       "      <td>0.189520</td>\n",
       "      <td>0.606533</td>\n",
       "      <td>0.243259</td>\n",
       "      <td>0.747067</td>\n",
       "      <td>0.938960</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.791036</td>\n",
       "      <td>0.276460</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>0.583879</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.848078e-01</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>0.998923</td>\n",
       "      <td>-0.046390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.467779</td>\n",
       "      <td>0.885606</td>\n",
       "      <td>0.553944</td>\n",
       "      <td>0.701633</td>\n",
       "      <td>0.229952</td>\n",
       "      <td>0.529323</td>\n",
       "      <td>0.200665</td>\n",
       "      <td>0.778293</td>\n",
       "      <td>0.955979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.782539</td>\n",
       "      <td>0.298192</td>\n",
       "      <td>0.047677</td>\n",
       "      <td>0.536027</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.848078e-01</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>0.996813</td>\n",
       "      <td>0.079771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.548448</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>0.727032</td>\n",
       "      <td>0.729125</td>\n",
       "      <td>0.188192</td>\n",
       "      <td>0.453813</td>\n",
       "      <td>0.170827</td>\n",
       "      <td>0.835722</td>\n",
       "      <td>0.945571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.795939</td>\n",
       "      <td>0.238237</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.760744</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.848078e-01</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.044519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.478614</td>\n",
       "      <td>0.870245</td>\n",
       "      <td>0.524048</td>\n",
       "      <td>0.657856</td>\n",
       "      <td>0.335451</td>\n",
       "      <td>0.723119</td>\n",
       "      <td>0.287402</td>\n",
       "      <td>0.630888</td>\n",
       "      <td>0.943749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744059</td>\n",
       "      <td>0.428781</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.253325</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.848078e-01</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>0.898254</td>\n",
       "      <td>0.439478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.466346</td>\n",
       "      <td>0.875845</td>\n",
       "      <td>0.523715</td>\n",
       "      <td>0.669650</td>\n",
       "      <td>0.361645</td>\n",
       "      <td>0.676684</td>\n",
       "      <td>0.267868</td>\n",
       "      <td>0.649181</td>\n",
       "      <td>0.946179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754016</td>\n",
       "      <td>0.461043</td>\n",
       "      <td>0.039957</td>\n",
       "      <td>0.281933</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.848078e-01</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>0.872807</td>\n",
       "      <td>0.488066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>0.612372</td>\n",
       "      <td>0.871126</td>\n",
       "      <td>0.695471</td>\n",
       "      <td>0.786688</td>\n",
       "      <td>0.437389</td>\n",
       "      <td>0.287736</td>\n",
       "      <td>0.131252</td>\n",
       "      <td>0.723144</td>\n",
       "      <td>0.950593</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>0.887510</td>\n",
       "      <td>0.566564</td>\n",
       "      <td>0.127509</td>\n",
       "      <td>0.447167</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.349015</td>\n",
       "      <td>0.937117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>0.577283</td>\n",
       "      <td>0.860826</td>\n",
       "      <td>0.736479</td>\n",
       "      <td>0.798206</td>\n",
       "      <td>0.209268</td>\n",
       "      <td>0.139421</td>\n",
       "      <td>0.069374</td>\n",
       "      <td>0.703632</td>\n",
       "      <td>0.967297</td>\n",
       "      <td>0.998435</td>\n",
       "      <td>0.821008</td>\n",
       "      <td>0.357253</td>\n",
       "      <td>0.288320</td>\n",
       "      <td>0.425844</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606503</td>\n",
       "      <td>0.795081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>0.623102</td>\n",
       "      <td>0.790213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832806</td>\n",
       "      <td>0.123590</td>\n",
       "      <td>0.031926</td>\n",
       "      <td>0.017441</td>\n",
       "      <td>0.901885</td>\n",
       "      <td>0.960020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.817613</td>\n",
       "      <td>0.191004</td>\n",
       "      <td>0.394501</td>\n",
       "      <td>0.945221</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765355</td>\n",
       "      <td>0.643609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>0.689016</td>\n",
       "      <td>0.817874</td>\n",
       "      <td>0.632661</td>\n",
       "      <td>0.636314</td>\n",
       "      <td>0.300612</td>\n",
       "      <td>0.168881</td>\n",
       "      <td>0.116935</td>\n",
       "      <td>0.750266</td>\n",
       "      <td>0.959211</td>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.696706</td>\n",
       "      <td>0.337182</td>\n",
       "      <td>0.363809</td>\n",
       "      <td>0.600385</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354584</td>\n",
       "      <td>0.935024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.732703</td>\n",
       "      <td>0.796753</td>\n",
       "      <td>0.517833</td>\n",
       "      <td>0.534472</td>\n",
       "      <td>0.372173</td>\n",
       "      <td>0.224406</td>\n",
       "      <td>0.157228</td>\n",
       "      <td>0.630082</td>\n",
       "      <td>0.928392</td>\n",
       "      <td>0.995234</td>\n",
       "      <td>0.603290</td>\n",
       "      <td>0.358189</td>\n",
       "      <td>0.350772</td>\n",
       "      <td>0.386829</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.389989</td>\n",
       "      <td>0.920820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>855 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Air pressure (upper boom)  Air temperature (upper boom)  \\\n",
       "0                      0.487301                      0.823685   \n",
       "1                      0.467779                      0.885606   \n",
       "2                      0.548448                      0.828361   \n",
       "3                      0.478614                      0.870245   \n",
       "4                      0.466346                      0.875845   \n",
       "...                         ...                           ...   \n",
       "2044                   0.612372                      0.871126   \n",
       "2045                   0.577283                      0.860826   \n",
       "2046                   0.623102                      0.790213   \n",
       "2047                   0.689016                      0.817874   \n",
       "2048                   0.732703                      0.796753   \n",
       "\n",
       "      Relative humidity (upper boom) - corrected  \\\n",
       "0                                       0.632328   \n",
       "1                                       0.553944   \n",
       "2                                       0.727032   \n",
       "3                                       0.524048   \n",
       "4                                       0.523715   \n",
       "...                                          ...   \n",
       "2044                                    0.695471   \n",
       "2045                                    0.736479   \n",
       "2046                                    1.000000   \n",
       "2047                                    0.632661   \n",
       "2048                                    0.517833   \n",
       "\n",
       "      Specific humidity (upper boom)  Wind speed (upper boom)  \\\n",
       "0                           0.679327                 0.189520   \n",
       "1                           0.701633                 0.229952   \n",
       "2                           0.729125                 0.188192   \n",
       "3                           0.657856                 0.335451   \n",
       "4                           0.669650                 0.361645   \n",
       "...                              ...                      ...   \n",
       "2044                        0.786688                 0.437389   \n",
       "2045                        0.798206                 0.209268   \n",
       "2046                        0.832806                 0.123590   \n",
       "2047                        0.636314                 0.300612   \n",
       "2048                        0.534472                 0.372173   \n",
       "\n",
       "      Downwelling shortwave radiation - corrected  \\\n",
       "0                                        0.606533   \n",
       "1                                        0.529323   \n",
       "2                                        0.453813   \n",
       "3                                        0.723119   \n",
       "4                                        0.676684   \n",
       "...                                           ...   \n",
       "2044                                     0.287736   \n",
       "2045                                     0.139421   \n",
       "2046                                     0.031926   \n",
       "2047                                     0.168881   \n",
       "2048                                     0.224406   \n",
       "\n",
       "      Upwelling shortwave radiation - corrected  \\\n",
       "0                                      0.243259   \n",
       "1                                      0.200665   \n",
       "2                                      0.170827   \n",
       "3                                      0.287402   \n",
       "4                                      0.267868   \n",
       "...                                         ...   \n",
       "2044                                   0.131252   \n",
       "2045                                   0.069374   \n",
       "2046                                   0.017441   \n",
       "2047                                   0.116935   \n",
       "2048                                   0.157228   \n",
       "\n",
       "      Downwelling longwave radiation  Upwelling longwave radiation  \\\n",
       "0                           0.747067                      0.938960   \n",
       "1                           0.778293                      0.955979   \n",
       "2                           0.835722                      0.945571   \n",
       "3                           0.630888                      0.943749   \n",
       "4                           0.649181                      0.946179   \n",
       "...                              ...                           ...   \n",
       "2044                        0.723144                      0.950593   \n",
       "2045                        0.703632                      0.967297   \n",
       "2046                        0.901885                      0.960020   \n",
       "2047                        0.750266                      0.959211   \n",
       "2048                        0.630082                      0.928392   \n",
       "\n",
       "      Surface temperature  Latent heat flux (upper boom)  \\\n",
       "0                0.998389                       0.791036   \n",
       "1                1.000000                       0.782539   \n",
       "2                1.000000                       0.795939   \n",
       "3                1.000000                       0.744059   \n",
       "4                1.000000                       0.754016   \n",
       "...                   ...                            ...   \n",
       "2044             0.999937                       0.887510   \n",
       "2045             0.998435                       0.821008   \n",
       "2046             1.000000                       0.817613   \n",
       "2047             0.999794                       0.696706   \n",
       "2048             0.995234                       0.603290   \n",
       "\n",
       "      Sensible heat flux (upper boom)    Albedo  Cloud cover  Melting Season  \\\n",
       "0                            0.276460  0.058874     0.583879               3   \n",
       "1                            0.298192  0.047677     0.536027               3   \n",
       "2                            0.238237  0.050251     0.760744               3   \n",
       "3                            0.428781  0.045781     0.253325               3   \n",
       "4                            0.461043  0.039957     0.281933               3   \n",
       "...                               ...       ...          ...             ...   \n",
       "2044                         0.566564  0.127509     0.447167               3   \n",
       "2045                         0.357253  0.288320     0.425844               3   \n",
       "2046                         0.191004  0.394501     0.945221               3   \n",
       "2047                         0.337182  0.363809     0.600385               3   \n",
       "2048                         0.358189  0.350772     0.386829               3   \n",
       "\n",
       "         month_sin  month_cos  Wind from direction (upper boom)_sin  \\\n",
       "0    -9.848078e-01   0.173648                              0.998923   \n",
       "1    -9.848078e-01   0.173648                              0.996813   \n",
       "2    -9.848078e-01   0.173648                              0.999009   \n",
       "3    -9.848078e-01   0.173648                              0.898254   \n",
       "4    -9.848078e-01   0.173648                              0.872807   \n",
       "...            ...        ...                                   ...   \n",
       "2044 -2.449294e-16   1.000000                              0.349015   \n",
       "2045 -2.449294e-16   1.000000                              0.606503   \n",
       "2046 -2.449294e-16   1.000000                              0.765355   \n",
       "2047 -2.449294e-16   1.000000                              0.354584   \n",
       "2048 -2.449294e-16   1.000000                              0.389989   \n",
       "\n",
       "      Wind from direction (upper boom)_cos  \n",
       "0                                -0.046390  \n",
       "1                                 0.079771  \n",
       "2                                 0.044519  \n",
       "3                                 0.439478  \n",
       "4                                 0.488066  \n",
       "...                                    ...  \n",
       "2044                              0.937117  \n",
       "2045                              0.795081  \n",
       "2046                              0.643609  \n",
       "2047                              0.935024  \n",
       "2048                              0.920820  \n",
       "\n",
       "[855 rows x 19 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Melting Season'] == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Melting Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0], got [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m y_test_category \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mloc[y_Test_valid_indices]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Apply XGB on all periods  \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_category\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_category\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit a new model on each subset\u001b[39;00m\n\u001b[1;32m     18\u001b[0m y_pred_category \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_category)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy score on test set: \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test_category, y_pred_category))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1435\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1438\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m   1439\u001b[0m ):\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1443\u001b[0m     )\n\u001b[1;32m   1445\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0], got [4]"
     ]
    }
   ],
   "source": [
    "for category in ['pre', 'beginning', 'middle', 'end']:\n",
    "    indices = eval(f\"indices_{category}\")  # Get the indices for the current category\n",
    "\n",
    "    # Find common indexes\n",
    "    X_Train_valid_indices = indices.intersection(X_train.index)\n",
    "    y_Train_valid_indices = indices.intersection(y_train.index)\n",
    "    X_Test_valid_indices = indices.intersection(X_test.index)\n",
    "    y_Test_valid_indices = indices.intersection(y_test.index)\n",
    "\n",
    "    # filter out by indeces\n",
    "    X_train_category = X_train.loc[X_Train_valid_indices]\n",
    "    y_train_category = y_train.loc[y_Train_valid_indices]\n",
    "    X_test_category = X_test.loc[X_Test_valid_indices]\n",
    "    y_test_category = y_test.loc[y_Test_valid_indices]\n",
    "\n",
    "    # Apply XGB on all periods  \n",
    "    model.fit(X_train_category, y_train_category)  # Fit a new model on each subset\n",
    "    y_pred_category = model.predict(X_test_category)\n",
    "\n",
    "    print(\"Accuracy score on test set: \", accuracy_score(y_test_category, y_pred_category))\n",
    "    print(\"F1 score on test set: \", f1_score(y_test_category, y_pred_category, average='weighted'))\n",
    "\n",
    "    labels = [k for k, v in sorted(label_dict.items(), key=lambda item: item[1])]\n",
    "    cm = confusion_matrix(y_test_category, y_pred_category)\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    print(\"Confusion matrix on test set: \")\n",
    "    print(cm_df)\n",
    "\n",
    "    # Get the feature importance scores and plot them\n",
    "    feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "    print(feature_importance)\n",
    "\n",
    "    # Plot the feature importance scores\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Feature Importance ({category})')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score on test set (delta):  -0.10340557736615308\n",
      "MSE on test set (delta):  0.0038937158256251167\n",
      "R^2 score on test set (absolute):  0.013685247670754519\n",
      "MSE on test set (absolute):  157.86584193346172\n"
     ]
    }
   ],
   "source": [
    "# This bit to find best LSTM model for when Ablation == True and for both target variables\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "for i in [\"delta\",\"absolute\"]:\n",
    "    \n",
    "    indices = eval(f\"indices_ablation\")  # Get the indices for the current category\n",
    "\n",
    "    # Find common indexes\n",
    "    X_Train_valid_indices = indices.intersection(eval(f\"X_{i}_train\").index)\n",
    "    y_Train_valid_indices = indices.intersection(eval(f\"y_{i}_train\").index)\n",
    "    X_Test_valid_indices = indices.intersection(eval(f\"X_{i}_test\").index)\n",
    "    y_Test_valid_indices = indices.intersection(eval(f\"y_{i}_test\").index)\n",
    "\n",
    "    # filter out by indeces\n",
    "    X_train_category = eval(f\"X_{i}_train\").loc[X_Train_valid_indices]  \n",
    "    y_train_category = eval(f\"y_{i}_train\").loc[y_Train_valid_indices] \n",
    "    X_test_category = eval(f\"X_{i}_test\").loc[X_Test_valid_indices]  \n",
    "    y_test_category = eval(f\"y_{i}_test\").loc[y_Test_valid_indices]\n",
    "    \n",
    "    # Impute missing values\n",
    "    X_train_category = X_train_category.fillna(X_train_category.mean())\n",
    "    y_train_category = y_train_category.fillna(y_train_category.mean())\n",
    "    X_test_category = X_test_category.fillna(X_test_category.mean())\n",
    "\n",
    "    \n",
    "    # Reshape the input data to 3D array\n",
    "    X_train_category = X_train_category.values.reshape((X_train_category.shape[0], 1, X_train_category.shape[1]))\n",
    "    X_test_category = X_test_category.values.reshape((X_test_category.shape[0], 1, X_test_category.shape[1]))\n",
    "    \n",
    "    # Define the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(1, X_train_category.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train_category, y_train_category, epochs=50, batch_size=64, validation_data=(X_test_category, y_test_category), verbose=0)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_category = model.predict(X_test_category)\n",
    "    r2 = r2_score(y_test_category, y_pred_category)\n",
    "    mse = mean_squared_error(y_test_category, y_pred_category)\n",
    "    print(f\"R^2 score on test set ({i}): \", r2)\n",
    "    print(f\"MSE on test set ({i}): \", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
