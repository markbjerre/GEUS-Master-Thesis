{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Mass Balance: XGB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTbsuMgO3ery"
   },
   "source": [
    "Import relevant Libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyarrow) (1.22.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (5.13.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from plotly) (8.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: feature_engine in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (1.2.1)\n",
      "Requirement already satisfied: pandas>=1.0.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (1.3.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (1.5.4)\n",
      "Requirement already satisfied: statsmodels>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from feature_engine) (0.13.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas>=1.0.3->feature_engine) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas>=1.0.3->feature_engine) (2020.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn>=1.0.0->feature_engine) (2.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from statsmodels>=0.11.1->feature_engine) (23.0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ERROR: unknown command \"update\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow\n",
    "%pip install plotly\n",
    "%pip install feature_engine\n",
    "%pip update pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "id": "yJLqdDNx3j-Y"
   },
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math as math\n",
    "import datetime\n",
    "from scipy import stats\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set WD\n",
    "import os\n",
    "from pyrsistent import v\n",
    "os.getcwd()\n",
    "#os.chdir('/Users/asgerlyngeholst-hansen/Desktop/GEUS-Master-Thesis/')\n",
    "#os.chdir('C:\\\\Users\\\\nifu18ab\\\\Desktop\\\\GEUS-Master-Thesis')\n",
    "#os.chdir('/Users/nilsfulde/Desktop/GEUS-Master-Thesis')\n",
    "os.chdir('/Users/nilsfulde/Desktop/Master_Thesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load KM_L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/promice/preprocessed/daily/KAN_L.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing (0.1: Change months to cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to change the datetime variables into a form that a ML models can understand\n",
    "  # https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "  # https://feature-engine.trainindata.com/en/1.3.x/user_guide/creation/CyclicalFeatures.html\n",
    "    \n",
    "    \n",
    "# This bit to split Datetime column into year, month, day, hour columns\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "df['Datetime'] = df['Datetime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "#Create new columns\n",
    "#df['year'] = df['Datetime'].dt.year \n",
    "df['month'] = df['Datetime'].dt.month\n",
    "#df['day'] = df['Datetime'].dt.day\n",
    "#df['hour'] = df['Datetime'].dt.hour\n",
    "\n",
    "#Drop the datetime column\n",
    "df.drop(columns=['Datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to change the datetime variables into a form that a ML models can understand (continued)\n",
    "  # https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "  # https://feature-engine.trainindata.com/en/1.3.x/user_guide/creation/CyclicalFeatures.html \n",
    "\n",
    "# Use CyclicalFeatures Package to convert year, month, day & hour\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "\n",
    "cyclical = CyclicalFeatures(variables=None, drop_original=True)\n",
    "\n",
    "# with year\n",
    "#cyclical = cyclical.fit_transform(df[['month', 'year', 'Wind from direction (upper boom)']]) \n",
    "\n",
    "# Remove only values from Wind direction because CyclicalFeatures does not work with NAN\n",
    "df = df[df['Wind from direction (upper boom)'].notna()] \n",
    "\n",
    "# without year\n",
    "#cyclical = cyclical.fit_transform(df[['month', 'Wind from direction (upper boom)']]) \n",
    "cyclical = cyclical.fit_transform(df[['Wind from direction (upper boom)']]) \n",
    "df = pd.merge(df, cyclical, left_index=True, right_index=True)\n",
    "\n",
    "df.drop(columns=['month', 'Wind from direction (upper boom)'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing (1.0: Filter relevant periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA values in Melting season as blank\n",
    "df['Melting Season'] = df['Melting Season'].fillna('no_ablation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only Ablation Period Or Melting Season\n",
    "#df = df[df['Ablation'] == True]\n",
    "df =  df[df['Melting Season'].isin(['pre', 'beginning', 'middle', 'end'])]\n",
    "\n",
    "#Remove all NA values if training should be done only on ablation\n",
    "#df = df.dropna()\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit retrieve the indexes for all melting season categories and remove column from df\n",
    "for category in ['no_ablation', 'pre', 'beginning', 'middle', 'end']:\n",
    "    exec(f\"indices_{category} = df[df['Melting Season'] == '{category}'].index\")\n",
    "\n",
    "# This bit retrieve the indexes for Ablation periods\n",
    "for category in [True]:\n",
    "    exec(f\"indices_ablation = df[df['Ablation'] == {category}].index\")\n",
    "\n",
    "indices_all = df[df['Ablation'].isin([True, False])].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing (2.0: Formatting and transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "# Creating a instance of label Encoder.\n",
    "le = LabelEncoder()\n",
    " \n",
    "# Using .fit_transform function to fit label\n",
    "# encoder and return encoded label\n",
    "df['Melting Season'] = le.fit_transform(df['Melting Season'])\n",
    "label_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "class_order = ['no_ablation', 'pre', 'beginning', 'middle', 'end']\n",
    "\n",
    "# Sort the label_dict based on the desired order of class names\n",
    "label_dict = {class_name: label_dict[class_name] for class_name in class_order if class_name in label_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to include only relevant features\n",
    "exclude_list = [                                                    'index', # excluded because of unimportant information\n",
    "                                                                     'stid', # excluded because of unimportant information\n",
    "                                                               'Unnamed: 0', # Old Index\n",
    "                                                                 'Ablation',\n",
    "                                                          # 'Melting Season', # We drop melting season later \n",
    "                                              'year', 'month','day', 'hour', # Drop because of sin & cos values\n",
    "                                                                 'subgroup',\n",
    "                'Surface height from combined measurements',\n",
    "                'Surface height from combined measurements DELTA'\n",
    "               ]\n",
    "\n",
    "df = df[[column for column in df.columns if column not in exclude_list]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to convert all numerical values into the same scale \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler()]\n",
    "\n",
    "# with year\n",
    "#num_cols = df.select_dtypes(include=['int64','float64']).drop(['Surface height from combined measurements', 'Surface height from combined measurements DELTA','month_sin','month_cos','year_sin','year_cos','Wind from direction (upper boom)_sin', 'Wind from direction (upper boom)_cos', 'Melting Season'], axis=1).columns #select numerical columns except y and cyclical feature\n",
    "\n",
    "# without year\n",
    "#num_cols = df.select_dtypes(include=['int64','float64']).drop(['month_sin','month_cos','Wind from direction (upper boom)_sin', 'Wind from direction (upper boom)_cos', 'Melting Season'], axis=1).columns #select numerical columns except y and cyclical feature\n",
    "num_cols = df.select_dtypes(include=['int64','float64']).drop(['Wind from direction (upper boom)_sin', 'Wind from direction (upper boom)_cos', 'Melting Season'], axis=1).columns #select numerical columns except y and cyclical feature\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#fit and transform numerical columns\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to split into Train & Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# split the data into training and testing sets while maintaining the subgroup distribution\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(df, df['Melting Season']):\n",
    "    train = df.loc[train_index]\n",
    "    test = df.loc[test_index]\n",
    "\n",
    "# separate the features and target variables for the train and test sets\n",
    "X_train = train.drop(['Melting Season'], axis=1)\n",
    "y_train = train['Melting Season']\n",
    "X_test = test.drop(['Melting Season'], axis=1)\n",
    "y_test = test['Melting Season']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [362]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid \u001b[38;5;241m=\u001b[39m param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Fit the grid search object to the training data\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters and the corresponding accuracy score on the test set\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py:434\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This bit to find best XBG Regressor model for when Ablation == True and for both target variables\n",
    "\n",
    "indices = eval(f\"indices_all\")  # Get the indices for the current category\n",
    "\n",
    "# Find common indexes\n",
    "X_Train_valid_indices = indices.intersection(X_train.index)\n",
    "y_Train_valid_indices = indices.intersection(y_train.index)\n",
    "X_Test_valid_indices = indices.intersection(X_test.index)\n",
    "y_Test_valid_indices = indices.intersection(y_test.index)\n",
    "\n",
    "# filter out by indeces\n",
    "X_train = X_train.loc[X_Train_valid_indices]  \n",
    "y_train = y_train.loc[y_Train_valid_indices] \n",
    "X_test  = X_test.loc[X_Test_valid_indices]  \n",
    "y_test  = y_test.loc[y_Test_valid_indices]\n",
    "\n",
    "num_classes = y_test.unique().sum()\n",
    "\n",
    "# Define the XGBClassifier model\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class= num_classes)\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid = param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score on the test set\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Accuracy score on test set: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score on test set: \", f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "labels = [k for k, v in sorted(label_dict.items(), key=lambda item: item[1])]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(\"Confusion matrix on test set: \")\n",
    "print(cm_df)\n",
    "\n",
    "\n",
    "# Save the best model parameters as 'model' variable\n",
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, class_label in label_dict.items():\n",
    "    indices = eval(f\"indices_{category}\")  # Get the indices for the current category\n",
    "\n",
    "    # Find common indexes\n",
    "    X_Train_valid_indices = indices.intersection(X_train.index)\n",
    "    y_Train_valid_indices = indices.intersection(y_train.index)\n",
    "    X_Test_valid_indices = indices.intersection(X_test.index)\n",
    "    y_Test_valid_indices = indices.intersection(y_test.index)\n",
    "\n",
    "    # filter out by indeces\n",
    "    X_train_category = X_train.loc[X_Train_valid_indices]\n",
    "    y_train_category = y_train.loc[y_Train_valid_indices]\n",
    "    X_test_category = X_test.loc[X_Test_valid_indices]\n",
    "    y_test_category = y_test.loc[y_Test_valid_indices]\n",
    "\n",
    "    # Apply XGB on all periods  \n",
    "    y_pred_category = model.predict(X_test_category)\n",
    "    \n",
    "    exec(f\"accuracy_{category} = accuracy_score(y_test_category, y_pred_category)\")\n",
    "    exec(f\"f1_{category} = f1_score(y_test_category, y_pred_category, average='weighted')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test,approximate=True)\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar', class_names=list(label_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Melting Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each class\n",
    "for class_name, class_label in label_dict.items():\n",
    "\n",
    "    # Calculate SHAP values for the current class\n",
    "    shap_values = shap.TreeExplainer(model).shap_values(X_test)\n",
    "    shap_values_class = shap_values[class_label]\n",
    "\n",
    "    # Calculate absolute SHAP values for each feature\n",
    "    shap_df = pd.DataFrame(abs(shap_values_class), columns=X_test.columns)\n",
    "\n",
    "    # Get top 5 features for the current class\n",
    "    top_features = shap_df.mean().sort_values(ascending=False)[:5]\n",
    "\n",
    "    # Create a table with top 5 features and their corresponding SHAP values for the current class\n",
    "    top_features_table = pd.DataFrame({'Feature': top_features.index,\n",
    "                                       'SHAP Value': top_features.values})\n",
    "\n",
    "    # Add a column with the class name\n",
    "    #top_features_table['Class'] = class_name\n",
    "\n",
    "    # Print the table for the current class\n",
    "    print(f'{class_name}:')\n",
    "    \n",
    "    print(\"Accuracy:\" , eval(f\"accuracy_{class_name}\"))\n",
    "    print(\"F1:\" , eval(f\"f1_{class_name}\"))\n",
    "    \n",
    "    display(top_features_table)\n",
    "\n",
    "    # Create summary plot for the current class\n",
    "    shap.summary_plot(shap_values_class, X_test, plot_type='bar', max_display=5, show=False)\n",
    "    plt.title(f'Summary plot for {class_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
