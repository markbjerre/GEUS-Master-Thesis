{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_csv_files(csv_directory=\"..\\..\\PROMICE-AWS-toolbox\\out\\L4\"):\n",
    "    \n",
    "    # List all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Combine all CSV files into a single DataFrame\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(os.path.join(csv_directory, f), index_col=False)\n",
    "        df.insert(0, 'stid', f[:-7])\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    #read metadata from promice repository\n",
    "    station = pd.read_csv('..\\..\\PROMICE-AWS-toolbox\\metadata\\AWS_station_locations.csv', index_col=False)\n",
    "    station.to_csv('..\\\\data\\\\new_promice\\\\AWS_station_locations.csv', index=False)\n",
    "\n",
    "    output_file = \"..\\\\Data\\\\new_promice\\\\all_promice_data.parquet.gzip\"\n",
    "    df.to_parquet(output_file, compression='gzip', engine='pyarrow')\n",
    "\n",
    "    #display(output station and columns summary)\n",
    "    print('Stations loaded:')\n",
    "    display(df['stid'].unique())\n",
    "    print('columns in dataset:')\n",
    "    print(list(df.columns))\n",
    "    print(\"FINISHED LOADING CSV's\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_hourly_data(dataframe, directory=\"../data/new_promice/all_promice_data.parquet.gzip\", add_meta_data=False):\n",
    "\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        df_hourly = pd.read_parquet(directory)\n",
    "    else:\n",
    "        df_hourly = dataframe.copy()\n",
    "\n",
    "    # Add year column to dataframe\n",
    "    df_hourly[\"Datetime\"] = pd.to_datetime(df_hourly.time)\n",
    "\n",
    "    #Rename Index Column to Datetime\n",
    "    df_hourly = df_hourly.reset_index(inplace=False)\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        add_nice_to_haves(df_hourly, 'hourly')\n",
    "\n",
    "    #display(df_hourly.head(10))\n",
    "\n",
    "    # Convert the DataFrame to a compressed Parquet file\n",
    "    output_file = \"../data/new_promice/all_promice_data_hourly.parquet.gzip\"\n",
    "    df_hourly.to_parquet(output_file, compression='gzip', engine='pyarrow')\n",
    "\n",
    "    print(\"FINISHED PROCESSING HOURLY DATA\")\n",
    "    return df_hourly\n",
    "\n",
    "def process_daily_data(dataframe=None, directory=\"\", add_meta_data=False):\n",
    "    \n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        dataframe = pd.read_parquet(directory)\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "\n",
    "    # Define the date column that you want to group by (replace \"date_column\" with the name of your column)\n",
    "    min_values_per_day = 20\n",
    "\n",
    "    # Group the data by weather station and date\n",
    "    grouped = df.groupby(['stid','date'])\n",
    "\n",
    "    # Specify columns containing numerical values to be averaged\n",
    "    columns_to_average = (df\n",
    "                .select_dtypes(exclude=['object'])\n",
    "                .drop(columns=['index', 'Datetime', 'DayOfYear', 'DayOfCentury'])\n",
    "                .columns\n",
    "    )\n",
    "    # Calculate the number of non-NaN values for each variable within each group\n",
    "    valid_date_observations = grouped[columns_to_average].apply(lambda x: x.notna().sum() <= min_values_per_day)\n",
    "\n",
    "    # Calculate average per day per station\n",
    "    df_filtered = grouped[columns_to_average].mean()\n",
    "\n",
    "    #Remove means with less than 20 observations per day\n",
    "    df_masked = df_filtered.mask(valid_date_observations, np.nan)\n",
    "\n",
    "    df_daily = df_masked.reset_index().copy()\n",
    "\n",
    "    df_daily[\"Datetime\"] = pd.to_datetime(df_daily['date'])\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        df_daily = add_nice_to_haves(df_daily, 'daily')\n",
    "\n",
    "    # Convert the DataFrame to a compressed Parquet file\n",
    "    output_file = \"..\\\\Data\\\\new_promice\\\\all_promice_data_daily.parquet.gzip\"\n",
    "    df_daily.to_parquet(output_file, compression='gzip', engine='pyarrow')\n",
    "\n",
    "    print(\"FINISHED PROCESSING DAILY DATA\")\n",
    "    # display(df_daily)\n",
    "    return df_daily\n",
    "\n",
    "def process_monthly_data(dataframe=None, directory=\"\", add_meta_data=False):\n",
    "\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        df = pd.read_parquet(directory)\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "\n",
    "    df['month_year'] = df['Datetime'].dt.to_period('M')\n",
    "    # Define the date column that you want to group by (replace \"date_column\" with the name of your column)\n",
    "    min_values_per_month = 24\n",
    "\n",
    "    # Create a new column with the month and year of the date column\n",
    "\n",
    "    # Group the data by weather station and date\n",
    "    grouped = df.groupby(['stid','month_year'])\n",
    "\n",
    "    # Specify columns containing numerical values to be averaged\n",
    "    columns_to_average = (df\n",
    "                .select_dtypes(exclude=['object'])\n",
    "                .drop(columns=['Datetime', 'DayOfYear', 'DayOfCentury'])\n",
    "                .columns\n",
    "    )\n",
    "\n",
    "    # Calculate the number of non-NaN values for each variable within each group\n",
    "    valid_date_observations = grouped[columns_to_average].apply(lambda x: x.notna().sum() <= min_values_per_month)\n",
    "\n",
    "    # Calculate average per day per station\n",
    "    df_filtered = grouped[columns_to_average].mean()\n",
    "\n",
    "    #Remove means with less than 20 observations per day\n",
    "    df_masked = df_filtered.mask(valid_date_observations, np.nan)\n",
    "\n",
    "    df_monthly = df_masked.reset_index().copy()\n",
    "\n",
    "    df_monthly[\"Datetime\"] = pd.to_datetime(df_monthly['month_year'].astype(str) + '-01')\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        df_monthly = add_nice_to_haves(df_monthly, 'monthly')\n",
    "\n",
    "    #display(df_monthly)\n",
    "\n",
    "    # Convert the DataFrame to a compressed Parquet file\n",
    "    output_file = \"..\\\\Data\\\\new_promice\\\\all_promice_data_monthly.parquet.gzip\"\n",
    "    df_monthly.to_parquet(output_file, compression='gzip', engine='pyarrow')\n",
    "\n",
    "    print('FINISHED PROCESSING MONTHLY DATA')\n",
    "    return df_monthly\n",
    "\n",
    "###########################################################################################\n",
    "### Helper functions\n",
    "\n",
    "def add_station_info(df):\n",
    "    station = pd.read_csv('../data/new_promice/AWS_station_locations.csv')\n",
    "    station.rename(columns={'timestamp':'station_location_timestamp'})\n",
    "    df.merge(station, how='left',on=['stid','stid'])\n",
    "    return df\n",
    "\n",
    "# helper functions for adding metadata like season, day, month, year, date, DayOfYear and DayOfCentury\n",
    "def add_nice_to_haves(df, period):\n",
    "    def add_season(df):\n",
    "        seasons = {\n",
    "            1: \"Winter\",\n",
    "            2: \"Winter\",\n",
    "            3: \"Spring\",\n",
    "            4: \"Spring\",\n",
    "            5: \"Spring\",\n",
    "            6: \"Summer\",\n",
    "            7: \"Summer\",\n",
    "            8: \"Summer\",\n",
    "            9: \"Autumn\",\n",
    "            10: \"Autumn\",\n",
    "            11: \"Autumn\",\n",
    "            12: \"Winter\",\n",
    "        }\n",
    "\n",
    "        # Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "        df[\"season\"] = df['Datetime'].dt.month.map(seasons)\n",
    "        return df\n",
    "\n",
    "    def add_common(df):\n",
    "        df[\"year\"] = df['Datetime'].dt.strftime(\"%Y\")\n",
    "        df[\"month\"] = df['Datetime'].dt.strftime(\"%B\")\n",
    "        df[\"date\"] = df['Datetime'].dt.date\n",
    "        df['DayOfYear'] = df['Datetime'].dt.dayofyear \n",
    "        df['DayOfCentury'] = df['Datetime'].dt.dayofyear+365*(df['Datetime'].dt.year-1)\n",
    "        df = add_season(df)\n",
    "        return df\n",
    "        \n",
    "    def add_day(df):\n",
    "        # Add day column to dataframe\n",
    "        df[\"day\"] = df['Datetime'].dt.strftime(\"%d\")\n",
    "        return df\n",
    "\n",
    "    def add_hour(df):\n",
    "        # Add hour column to dataframe\n",
    "        df[\"hour\"] = df['Datetime'].dt.strftime(\"%h\")\n",
    "        return df\n",
    "\n",
    "    if period == 'hourly':\n",
    "        df = add_hour(\n",
    "            add_day(\n",
    "            add_common(df)))\n",
    "\n",
    "    elif period == 'daily':\n",
    "        df = add_day(\n",
    "            add_common(df))\n",
    "    elif period == 'monthly':\n",
    "        df = add_common(df)\n",
    "    \n",
    "    return df \n",
    "\n",
    "#### Unused code\n",
    "    # Process data\n",
    "    # echo -ne 'Processing dairly data...\\r'\n",
    "    # python scripts/process_data_daily.py\n",
    "    # echo -ne 'Processing hourly data...\\r'\n",
    "    # python scripts/process_data_hourly.py\n",
    "\n",
    "    # Delete unprocessed data\n",
    "    # rm -r data_daily data_hourly\n",
    "\n",
    "    # %% process data hourly\n",
    "    # Reading station metadata\n",
    "\n",
    "    # Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
    "    # null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "    # flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "    # print(null_columns)\n",
    "    # print(flag_columns)\n",
    "\n",
    "    #df_hourly = df_hourly.drop(\n",
    "    #    columns=[\n",
    "    #           'gps_geounit', \n",
    "    #           'batt_v_ini', \n",
    "    #           'freq_vw',\n",
    "    #    ]\n",
    "    #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CEN1', 'CEN2', 'CP1', 'DY2', 'EGP', 'HUM', 'JAR', 'JAR_O',\n",
       "       'KAN_B', 'KAN_L', 'KAN_M', 'KAN_U', 'KPC_Lv3', 'KPC_L', 'KPC_Uv3',\n",
       "       'KPC_U', 'LYN_L', 'LYN_T', 'MIT', 'NAE', 'NAU', 'NEM', 'NSE',\n",
       "       'NUK_K', 'NUK_L', 'NUK_N', 'NUK_Uv3', 'NUK_U', 'QAS_A', 'QAS_Lv3',\n",
       "       'QAS_L', 'QAS_M', 'QAS_Uv3', 'QAS_U', 'Roof_GEUS', 'Roof_PROMICE',\n",
       "       'SCO_L', 'SCO_U', 'SDL', 'SDM', 'SWC', 'SWC_O', 'TAS_A', 'TAS_L',\n",
       "       'TAS_U', 'THU_L2', 'THU_L', 'THU_U2', 'THU_U', 'TUN', 'UPE_L',\n",
       "       'UPE_U', 'UWN', 'WEG_B', 'ZAK_L', 'ZAK_Uv3', 'ZAK_U'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stid', 'time', 'p_u', 't_u', 'rh_u', 'rh_u_cor', 'qh_u', 'wspd_u', 'wdir_u', 'dsr', 'dsr_cor', 'usr', 'usr_cor', 'albedo', 'dlr', 'ulr', 'cc', 't_surf', 'dlhf_u', 'dshf_u', 'z_boom_u', 'z_stake', 'z_pt', 'z_pt_cor', 'precip_u', 'precip_u_cor', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7', 't_i_8', 'tilt_x', 'tilt_y', 'rot', 'gps_lat', 'gps_lon', 'gps_alt', 'gps_time', 'gps_geoid', 'gps_geounit', 'gps_hdop', 'gps_numsat', 'gps_q', 'batt_v', 'batt_v_ini', 'batt_v_ss', 'fan_dc_u', 'freq_vw', 't_log', 't_rad', 'msg_lat', 'msg_lon', 'z_surf_1', 'z_surf_2', 'z_surf_1_adj_flag', 'z_surf_2_adj_flag', 'z_surf_combined', 'depth_t_i_1', 'depth_t_i_2', 'depth_t_i_3', 'depth_t_i_4', 'depth_t_i_5', 'depth_t_i_6', 'depth_t_i_7', 'depth_t_i_8', 't_i_10m', 'p_l', 't_l', 'rh_l', 'rh_l_cor', 'qh_l', 'wspd_l', 'wdir_l', 'dlhf_l', 'dshf_l', 'z_boom_l', 'precip_l', 'precip_l_cor', 't_i_9', 't_i_10', 't_i_11', 'fan_dc_l', 'depth_t_i_9', 'depth_t_i_10', 'depth_t_i_11', 'z_pt_cor_adj_flag', 'z_pt_cor_adj', 'z_surf_1_adj', 'z_surf_2_adj']\n"
     ]
    }
   ],
   "source": [
    "# code to ensure path points to current directory to load local functions \n",
    "#import sys\n",
    "#sys.path.insert(0, \".\")\n",
    "\"\"\"\n",
    "These functions process the dataframe in question.\n",
    "- Functions can be initiated with a dataframe from the previous step, or with a parquet file using directory='<path>'\n",
    "- I have included both versions of the inititation functions for ease of use, choose on of the grayed out versions of each step\n",
    "- Hourly, daily and monthly files can have added 'meta data' such as year, season, month, day, DayOfYear and DayOfCentury\n",
    "\"\"\"\n",
    "df = load_csv_files()\n",
    "\n",
    "df_hourly = process_hourly_data(dataframe=df, add_metadata=True)\n",
    "# df_hourly = process_hourly_data(directory=\"../data/new_promice/all_promice_data.parquet.gzip\")\n",
    "\n",
    "df_daily = process_daily_data(dataframe=df_hourly, add_meta_data=True)\n",
    "# df_daily = process_daily_data(directory=\"../data/new_promice/all_promice_data_hourly.parquet.gzip\", add_meta_data=True)\n",
    "\n",
    "df_monthly = process_monthly_data(dataframe=df_daily, add_meta_data=True)\n",
    "#df_monthly = process_monthly_data(directory='..\\\\Data\\\\new_promice\\\\all_promice_data_daily.parquet.gzip', add_meta_data=True)\n",
    "\n",
    "### helper function to add station information such as # of booms, location and classification\n",
    "\n",
    "# df_hourly = add_station_info(df_hourly)\n",
    "# df_daily = add_station_info(df_daily)\n",
    "# df_monthly = add_station_info(df_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null columns:\n",
      " Index(['gps_geounit', 'batt_v_ini', 'freq_vw'], dtype='object')\n",
      "flag_columns:\n",
      " Index(['z_surf_1_adj_flag', 'z_surf_2_adj_flag', 'z_pt_cor_adj_flag'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "print(\"null columns:\\n\", null_columns)\n",
    "print(\"flag_columns:\\n\", flag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of          stid month_year         p_u        t_u       rh_u   rh_u_cor  \\\n",
       "0        CEN1    2017-05         NaN        NaN        NaN        NaN   \n",
       "1        CEN1    2017-06         NaN        NaN        NaN        NaN   \n",
       "2        CEN1    2017-07         NaN        NaN        NaN        NaN   \n",
       "3        CEN1    2017-08  799.124410  -7.917199  88.455853  94.813997   \n",
       "4        CEN1    2017-09  793.052183 -16.824626  84.394839  98.425428   \n",
       "...       ...        ...         ...        ...        ...        ...   \n",
       "8195  ZAK_Uv3    2022-08  911.193593   1.671179  70.281977  70.517806   \n",
       "8196  ZAK_Uv3    2022-09  910.964335  -1.641274  68.437326  70.609115   \n",
       "8197  ZAK_Uv3    2022-10  905.758516  -8.781081  62.708000  68.377748   \n",
       "8198  ZAK_Uv3    2022-11  906.854591  -7.709388  57.323370  61.688199   \n",
       "8199  ZAK_Uv3    2022-12         NaN        NaN        NaN        NaN   \n",
       "\n",
       "          qh_u    wspd_u      wdir_u         dsr  ...  z_pt_cor_adj  \\\n",
       "0          NaN       NaN         NaN         NaN  ...           NaN   \n",
       "1          NaN       NaN         NaN         NaN  ...           NaN   \n",
       "2          NaN       NaN         NaN         NaN  ...           NaN   \n",
       "3     2.456919  6.163883  176.534265  165.803953  ...           NaN   \n",
       "4     1.179660  6.211591  156.409902   53.274829  ...           NaN   \n",
       "...        ...       ...         ...         ...  ...           ...   \n",
       "8195  3.271951  2.368994  276.671090  157.557931  ...     -1.380665   \n",
       "8196  2.604413  2.885144  278.737042   73.156287  ...     -1.802585   \n",
       "8197  1.350249  3.190974  292.853676    9.974857  ...     -2.391550   \n",
       "8198  1.403053  2.786896  272.532050   -0.863138  ...     -2.349411   \n",
       "8199       NaN       NaN         NaN         NaN  ...           NaN   \n",
       "\n",
       "      z_surf_1_adj  z_surf_2_adj   Datetime  year      month        date  \\\n",
       "0              NaN           NaN 2017-05-01  2017        May  2017-05-01   \n",
       "1              NaN           NaN 2017-06-01  2017       June  2017-06-01   \n",
       "2              NaN           NaN 2017-07-01  2017       July  2017-07-01   \n",
       "3              NaN           NaN 2017-08-01  2017     August  2017-08-01   \n",
       "4              NaN           NaN 2017-09-01  2017  September  2017-09-01   \n",
       "...            ...           ...        ...   ...        ...         ...   \n",
       "8195     -0.284371     -1.706878 2022-08-01  2022     August  2022-08-01   \n",
       "8196     -0.263356     -1.926279 2022-09-01  2022  September  2022-09-01   \n",
       "8197     -2.337516     -2.337133 2022-10-01  2022    October  2022-10-01   \n",
       "8198     -2.306924     -2.278979 2022-11-01  2022   November  2022-11-01   \n",
       "8199           NaN           NaN 2022-12-01  2022   December  2022-12-01   \n",
       "\n",
       "      DayOfYear  DayOfCentury  season  \n",
       "0           121        735961  Spring  \n",
       "1           152        735992  Summer  \n",
       "2           182        736022  Summer  \n",
       "3           213        736053  Summer  \n",
       "4           244        736084  Autumn  \n",
       "...         ...           ...     ...  \n",
       "8195        213        737878  Summer  \n",
       "8196        244        737909  Autumn  \n",
       "8197        274        737939  Autumn  \n",
       "8198        305        737970  Autumn  \n",
       "8199        335        738000  Winter  \n",
       "\n",
       "[8200 rows x 99 columns]>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_monthly.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "809dd00cb0b4cf7cf034b21fe0b3828c1cd05894ca0015867f4d20f1f520ae9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
