{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-win_amd64.whl (20.6 MB)\n",
      "     --------------------------------------- 20.6/20.6 MB 50.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from pyarrow) (1.21.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-11.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\..\\\\PROMICE-AWS-toolbox\\\\out\\\\L4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\3/ipykernel_10788/3955089079.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# List all CSV files in the directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcsv_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_directory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Combine all CSV files into a single DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '..\\\\..\\\\PROMICE-AWS-toolbox\\\\out\\\\L4'"
     ]
    }
   ],
   "source": [
    "#Fetch file from Promice folder (should be in same parent directory as GEUS-Master_Thesis)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing CSV files\n",
    "csv_directory = \"..\\..\\PROMICE-AWS-toolbox\\out\\L4\"\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Combine all CSV files into a single DataFrame\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(os.path.join(csv_directory, f))\n",
    "    df.insert(0, 'station_name', f[:-7])\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "#read metadata from promice repository\n",
    "station = pd.read_csv('..\\..\\PROMICE-AWS-toolbox\\metadata\\AWS_station_locations.csv')\n",
    "station.to_csv('..\\\\data\\\\new_promice\\\\AWS_station_locations.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "output_file = \"..\\\\Data\\\\new_promice\\\\all_promice_data.parquet.gzip\"\n",
    "df.to_parquet(output_file, compression='gzip', engine='pyarrow')\n",
    "\n",
    "#read metadata from promice repository\n",
    "station = pd.read_csv('..\\..\\PROMICE-AWS-toolbox\\metadata\\AWS_station_locations.csv')\n",
    "station.to_csv('..\\\\data\\\\new_promice\\\\AWS_station_locations.csv', index=False)\n",
    "\n",
    "\n",
    "#display(output station and columns summary)\n",
    "display(df['station_name'].unique())\n",
    "print(list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>stid</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>p_u</th>\n",
       "      <th>t_u</th>\n",
       "      <th>rh_u</th>\n",
       "      <th>rh_u_cor</th>\n",
       "      <th>qh_u</th>\n",
       "      <th>wspd_u</th>\n",
       "      <th>wdir_u</th>\n",
       "      <th>...</th>\n",
       "      <th>depth_7</th>\n",
       "      <th>depth_8</th>\n",
       "      <th>IceTemperature10m(C)</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>DayOfCentury</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 10:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.33</td>\n",
       "      <td>172.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 11:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 12:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 13:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 14:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 15:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 16:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 17:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 18:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>CEN1</td>\n",
       "      <td>2017-05-23 19:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>Spring</td>\n",
       "      <td>143</td>\n",
       "      <td>735983</td>\n",
       "      <td>23</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  stid                  Datetime  p_u  t_u  rh_u  rh_u_cor  qh_u  \\\n",
       "0      0  CEN1 2017-05-23 10:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "1      1  CEN1 2017-05-23 11:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "2      2  CEN1 2017-05-23 12:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "3      3  CEN1 2017-05-23 13:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "4      4  CEN1 2017-05-23 14:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "5      5  CEN1 2017-05-23 15:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "6      6  CEN1 2017-05-23 16:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "7      7  CEN1 2017-05-23 17:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "8      8  CEN1 2017-05-23 18:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "9      9  CEN1 2017-05-23 19:00:00+00:00  NaN  NaN   NaN       NaN   NaN   \n",
       "\n",
       "   wspd_u  wdir_u  ...  depth_7  depth_8  IceTemperature10m(C)  year  month  \\\n",
       "0   17.33   172.5  ...      NaN      NaN                   NaN  2017    May   \n",
       "1     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "2     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "3     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "4     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "5     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "6     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "7     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "8     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "9     NaN     NaN  ...      NaN      NaN                   NaN  2017    May   \n",
       "\n",
       "   season  DayOfYear  DayOfCentury  day  hour  \n",
       "0  Spring        143        735983   23   May  \n",
       "1  Spring        143        735983   23   May  \n",
       "2  Spring        143        735983   23   May  \n",
       "3  Spring        143        735983   23   May  \n",
       "4  Spring        143        735983   23   May  \n",
       "5  Spring        143        735983   23   May  \n",
       "6  Spring        143        735983   23   May  \n",
       "7  Spring        143        735983   23   May  \n",
       "8  Spring        143        735983   23   May  \n",
       "9  Spring        143        735983   23   May  \n",
       "\n",
       "[10 rows x 157 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data cleaning. Taking promice data and adding datetime columns + station metadata\n",
    "#Leaving old code in case it's usefull later\n",
    "\n",
    "# Process data\n",
    "# echo -ne 'Processing dairly data...\\r'\n",
    "# python scripts/process_data_daily.py\n",
    "# echo -ne 'Processing hourly data...\\r'\n",
    "# python scripts/process_data_hourly.py\n",
    "\n",
    "# Delete unprocessed data\n",
    "# rm -r data_daily data_hourly\n",
    "\n",
    "# %% process data hourly\n",
    "# Reading station metadata\n",
    "station = pd.read_csv('../data/new_promice/AWS_station_locations.csv')\n",
    "\n",
    "station.rename(columns={'timestamp':'station_location_timestamp'})\n",
    "\n",
    "#Reading parquet dataframe\n",
    "df_hourly = df.copy()\n",
    "\n",
    "# Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
    "# null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "# flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "# print(null_columns)\n",
    "# print(flag_columns)\n",
    "\n",
    "#df_hourly = df_hourly.drop(\n",
    "#    columns=[\n",
    "#           'gps_geounit', \n",
    "#           'batt_v_ini', \n",
    "#           'freq_vw', \n",
    "#           'DepthPressureTransducer(m)',\n",
    "#           'DepthPressureTransducer_Cor(m)', \n",
    "#           'IceTemperature7(C)',\n",
    "#           'IceTemperature8(C)', \n",
    "#           'LatitudeGPS(degN)', \n",
    "#           'LongitudeGPS(degW)',\n",
    "#           'ElevationGPS(m)', \n",
    "#           'TimeGPS(hhmmssUTC)', \n",
    "#           'HorDilOfPrecGPS',\n",
    "#           'DepthPressureTransducer_Cor_adj(m)', \n",
    "#           'IceTemperature10m(C)', \n",
    "#           'z_surf_1_adj_flag', \n",
    "#           'z_surf_2_adj_flag', \n",
    "#           'z_pt_cor_adj_flag'\n",
    "#    ]\n",
    "#)\n",
    "# Add year column to dataframe\n",
    "df_hourly[\"time\"] = pd.to_datetime(df_hourly.time)\n",
    "\n",
    "df_hourly[\"year\"] = df_hourly.time.dt.strftime(\"%Y\")\n",
    "\n",
    "# Add month column to dataframe\n",
    "df_hourly[\"month\"] = df_hourly.time.dt.strftime(\"%B\")\n",
    "\n",
    "# Add season column to dataframe\n",
    "seasons = {\n",
    "    1: \"Winter\",\n",
    "    2: \"Winter\",\n",
    "    3: \"Spring\",\n",
    "    4: \"Spring\",\n",
    "    5: \"Spring\",\n",
    "    6: \"Summer\",\n",
    "    7: \"Summer\",\n",
    "    8: \"Summer\",\n",
    "    9: \"Autumn\",\n",
    "    10: \"Autumn\",\n",
    "    11: \"Autumn\",\n",
    "    12: \"Winter\",\n",
    "}\n",
    "\n",
    "# Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "df_hourly[\"season\"] = df_hourly.time.dt.month.map(seasons)\n",
    "\n",
    "\n",
    "#Rename Index Column to Datetime\n",
    "df_hourly = df_hourly.reset_index(inplace=False)\n",
    "df_hourly = df_hourly.rename(columns={'time': 'Datetime'}, inplace=False) \n",
    "\n",
    "# Rename 'station_name' to file\n",
    "df_hourly = df_hourly.rename(columns={'station_name': 'stid'}, inplace=False)\n",
    "\n",
    "# Add Day of Year & Day of Century \n",
    "df_hourly['DayOfYear'] = df_hourly['Datetime'].dt.dayofyear \n",
    "df_hourly['DayOfCentury'] = df_hourly['Datetime'].dt.dayofyear+365*(df_hourly['Datetime'].dt.year-1)\n",
    "\n",
    "# # Add day column to dataframe\n",
    "df_hourly[\"day\"] = df_hourly['Datetime'].dt.strftime(\"%d\")\n",
    "\n",
    "# # Add hour column to dataframe\n",
    "df_hourly[\"hour\"] = df_hourly['Datetime'].dt.strftime(\"%h\")\n",
    "\n",
    "#merging with station metadata\n",
    "\n",
    "#df_hourly.merge(station, how='left',on=['stid','stid'])\n",
    "\n",
    "display(df_hourly.head(10))\n",
    "# Convert the DataFrame to a compressed Parquet file\n",
    "#output_file = \"..\\\\Data\\\\new_promice\\\\all_promice_data.parquet.gzip\"\n",
    "#df.to_parquet(output_file, compression='gzip', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null columns:\n",
      " Index(['gps_geounit', 'batt_v_ini', 'freq_vw', 'DepthPressureTransducer(m)',\n",
      "       'DepthPressureTransducer_Cor(m)', 'IceTemperature7(C)',\n",
      "       'IceTemperature8(C)', 'LatitudeGPS(degN)', 'LongitudeGPS(degW)',\n",
      "       'ElevationGPS(m)', 'TimeGPS(hhmmssUTC)', 'HorDilOfPrecGPS',\n",
      "       'DepthPressureTransducer_Cor_adj(m)', 'IceTemperature10m(C)'],\n",
      "      dtype='object')\n",
      "flag_columns:\n",
      " Index(['z_surf_1_adj_flag', 'z_surf_2_adj_flag', 'z_pt_cor_adj_flag'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "print(\"null columns:\\n\", null_columns)\n",
    "print(\"flag_columns:\\n\", flag_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "809dd00cb0b4cf7cf034b21fe0b3828c1cd05894ca0015867f4d20f1f520ae9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
