{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLqu3qB_6MQ0"
   },
   "source": [
    "GC Net Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from pyarrow) (1.24.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/Users/nilsfulde/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "iFn7hTObSpj6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/GEUS-PROMICE/pyNEAD.git\n",
      "  Cloning https://github.com/GEUS-PROMICE/pyNEAD.git to c:\\users\\mabj16ac\\appdata\\local\\temp\\4\\pip-req-build-1zu17gro\n",
      "  Resolved https://github.com/GEUS-PROMICE/pyNEAD.git to commit d811220c2d048d84d4612b045880d6ca914c39a4\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from nead==0.0.0) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from nead==0.0.0) (1.4.4)\n",
      "Requirement already satisfied: xarray in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from nead==0.0.0) (0.20.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from pandas->nead==0.0.0) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from pandas->nead==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->nead==0.0.0) (1.16.0)\n",
      "Building wheels for collected packages: nead\n",
      "  Building wheel for nead (setup.py): started\n",
      "  Building wheel for nead (setup.py): finished with status 'done'\n",
      "  Created wheel for nead: filename=nead-0.0.0-py2.py3-none-any.whl size=10663 sha256=4736ab4fc8e3fe62dc67c2c15b6c58ff60aed7a9afae1d2942330122de5b691b\n",
      "  Stored in directory: C:\\Users\\mabj16ac\\AppData\\Local\\Temp\\4\\pip-ephem-wheel-cache-zymeu2hk\\wheels\\7e\\b5\\9c\\bf1fb23ef32c0fd131d467e2ee10e1bc483361a4dba16b3600\n",
      "Successfully built nead\n",
      "Installing collected packages: nead\n",
      "Successfully installed nead-0.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/GEUS-PROMICE/pyNEAD.git 'C:\\Users\\mabj16ac\\AppData\\Local\\Temp\\4\\pip-req-build-1zu17gro'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/GEUS-PROMICE/pyNEAD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mabj16ac\\Desktop\\Thesis\\GEUS-Master-Thesis\\scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('c:\\\\Users\\\\mabj16ac\\\\Desktop\\\\Thesis\\\\GEUS-Master-Thesis\\\\scripts')\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba5Hy-lHR2Jr",
    "outputId": "22868e77-0da1-4934-9b33-e66e47e507c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mabj16ac\\Desktop\\Thesis\\GEUS-Master-Thesis\n",
      "Overwritting existing data in \"/data\"\n",
      "Downloading daily data...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "# Latest L1 data: https://github.com/GEUS-Glaciology-and-Climate/GC-Net-level-1-data-processing/tree/main/L1\n",
    "# API contents of latest L1 data (raw URLs etc.): https://api.github.com/repositories/319306521/contents/L1\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "#os.chdir('C:\\\\Users\\\\nifu18ab\\\\Desktop\\\\GEUS-Master-Thesis')\n",
    "os.chdir('c:\\\\Users\\\\mabj16ac\\\\Desktop\\\\Thesis\\\\GEUS-Master-Thesis\\\\scripts')\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"data\")\n",
    "    os.mkdir(\"data/gc_net/raw_data/data_daily\")\n",
    "    os.mkdir(\"data/gc_net/raw_data/data_hourly\")\n",
    "except:\n",
    "    print('Overwritting existing data in \"/data\"')\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading daily data...\\r\")\n",
    "\n",
    "\n",
    "# xargs -n 1 curl --silent -O --output-dir data_daily < ../metadata/urls_1.txt\n",
    "for url in open(\"metadata/urls_1.txt\"):\n",
    "    # Split on the rightmost / and take everything on the right side of that\n",
    "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
    "    # Strip /n at the end of filename\n",
    "    name = name.strip()\n",
    "    # Combine the name and the downloads directory to get the local filename\n",
    "    filename = os.path.join(\"data/gc_net/raw_data/data_daily\", name)\n",
    "\n",
    "    # Download the file if it does not exist\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exKNv5IVbMx-",
    "outputId": "3f0f0b01-578c-4d40-940c-ecfd2547209b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hourly data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading hourly data...\\r\")\n",
    "# xargs -n 1 curl --silent -O --output-dir data_hourly < ../metadata/urls_2.txt\n",
    "for url in open(\"metadata/urls_2.txt\"):\n",
    "    # Split on the rightmost / and take everything on the right side of that\n",
    "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
    "    # Strip /n at the end of filename\n",
    "    name = name.strip()\n",
    "    # Combine the name and the downloads directory to get the local filename\n",
    "    filename = os.path.join(\"data/gc_net/raw_data/data_hourly\", name)\n",
    "\n",
    "    # Download the file if it does not exist\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nead\n",
    "\n",
    "def process_gcnet_daily():\n",
    "    print('PROCESSING GC_NET DAILY...')\n",
    "    # Convert NEAD files to Pandas dataframes\n",
    "    station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
    "    dfs_daily = []\n",
    "\n",
    "    for name, ID in zip(station.Name, station.ID):\n",
    "        format_name = name.replace(\" \", \"\")\n",
    "        files = \"data/gc_net/raw_data/data_daily/\" + str(ID).zfill(2) + \"-\" + format_name + \"_daily.csv\"\n",
    "        ds_daily = nead.read(files, index_col=0)\n",
    "        df_daily = ds_daily.to_dataframe()\n",
    "        df_daily.insert(\n",
    "            loc=0, column=\"station_name\", value=name\n",
    "        )  # Add station_name column to each dataframe\n",
    "        dfs_daily.append(df_daily)\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    df_daily = pd.concat(dfs_daily).sort_index()\n",
    "\n",
    "    # Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
    "    # null_columns = df_daily.columns[df_daily.isnull().all()]\n",
    "    # flag_columns = df_daily.filter(regex=\"flag$\").columns\n",
    "    # print(null_columns)\n",
    "    # print(flag_columns)\n",
    "\n",
    "    df_daily = df_daily.drop(\n",
    "        columns=[\n",
    "            \"OSWR_max\",\n",
    "            \"HW2_adj_flag\",\n",
    "            \"P_adj_flag\",\n",
    "            \"HW1_adj_flag\",\n",
    "            \"OSWR_adj_flag\",\n",
    "            \"HS1_adj_flag\",\n",
    "            \"HS2_adj_flag\",\n",
    "            \"TA3_adj_flag\",\n",
    "            \"TA4_adj_flag\",\n",
    "            \"DW1_adj_flag\",\n",
    "        ]\n",
    "    )\n",
    "    # Add season column to dataframe\n",
    "    seasons = {\n",
    "        1: \"Winter\",\n",
    "        2: \"Winter\",\n",
    "        3: \"Spring\",\n",
    "        4: \"Spring\",\n",
    "        5: \"Spring\",\n",
    "        6: \"Summer\",\n",
    "        7: \"Summer\",\n",
    "        8: \"Summer\",\n",
    "        9: \"Autumn\",\n",
    "        10: \"Autumn\",\n",
    "        11: \"Autumn\",\n",
    "        12: \"Winter\",\n",
    "    }\n",
    "\n",
    "    # Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "    df_daily[\"season\"] = df_daily.index.month.map(seasons)\n",
    "\n",
    "    # Add year column to dataframe\n",
    "    df_daily[\"year\"] = df_daily.index.strftime(\"%Y\")\n",
    "\n",
    "    # Add month column to dataframe\n",
    "    df_daily[\"month\"] = df_daily.index.strftime(\"%B\")\n",
    "\n",
    "    # Rename Index Column to Datetime\n",
    "    df_daily = df_daily.reset_index(inplace=False)\n",
    "    df_daily = df_daily.rename(columns={'timestamp': 'Datetime'}, inplace=False) \n",
    "\n",
    "    # Rename 'station_name' to file\n",
    "    df_daily = df_daily.rename(columns={'station_name': 'file'}, inplace=False)\n",
    "\n",
    "    # Add Day of Year & Day of Century \n",
    "    df_daily['DayOfYear'] = df_daily['Datetime'].dt.dayofyear \n",
    "    df_daily['DayOfCentury'] = df_daily['Datetime'].dt.dayofyear+365*(df_daily['Datetime'].dt.year-1)\n",
    "\n",
    "\n",
    "    # # Add day column to dataframe\n",
    "    # df_daily[\"day\"] = df_daily.index.strftime(\"%d\")\n",
    "\n",
    "    # Add hour column to dataframe\n",
    "    # df_daily[\"hour\"] = df_daily.index.strftime(\"%h\")\n",
    "\n",
    "    # Change column headers\n",
    "    #header = pd.read_csv('/content/drive/MyDrive/Master_Thesis/metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "    header = pd.read_csv('metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "    df_daily = df_daily.rename(columns = header.set_index('fields')['display_description'])\n",
    "\n",
    "    print('SAVING PROCESSED GC_NET_DAILY TO CSVs...')\n",
    "    counter = 1\n",
    "    for station in df_daily['file'].unique():\n",
    "        station_csv = df_daily.loc[df_daily['file'] == station]\n",
    "        station_csv.to_csv('data/gc_net/daily_data/{num}_{station}.csv'.format(num=counter,station=station))\n",
    "        counter += 1 \n",
    "    print('FINISHED SAVING GC_NET DAILY TO CSVs')\n",
    "    return df_daily\n",
    "\n",
    "def process_gcnet_hourly():\n",
    "    print('PROCESSING GC_NET HOURLY...')\n",
    "    station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
    "    dfs_hourly = []\n",
    "\n",
    "    for name, ID in zip(station.Name, station.ID):\n",
    "        format_name = name.replace(\" \", \"\")\n",
    "        files = \"data/gc_net/raw_data/data_hourly/\" + str(ID).zfill(2) + \"-\" + format_name + \".csv\"\n",
    "        ds_hourly = nead.read(files, index_col=0)\n",
    "        df_hourly = ds_hourly.to_dataframe()\n",
    "        df_hourly.insert(\n",
    "            loc=0, column=\"station_name\", value=name\n",
    "        )  # Add station_name column to each dataframe\n",
    "        dfs_hourly.append(df_hourly)\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    df_hourly = pd.concat(dfs_hourly).sort_index()\n",
    "\n",
    "    # Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
    "    # null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "    # flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "    # print(null_columns)\n",
    "    # print(flag_columns)\n",
    "\n",
    "    df_hourly = df_hourly.drop(\n",
    "        columns=[\n",
    "            \"OSWR_max\",\n",
    "            \"HW2_adj_flag\",\n",
    "            \"P_adj_flag\",\n",
    "            \"HW1_adj_flag\",\n",
    "            \"OSWR_adj_flag\",\n",
    "            \"HS1_adj_flag\",\n",
    "            \"HS2_adj_flag\",\n",
    "            \"TA3_adj_flag\",\n",
    "            \"TA4_adj_flag\",\n",
    "            \"DW1_adj_flag\",\n",
    "        ]\n",
    "    )\n",
    "    # Add year column to dataframe\n",
    "    df_hourly[\"year\"] = df_hourly.index.strftime(\"%Y\")\n",
    "\n",
    "    # Add month column to dataframe\n",
    "    df_hourly[\"month\"] = df_hourly.index.strftime(\"%B\")\n",
    "\n",
    "    # Add season column to dataframe\n",
    "    seasons = {\n",
    "        1: \"Winter\",\n",
    "        2: \"Winter\",\n",
    "        3: \"Spring\",\n",
    "        4: \"Spring\",\n",
    "        5: \"Spring\",\n",
    "        6: \"Summer\",\n",
    "        7: \"Summer\",\n",
    "        8: \"Summer\",\n",
    "        9: \"Autumn\",\n",
    "        10: \"Autumn\",\n",
    "        11: \"Autumn\",\n",
    "        12: \"Winter\",\n",
    "    }\n",
    "\n",
    "    # Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "    df_hourly[\"season\"] = df_hourly.index.month.map(seasons)\n",
    "\n",
    "\n",
    "    #Rename Index Column to Datetime\n",
    "    df_hourly = df_hourly.reset_index(inplace=False)\n",
    "    df_hourly = df_hourly.rename(columns={'timestamp': 'Datetime'}, inplace=False) \n",
    "\n",
    "    # Rename 'station_name' to file\n",
    "    df_hourly = df_hourly.rename(columns={'station_name': 'file'}, inplace=False)\n",
    "\n",
    "    # Add Day of Year & Day of Century \n",
    "    df_hourly['DayOfYear'] = df_hourly['Datetime'].dt.dayofyear \n",
    "    df_hourly['DayOfCentury'] = df_hourly['Datetime'].dt.dayofyear+365*(df_hourly['Datetime'].dt.year-1)\n",
    "\n",
    "\n",
    "    # # Add day column to dataframe\n",
    "    # df_hourly[\"day\"] = df_hourly.index.strftime(\"%d\")\n",
    "\n",
    "    # # Add hour column to dataframe\n",
    "    # df_hourly[\"hour\"] = df_hourly.index.strftime(\"%h\")\n",
    "\n",
    "\n",
    "    #header = pd.read_csv('/content/drive/MyDrive/Master_Thesis/metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "    header = pd.read_csv('metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "    df_hourly = df_hourly.rename(columns = header.set_index('fields')['display_description'])\n",
    "\n",
    "    print('SAVING PROCESSED GC_NET HOURLY TO CSVs')\n",
    "    counter = 1\n",
    "    for station in df_hourly['file'].unique():\n",
    "        station_csv = df_hourly.loc[df_hourly['file'] == station]\n",
    "        station_csv.to_csv('data/gc_net/hourly_data/{num}_{station}.csv'.format(num=counter,station=station))\n",
    "        counter += 1 \n",
    "    print('FINISHED SAVING GC_NET HOURLY TO CSVs')\n",
    "    \n",
    "\n",
    "    # Save dataframe as parquet file\n",
    "    #df_hourly.to_parquet(\"data/df_hourly.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>file</th>\n",
       "      <th>shortwave_incoming_radiation</th>\n",
       "      <th>shortwave_outgoing_radiation</th>\n",
       "      <th>net_radiation</th>\n",
       "      <th>air_temperature_1</th>\n",
       "      <th>air_temperature_1_max</th>\n",
       "      <th>air_temperature_1_min</th>\n",
       "      <th>air_temperature_cs500_air1</th>\n",
       "      <th>air_temperature_cs500_air2</th>\n",
       "      <th>...</th>\n",
       "      <th>incoming_uv_radiation</th>\n",
       "      <th>incoming_longwave_radiation</th>\n",
       "      <th>surface_temperature_1</th>\n",
       "      <th>surface_temperature_2</th>\n",
       "      <th>net_radiation_maximum</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>DayOfCentury</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-06-01 01:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-06-01 02:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-06-01 03:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-06-01 04:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-06-01 05:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967230</th>\n",
       "      <td>2022-10-16 22:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>289</td>\n",
       "      <td>737954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967231</th>\n",
       "      <td>2022-10-16 23:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>289</td>\n",
       "      <td>737954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967232</th>\n",
       "      <td>2022-10-17 00:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>290</td>\n",
       "      <td>737955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967233</th>\n",
       "      <td>2022-10-17 01:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>290</td>\n",
       "      <td>737955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967234</th>\n",
       "      <td>2022-10-17 02:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>290</td>\n",
       "      <td>737955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3967235 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime            file  shortwave_incoming_radiation  \\\n",
       "0       1990-06-01 01:00:00  Swiss Camp 10m                           NaN   \n",
       "1       1990-06-01 02:00:00  Swiss Camp 10m                           NaN   \n",
       "2       1990-06-01 03:00:00  Swiss Camp 10m                           NaN   \n",
       "3       1990-06-01 04:00:00  Swiss Camp 10m                           NaN   \n",
       "4       1990-06-01 05:00:00  Swiss Camp 10m                           NaN   \n",
       "...                     ...             ...                           ...   \n",
       "3967230 2022-10-16 22:00:00        Humboldt                           NaN   \n",
       "3967231 2022-10-16 23:00:00        Humboldt                           NaN   \n",
       "3967232 2022-10-17 00:00:00        Humboldt                           NaN   \n",
       "3967233 2022-10-17 01:00:00        Humboldt                           NaN   \n",
       "3967234 2022-10-17 02:00:00        Humboldt                           NaN   \n",
       "\n",
       "         shortwave_outgoing_radiation  net_radiation  air_temperature_1  \\\n",
       "0                                 NaN            NaN               1.72   \n",
       "1                                 NaN            NaN               1.42   \n",
       "2                                 NaN            NaN               0.88   \n",
       "3                                 NaN            NaN              -0.13   \n",
       "4                                 NaN            NaN              -1.00   \n",
       "...                               ...            ...                ...   \n",
       "3967230                           NaN            NaN                NaN   \n",
       "3967231                           NaN            NaN                NaN   \n",
       "3967232                           NaN            NaN                NaN   \n",
       "3967233                           NaN            NaN                NaN   \n",
       "3967234                           NaN            NaN                NaN   \n",
       "\n",
       "         air_temperature_1_max  air_temperature_1_min  \\\n",
       "0                          NaN                    NaN   \n",
       "1                          NaN                    NaN   \n",
       "2                          NaN                    NaN   \n",
       "3                          NaN                    NaN   \n",
       "4                          NaN                    NaN   \n",
       "...                        ...                    ...   \n",
       "3967230                    NaN                    NaN   \n",
       "3967231                    NaN                    NaN   \n",
       "3967232                    NaN                    NaN   \n",
       "3967233                    NaN                    NaN   \n",
       "3967234                    NaN                    NaN   \n",
       "\n",
       "         air_temperature_cs500_air1  air_temperature_cs500_air2  ...  \\\n",
       "0                               NaN                         NaN  ...   \n",
       "1                               NaN                         NaN  ...   \n",
       "2                               NaN                         NaN  ...   \n",
       "3                               NaN                         NaN  ...   \n",
       "4                               NaN                         NaN  ...   \n",
       "...                             ...                         ...  ...   \n",
       "3967230                         NaN                         NaN  ...   \n",
       "3967231                         NaN                         NaN  ...   \n",
       "3967232                         NaN                         NaN  ...   \n",
       "3967233                         NaN                         NaN  ...   \n",
       "3967234                         NaN                         NaN  ...   \n",
       "\n",
       "         incoming_uv_radiation  incoming_longwave_radiation  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "...                        ...                          ...   \n",
       "3967230                    NaN                          NaN   \n",
       "3967231                    NaN                          NaN   \n",
       "3967232                    NaN                          NaN   \n",
       "3967233                    NaN                          NaN   \n",
       "3967234                    NaN                          NaN   \n",
       "\n",
       "         surface_temperature_1  surface_temperature_2  net_radiation_maximum  \\\n",
       "0                          NaN                    NaN                    NaN   \n",
       "1                          NaN                    NaN                    NaN   \n",
       "2                          NaN                    NaN                    NaN   \n",
       "3                          NaN                    NaN                    NaN   \n",
       "4                          NaN                    NaN                    NaN   \n",
       "...                        ...                    ...                    ...   \n",
       "3967230                    NaN                    NaN                    NaN   \n",
       "3967231                    NaN                    NaN                    NaN   \n",
       "3967232                    NaN                    NaN                    NaN   \n",
       "3967233                    NaN                    NaN                    NaN   \n",
       "3967234                    NaN                    NaN                    NaN   \n",
       "\n",
       "         year    month  season  DayOfYear  DayOfCentury  \n",
       "0        1990     June  Summer        152        726137  \n",
       "1        1990     June  Summer        152        726137  \n",
       "2        1990     June  Summer        152        726137  \n",
       "3        1990     June  Summer        152        726137  \n",
       "4        1990     June  Summer        152        726137  \n",
       "...       ...      ...     ...        ...           ...  \n",
       "3967230  2022  October  Autumn        289        737954  \n",
       "3967231  2022  October  Autumn        289        737954  \n",
       "3967232  2022  October  Autumn        290        737955  \n",
       "3967233  2022  October  Autumn        290        737955  \n",
       "3967234  2022  October  Autumn        290        737955  \n",
       "\n",
       "[3967235 rows x 65 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_gcnet_daily()\n",
    "process_gcnet_hourly()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Promice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_hourly_data(csv_directory=\"..\\..\\PROMICE-AWS-toolbox\\out\\L4\", add_meta_data=False):\n",
    "        # List all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Combine all CSV files into a single DataFrame\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(os.path.join(csv_directory, f), index_col=False)\n",
    "        df.insert(0, 'stid', f[:-7])\n",
    "        dfs.append(df)\n",
    "    df_hourly = pd.concat(dfs)\n",
    "\n",
    "    #read metadata from promice repository\n",
    "    station = pd.read_csv('../../PROMICE-AWS-toolbox/metadata/AWS_station_locations.csv', index_col=False)\n",
    "    station.to_csv('../data/promice/new_promice/AWS_station_locations.csv', index=False)\n",
    "\n",
    "\n",
    "    #display(output station and columns summary)\n",
    "    print('Stations loaded:')\n",
    "    display(df_hourly['stid'].unique())\n",
    "    print('columns in dataset:')\n",
    "    print(list(df_hourly.columns))\n",
    "    print(\"FINISHED LOADING CSV's\")\n",
    "\n",
    "    # Add year column to dataframe\n",
    "    df_hourly[\"Datetime\"] = pd.to_datetime(df_hourly.time)\n",
    "\n",
    "    #Rename Index Column to Datetime\n",
    "    df_hourly = df_hourly.reset_index(inplace=False)\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        add_nice_to_haves(df_hourly, 'hourly')\n",
    "\n",
    "    header = pd.read_csv('../metadata/promice_header.csv', sep = \";\")\n",
    "    df_hourly = df_hourly.rename(columns = header.set_index('standard_name')['long_name'])\n",
    "    print(df_hourly.columns)\n",
    "\n",
    "    counter = 1\n",
    "    for station in df_hourly['stid'].unique():\n",
    "        station_csv = df_hourly.loc[df_hourly['stid'] == station]\n",
    "        station_csv.to_csv('../data/promice/new_promice/hourly_data/{num}_{station}.csv'.format(num=counter,station=station))\n",
    "        counter += 1 \n",
    "\n",
    "    print(\"FINISHED PROCESSING HOURLY DATA\")\n",
    "    return df_hourly\n",
    "\n",
    "def process_daily_data(dataframe=None, directory=\"../data/promice/new_promice/hourly_data\", add_meta_data=False):\n",
    "    \n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        # List all CSV files in the directory\n",
    "        csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "        # Combine all CSV files into a single DataFrame\n",
    "        dfs = []\n",
    "        for f in csv_files:\n",
    "            df = pd.read_csv(os.path.join(directory, f), index_col=False)\n",
    "            dfs.append(df)\n",
    "            df = pd.concat(dfs)\n",
    "        print('finished loading csvs')\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "\n",
    "    # Define the date column that you want to group by (replace \"date_column\" with the name of your column)\n",
    "    min_values_per_day = 20\n",
    "\n",
    "    # Group the data by weather station and date\n",
    "    grouped = df.groupby(['stid','date'])\n",
    "    \n",
    "\n",
    "    # Specify columns containing numerical values to be averaged\n",
    "    columns_to_average = (df\n",
    "                .select_dtypes(exclude=['object'])\n",
    "                .drop(columns=['index', 'DayOfYear', 'DayOfCentury'])\n",
    "                .columns\n",
    "    )\n",
    "    # Calculate the number of non-NaN values for each variable within each group\n",
    "    valid_date_observations = grouped[columns_to_average].apply(lambda x: x.notna().sum() <= min_values_per_day)\n",
    "    filter_excluded_columns = ['albedo' ]\n",
    "    valid_date_observations[filter_excluded_columns] = False\n",
    "\n",
    "\n",
    "    # Calculate average per day per station\n",
    "    df_filtered = grouped[columns_to_average].mean()\n",
    "\n",
    "    #Remove means with less than 20 observations per day\n",
    "    df_masked = df_filtered.mask(valid_date_observations, np.nan)\n",
    "\n",
    "    df_daily = df_masked.reset_index().copy()\n",
    "\n",
    "    df_daily[\"Datetime\"] = pd.to_datetime(df_daily['date'])\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        df_daily = add_nice_to_haves(df_daily, 'daily')\n",
    "\n",
    "    counter = 1\n",
    "    for station in df_daily['stid'].unique():\n",
    "        station_csv = df_daily.loc[df_daily['stid'] == station]\n",
    "        station_csv.to_csv('../data/promice/new_promice/daily_data/{num}_{station}.csv'.format(num=counter,station=station))\n",
    "        counter += 1 \n",
    "\n",
    "    print(\"FINISHED PROCESSING HOURLY DATA\")\n",
    "    return df_daily\n",
    "\n",
    "###########################################################################################\n",
    "### Helper functions\n",
    "\n",
    "def add_station_info(df):\n",
    "    station = pd.read_csv('../data/promice/new_promice/AWS_station_locations.csv')\n",
    "    station.rename(columns={'timestamp':'station_location_timestamp'})\n",
    "    df.merge(station, how='left',on=['stid','stid'])\n",
    "    return df\n",
    "\n",
    "# helper functions for adding metadata like season, day, month, year, date, DayOfYear and DayOfCentury\n",
    "def add_nice_to_haves(df, period):\n",
    "    def add_season(df):\n",
    "        seasons = {\n",
    "            1: \"Winter\",\n",
    "            2: \"Winter\",\n",
    "            3: \"Spring\",\n",
    "            4: \"Spring\",\n",
    "            5: \"Spring\",\n",
    "            6: \"Summer\",\n",
    "            7: \"Summer\",\n",
    "            8: \"Summer\",\n",
    "            9: \"Autumn\",\n",
    "            10: \"Autumn\",\n",
    "            11: \"Autumn\",\n",
    "            12: \"Winter\",\n",
    "        }\n",
    "\n",
    "        # Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "        df[\"season\"] = df['Datetime'].dt.month.map(seasons)\n",
    "        return df\n",
    "\n",
    "    def add_common(df):\n",
    "        df[\"year\"] = df['Datetime'].dt.strftime(\"%Y\")\n",
    "        df[\"month\"] = df['Datetime'].dt.strftime(\"%B\")\n",
    "        df[\"date\"] = df['Datetime'].dt.date\n",
    "        df['DayOfYear'] = df['Datetime'].dt.dayofyear \n",
    "        df['DayOfCentury'] = df['Datetime'].dt.dayofyear+365*(df['Datetime'].dt.year-1)\n",
    "        df = add_season(df)\n",
    "        return df\n",
    "        \n",
    "    def add_day(df):\n",
    "        # Add day column to dataframe\n",
    "        df[\"day\"] = df['Datetime'].dt.strftime(\"%d\")\n",
    "        return df\n",
    "\n",
    "    def add_hour(df):\n",
    "        # Add hour column to dataframe\n",
    "        df[\"hour\"] = df['Datetime'].dt.strftime(\"%h\")\n",
    "        return df\n",
    "\n",
    "    if period == 'hourly':\n",
    "        df = add_hour(\n",
    "            add_day(\n",
    "            add_common(df)))\n",
    "\n",
    "    elif period == 'daily':\n",
    "        df = add_day(\n",
    "            add_common(df))\n",
    "    elif period == 'monthly':\n",
    "        df = add_common(df)\n",
    "    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading csvs\n",
      "FINISHED PROCESSING HOURLY DATA\n"
     ]
    }
   ],
   "source": [
    "os.chdir('c:\\\\Users\\\\mabj16ac\\\\Desktop\\\\Thesis\\\\GEUS-Master-Thesis\\\\scripts')\n",
    "\n",
    "#header = pd.read_csv('../metadata/promice_header.csv', sep = \";\")\n",
    "#df_hourly = df_hourly.rename(columns = header.set_index('standard_name')['long_name'])\n",
    "#df_hourly = process_hourly_data(add_meta_data=True)\n",
    "#df_hourly = df_hourly.rename(columns = header.set_index('standard_name')['long_name'])\n",
    "\n",
    "#df_hourly = process_hourly_data(directory=\"../data/new_promice/all_promice_data.parquet.gzip\")\n",
    "############################################\n",
    "\n",
    "df_daily = process_daily_data(add_meta_data=True)\n",
    "#df_daily = df_daily.rename(columns = header.set_index('standard_name')['long_name'])\n",
    "\n",
    "#df_daily = process_daily_data(directory=\"../data/new_promice/all_promice_data_hourly.parquet.gzip\", add_meta_data=True)\n",
    "\n",
    "############################################\n",
    "#df_monthly = process_monthly_data(dataframe=df_daily, add_meta_data=True)\n",
    "#df_monthly = df_monthly.rename(columns = header.set_index('standard_name')['long_name'])\n",
    "\n",
    "#df_monthly = process_monthly_data(directory='..\\\\Data\\\\new_promice\\\\all_promice_data_daily.parquet.gzip', add_meta_data=True)\n",
    "\n",
    "### helper function to add station information such as # of booms, location and classification\n",
    "\n",
    "# df_hourly = add_station_info(df_hourly)\n",
    "# df_daily = add_station_info(df_daily)\n",
    "# df_monthly = add_station_info(df_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promice_pull(directory='/data/promice/new_promice/daily_data'):\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Combine all CSV files into a single DataFrame\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(os.path.join(directory, f), index_col=False)\n",
    "        df.insert(0, 'stid', f[:-7])\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\mabj16ac\\Desktop\\Thesis\\GEUS-Master-Thesis\\data\\promice\\new_promice\\hourly_data\n",
    "null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "print(\"null columns:\\n\", null_columns)\n",
    "print(\"flag_columns:\\n\", flag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zyy-tj5MR42i"
   },
   "source": [
    "Promice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BkQ-3yLP-37W"
   },
   "outputs": [],
   "source": [
    "!wget -r -e robots=off -nH --cut-dirs=3 --content-disposition \"https://dataverse.geus.dk/api/datasets/:persistentId/dirindex?persistentId=doi:10.22008/FK2/8SS7EW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CWscWKUJxkCA"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Redundant\n",
    "\n",
    "def process_monthly_data(dataframe=None, directory=\"\", add_meta_data=False):\n",
    "\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        df = pd.read_parquet(directory)\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "\n",
    "    df['month_year'] = df['Datetime'].dt.to_period('M')\n",
    "    # Define the date column that you want to group by (replace \"date_column\" with the name of your column)\n",
    "    min_values_per_month = 24\n",
    "\n",
    "    # Create a new column with the month and year of the date column\n",
    "\n",
    "    # Group the data by weather station and date\n",
    "    grouped = df.groupby(['stid','month_year'])\n",
    "\n",
    "    # Specify columns containing numerical values to be averaged\n",
    "    columns_to_average = (df\n",
    "                .select_dtypes(exclude=['object'])\n",
    "                .drop(columns=['Datetime', 'DayOfYear', 'DayOfCentury'])\n",
    "                .columns\n",
    "    )\n",
    "\n",
    "    # Calculate the number of non-NaN values for each variable within each group\n",
    "    valid_date_observations = grouped[columns_to_average].apply(lambda x: x.notna().sum() <= min_values_per_month)\n",
    "\n",
    "    # Calculate average per day per station\n",
    "    df_filtered = grouped[columns_to_average].mean()\n",
    "\n",
    "    #Remove means with less than 20 observations per day\n",
    "    df_masked = df_filtered.mask(valid_date_observations, np.nan)\n",
    "\n",
    "    df_monthly = df_masked.reset_index().copy()\n",
    "\n",
    "    df_monthly[\"Datetime\"] = pd.to_datetime(df_monthly['month_year'].astype(str) + '-01')\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        df_monthly = add_nice_to_haves(df_monthly, 'monthly')\n",
    "\n",
    "    #display(df_monthly)\n",
    "\n",
    "    # Convert the DataFrame to a compressed Parquet file\n",
    "    output_file = \"..\\\\Data\\\\new_promice\\\\all_promice_data_monthly.parquet.gzip\"\n",
    "    df_monthly.to_parquet(output_file, compression='gzip', engine='pyarrow')\n",
    "\n",
    "    print('FINISHED PROCESSING MONTHLY DATA')\n",
    "    return df_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vwZWPw1I_YPn"
   },
   "outputs": [],
   "source": [
    "# save daily data \n",
    "# Get the files from the path provided in the OP\n",
    "files = Path(path).glob('*_day_v03.txt')  # .rglob to get subdirectories\n",
    "\n",
    "dfs = list()\n",
    "for f in files:\n",
    "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
    "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
    "    data['file'] = f.stem\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = df.replace(-999, np.nan)\n",
    "\n",
    "df[\"Month\"] = df[\"MonthOfYear\"]\n",
    "df[\"Day\"] = df[\"DayOfMonth\"]\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]], format='%Y%m%d')\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df.to_parquet(\"data/promice_daily.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_daily.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bDnLS8bd_a3y"
   },
   "outputs": [],
   "source": [
    "# save monthly data \n",
    "# Get the files from the path provided in the OP\n",
    "files = Path(path).glob('*_month_v03.txt')  # .rglob to get subdirectories\n",
    "\n",
    "dfs = list()\n",
    "for f in files:\n",
    "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
    "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
    "    data['file'] = f.stem\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = df.replace(-999, np.nan)\n",
    "\n",
    "df[\"Month\"] = df[\"MonthOfYear\"]\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(['{}-{}-01'.format(y, m) for y, m in zip(df.Year, df.Month)])\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df.to_parquet(\"data/promice_monthly.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_monthly.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
