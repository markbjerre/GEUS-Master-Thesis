{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLqu3qB_6MQ0"
   },
   "source": [
    "GC Net Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from pyarrow) (1.24.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/Users/nilsfulde/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "iFn7hTObSpj6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/GEUS-PROMICE/pyNEAD.git\n",
      "  Cloning https://github.com/GEUS-PROMICE/pyNEAD.git to c:\\users\\mabj16ac\\appdata\\local\\temp\\4\\pip-req-build-1zu17gro\n",
      "  Resolved https://github.com/GEUS-PROMICE/pyNEAD.git to commit d811220c2d048d84d4612b045880d6ca914c39a4\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from nead==0.0.0) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from nead==0.0.0) (1.4.4)\n",
      "Requirement already satisfied: xarray in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from nead==0.0.0) (0.20.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from pandas->nead==0.0.0) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from pandas->nead==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mabj16ac\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->nead==0.0.0) (1.16.0)\n",
      "Building wheels for collected packages: nead\n",
      "  Building wheel for nead (setup.py): started\n",
      "  Building wheel for nead (setup.py): finished with status 'done'\n",
      "  Created wheel for nead: filename=nead-0.0.0-py2.py3-none-any.whl size=10663 sha256=4736ab4fc8e3fe62dc67c2c15b6c58ff60aed7a9afae1d2942330122de5b691b\n",
      "  Stored in directory: C:\\Users\\mabj16ac\\AppData\\Local\\Temp\\4\\pip-ephem-wheel-cache-zymeu2hk\\wheels\\7e\\b5\\9c\\bf1fb23ef32c0fd131d467e2ee10e1bc483361a4dba16b3600\n",
      "Successfully built nead\n",
      "Installing collected packages: nead\n",
      "Successfully installed nead-0.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/GEUS-PROMICE/pyNEAD.git 'C:\\Users\\mabj16ac\\AppData\\Local\\Temp\\4\\pip-req-build-1zu17gro'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/GEUS-PROMICE/pyNEAD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nilsfulde/Desktop/Master_Thesis/scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#os.chdir('c:\\\\Users\\\\mabj16ac\\\\Desktop\\\\Thesis\\\\GEUS-Master-Thesis\\\\scripts')\n",
    "os.chdir('/Users/nilsfulde/Desktop/Master_Thesis/scripts')\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba5Hy-lHR2Jr",
    "outputId": "22868e77-0da1-4934-9b33-e66e47e507c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mabj16ac\\Desktop\\Thesis\\GEUS-Master-Thesis\n",
      "Overwritting existing data in \"/data\"\n",
      "Downloading daily data...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "# Latest L1 data: https://github.com/GEUS-Glaciology-and-Climate/GC-Net-level-1-data-processing/tree/main/L1\n",
    "# API contents of latest L1 data (raw URLs etc.): https://api.github.com/repositories/319306521/contents/L1\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "#os.chdir('C:\\\\Users\\\\nifu18ab\\\\Desktop\\\\GEUS-Master-Thesis')\n",
    "os.chdir('c:\\\\Users\\\\mabj16ac\\\\Desktop\\\\Thesis\\\\GEUS-Master-Thesis\\\\scripts')\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"data\")\n",
    "    os.mkdir(\"data/gc_net/raw_data/data_daily\")\n",
    "    os.mkdir(\"data/gc_net/raw_data/data_hourly\")\n",
    "except:\n",
    "    print('Overwritting existing data in \"/data\"')\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading daily data...\\r\")\n",
    "\n",
    "\n",
    "# xargs -n 1 curl --silent -O --output-dir data_daily < ../metadata/urls_1.txt\n",
    "for url in open(\"metadata/urls_1.txt\"):\n",
    "    # Split on the rightmost / and take everything on the right side of that\n",
    "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
    "    # Strip /n at the end of filename\n",
    "    name = name.strip()\n",
    "    # Combine the name and the downloads directory to get the local filename\n",
    "    filename = os.path.join(\"data/gc_net/raw_data/data_daily\", name)\n",
    "\n",
    "    # Download the file if it does not exist\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exKNv5IVbMx-",
    "outputId": "3f0f0b01-578c-4d40-940c-ecfd2547209b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hourly data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading hourly data...\\r\")\n",
    "# xargs -n 1 curl --silent -O --output-dir data_hourly < ../metadata/urls_2.txt\n",
    "for url in open(\"metadata/urls_2.txt\"):\n",
    "    # Split on the rightmost / and take everything on the right side of that\n",
    "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
    "    # Strip /n at the end of filename\n",
    "    name = name.strip()\n",
    "    # Combine the name and the downloads directory to get the local filename\n",
    "    filename = os.path.join(\"data/gc_net/raw_data/data_hourly\", name)\n",
    "\n",
    "    # Download the file if it does not exist\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nead\n",
    "\n",
    "def process_gcnet_daily():\n",
    "    print('PROCESSING GC_NET DAILY...')\n",
    "    # Convert NEAD files to Pandas dataframes\n",
    "    station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
    "    dfs_daily = []\n",
    "\n",
    "    for name, ID in zip(station.Name, station.ID):\n",
    "        format_name = name.replace(\" \", \"\")\n",
    "        files = \"data/gc_net/raw_data/data_daily/\" + str(ID).zfill(2) + \"-\" + format_name + \"_daily.csv\"\n",
    "        ds_daily = nead.read(files, index_col=0)\n",
    "        df_daily = ds_daily.to_dataframe()\n",
    "        df_daily.insert(\n",
    "            loc=0, column=\"station_name\", value=name\n",
    "        )  # Add station_name column to each dataframe\n",
    "        dfs_daily.append(df_daily)\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    df_daily = pd.concat(dfs_daily).sort_index()\n",
    "\n",
    "    # Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
    "    # null_columns = df_daily.columns[df_daily.isnull().all()]\n",
    "    # flag_columns = df_daily.filter(regex=\"flag$\").columns\n",
    "    # print(null_columns)\n",
    "    # print(flag_columns)\n",
    "\n",
    "    df_daily = df_daily.drop(\n",
    "        columns=[\n",
    "            \"OSWR_max\",\n",
    "            \"HW2_adj_flag\",\n",
    "            \"P_adj_flag\",\n",
    "            \"HW1_adj_flag\",\n",
    "            \"OSWR_adj_flag\",\n",
    "            \"HS1_adj_flag\",\n",
    "            \"HS2_adj_flag\",\n",
    "            \"TA3_adj_flag\",\n",
    "            \"TA4_adj_flag\",\n",
    "            \"DW1_adj_flag\",\n",
    "        ]\n",
    "    )\n",
    "    # Add season column to dataframe\n",
    "    seasons = {\n",
    "        1: \"Winter\",\n",
    "        2: \"Winter\",\n",
    "        3: \"Spring\",\n",
    "        4: \"Spring\",\n",
    "        5: \"Spring\",\n",
    "        6: \"Summer\",\n",
    "        7: \"Summer\",\n",
    "        8: \"Summer\",\n",
    "        9: \"Autumn\",\n",
    "        10: \"Autumn\",\n",
    "        11: \"Autumn\",\n",
    "        12: \"Winter\",\n",
    "    }\n",
    "\n",
    "    # Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "    df_daily[\"season\"] = df_daily.index.month.map(seasons)\n",
    "\n",
    "    # Add year column to dataframe\n",
    "    df_daily[\"year\"] = df_daily.index.strftime(\"%Y\")\n",
    "\n",
    "    # Add month column to dataframe\n",
    "    df_daily[\"month\"] = df_daily.index.strftime(\"%B\")\n",
    "\n",
    "    # Rename Index Column to Datetime\n",
    "    df_daily = df_daily.reset_index(inplace=False)\n",
    "    df_daily = df_daily.rename(columns={'timestamp': 'Datetime'}, inplace=False) \n",
    "\n",
    "    # Rename 'station_name' to file\n",
    "    df_daily = df_daily.rename(columns={'station_name': 'file'}, inplace=False)\n",
    "\n",
    "    # Add Day of Year & Day of Century \n",
    "    df_daily['DayOfYear'] = df_daily['Datetime'].dt.dayofyear \n",
    "    df_daily['DayOfCentury'] = df_daily['Datetime'].dt.dayofyear+365*(df_daily['Datetime'].dt.year-1)\n",
    "\n",
    "\n",
    "    # # Add day column to dataframe\n",
    "    # df_daily[\"day\"] = df_daily.index.strftime(\"%d\")\n",
    "\n",
    "    # Add hour column to dataframe\n",
    "    # df_daily[\"hour\"] = df_daily.index.strftime(\"%h\")\n",
    "\n",
    "    # Change column headers\n",
    "    #header = pd.read_csv('/content/drive/MyDrive/Master_Thesis/metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "    header = pd.read_csv('metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "    df_daily = df_daily.rename(columns = header.set_index('fields')['display_description'])\n",
    "\n",
    "    print('SAVING PROCESSED GC_NET_DAILY TO CSVs...')\n",
    "    counter = 1\n",
    "    for station in df_daily['file'].unique():\n",
    "        station_csv = df_daily.loc[df_daily['file'] == station]\n",
    "        station_csv.to_csv('data/gc_net/daily_data/{num}_{station}.csv'.format(num=counter,station=station))\n",
    "        counter += 1 \n",
    "    print('FINISHED SAVING GC_NET DAILY TO CSVs')\n",
    "    return df_daily\n",
    "\n",
    "def process_gcnet_hourly():\n",
    "    print('PROCESSING GC_NET HOURLY...')\n",
    "    station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
    "    dfs_hourly = []\n",
    "\n",
    "    for name, ID in zip(station.Name, station.ID):\n",
    "        format_name = name.replace(\" \", \"\")\n",
    "        files = \"data/gc_net/raw_data/data_hourly/\" + str(ID).zfill(2) + \"-\" + format_name + \".csv\"\n",
    "        ds_hourly = nead.read(files, index_col=0)\n",
    "        df_hourly = ds_hourly.to_dataframe()\n",
    "        df_hourly.insert(\n",
    "            loc=0, column=\"station_name\", value=name\n",
    "        )  # Add station_name column to each dataframe\n",
    "        dfs_hourly.append(df_hourly)\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    df_hourly = pd.concat(dfs_hourly).sort_index()\n",
    "\n",
    "    # Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
    "    # null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "    # flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "    # print(null_columns)\n",
    "    # print(flag_columns)\n",
    "\n",
    "    df_hourly = df_hourly.drop(\n",
    "        columns=[\n",
    "            \"OSWR_max\",\n",
    "            \"HW2_adj_flag\",\n",
    "            \"P_adj_flag\",\n",
    "            \"HW1_adj_flag\",\n",
    "            \"OSWR_adj_flag\",\n",
    "            \"HS1_adj_flag\",\n",
    "            \"HS2_adj_flag\",\n",
    "            \"TA3_adj_flag\",\n",
    "            \"TA4_adj_flag\",\n",
    "            \"DW1_adj_flag\",\n",
    "        ]\n",
    "    )\n",
    "    # Add year column to dataframe\n",
    "    df_hourly[\"year\"] = df_hourly.index.strftime(\"%Y\")\n",
    "\n",
    "    # Add month column to dataframe\n",
    "    df_hourly[\"month\"] = df_hourly.index.strftime(\"%B\")\n",
    "\n",
    "    # Add season column to dataframe\n",
    "    seasons = {\n",
    "        1: \"Winter\",\n",
    "        2: \"Winter\",\n",
    "        3: \"Spring\",\n",
    "        4: \"Spring\",\n",
    "        5: \"Spring\",\n",
    "        6: \"Summer\",\n",
    "        7: \"Summer\",\n",
    "        8: \"Summer\",\n",
    "        9: \"Autumn\",\n",
    "        10: \"Autumn\",\n",
    "        11: \"Autumn\",\n",
    "        12: \"Winter\",\n",
    "    }\n",
    "\n",
    "    # Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "    df_hourly[\"season\"] = df_hourly.index.month.map(seasons)\n",
    "\n",
    "\n",
    "    #Rename Index Column to Datetime\n",
    "    df_hourly = df_hourly.reset_index(inplace=False)\n",
    "    df_hourly = df_hourly.rename(columns={'timestamp': 'Datetime'}, inplace=False) \n",
    "\n",
    "    # Rename 'station_name' to file\n",
    "    df_hourly = df_hourly.rename(columns={'station_name': 'file'}, inplace=False)\n",
    "\n",
    "    # Add Day of Year & Day of Century \n",
    "    df_hourly['DayOfYear'] = df_hourly['Datetime'].dt.dayofyear \n",
    "    df_hourly['DayOfCentury'] = df_hourly['Datetime'].dt.dayofyear+365*(df_hourly['Datetime'].dt.year-1)\n",
    "\n",
    "\n",
    "    # # Add day column to dataframe\n",
    "    # df_hourly[\"day\"] = df_hourly.index.strftime(\"%d\")\n",
    "\n",
    "    # # Add hour column to dataframe\n",
    "    # df_hourly[\"hour\"] = df_hourly.index.strftime(\"%h\")\n",
    "\n",
    "\n",
    "    #header = pd.read_csv('/content/drive/MyDrive/Master_Thesis/metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "    header = pd.read_csv('metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "    df_hourly = df_hourly.rename(columns = header.set_index('fields')['display_description'])\n",
    "\n",
    "    print('SAVING PROCESSED GC_NET HOURLY TO CSVs')\n",
    "    counter = 1\n",
    "    for station in df_hourly['file'].unique():\n",
    "        station_csv = df_hourly.loc[df_hourly['file'] == station]\n",
    "        station_csv.to_csv('data/gc_net/hourly_data/{num}_{station}.csv'.format(num=counter,station=station))\n",
    "        counter += 1 \n",
    "    print('FINISHED SAVING GC_NET HOURLY TO CSVs')\n",
    "    \n",
    "\n",
    "    # Save dataframe as parquet file\n",
    "    #df_hourly.to_parquet(\"data/df_hourly.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>file</th>\n",
       "      <th>shortwave_incoming_radiation</th>\n",
       "      <th>shortwave_outgoing_radiation</th>\n",
       "      <th>net_radiation</th>\n",
       "      <th>air_temperature_1</th>\n",
       "      <th>air_temperature_1_max</th>\n",
       "      <th>air_temperature_1_min</th>\n",
       "      <th>air_temperature_cs500_air1</th>\n",
       "      <th>air_temperature_cs500_air2</th>\n",
       "      <th>...</th>\n",
       "      <th>incoming_uv_radiation</th>\n",
       "      <th>incoming_longwave_radiation</th>\n",
       "      <th>surface_temperature_1</th>\n",
       "      <th>surface_temperature_2</th>\n",
       "      <th>net_radiation_maximum</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>DayOfCentury</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-06-01 01:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-06-01 02:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-06-01 03:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-06-01 04:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-06-01 05:00:00</td>\n",
       "      <td>Swiss Camp 10m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>Summer</td>\n",
       "      <td>152</td>\n",
       "      <td>726137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967230</th>\n",
       "      <td>2022-10-16 22:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>289</td>\n",
       "      <td>737954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967231</th>\n",
       "      <td>2022-10-16 23:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>289</td>\n",
       "      <td>737954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967232</th>\n",
       "      <td>2022-10-17 00:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>290</td>\n",
       "      <td>737955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967233</th>\n",
       "      <td>2022-10-17 01:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>290</td>\n",
       "      <td>737955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967234</th>\n",
       "      <td>2022-10-17 02:00:00</td>\n",
       "      <td>Humboldt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>October</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>290</td>\n",
       "      <td>737955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3967235 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime            file  shortwave_incoming_radiation  \\\n",
       "0       1990-06-01 01:00:00  Swiss Camp 10m                           NaN   \n",
       "1       1990-06-01 02:00:00  Swiss Camp 10m                           NaN   \n",
       "2       1990-06-01 03:00:00  Swiss Camp 10m                           NaN   \n",
       "3       1990-06-01 04:00:00  Swiss Camp 10m                           NaN   \n",
       "4       1990-06-01 05:00:00  Swiss Camp 10m                           NaN   \n",
       "...                     ...             ...                           ...   \n",
       "3967230 2022-10-16 22:00:00        Humboldt                           NaN   \n",
       "3967231 2022-10-16 23:00:00        Humboldt                           NaN   \n",
       "3967232 2022-10-17 00:00:00        Humboldt                           NaN   \n",
       "3967233 2022-10-17 01:00:00        Humboldt                           NaN   \n",
       "3967234 2022-10-17 02:00:00        Humboldt                           NaN   \n",
       "\n",
       "         shortwave_outgoing_radiation  net_radiation  air_temperature_1  \\\n",
       "0                                 NaN            NaN               1.72   \n",
       "1                                 NaN            NaN               1.42   \n",
       "2                                 NaN            NaN               0.88   \n",
       "3                                 NaN            NaN              -0.13   \n",
       "4                                 NaN            NaN              -1.00   \n",
       "...                               ...            ...                ...   \n",
       "3967230                           NaN            NaN                NaN   \n",
       "3967231                           NaN            NaN                NaN   \n",
       "3967232                           NaN            NaN                NaN   \n",
       "3967233                           NaN            NaN                NaN   \n",
       "3967234                           NaN            NaN                NaN   \n",
       "\n",
       "         air_temperature_1_max  air_temperature_1_min  \\\n",
       "0                          NaN                    NaN   \n",
       "1                          NaN                    NaN   \n",
       "2                          NaN                    NaN   \n",
       "3                          NaN                    NaN   \n",
       "4                          NaN                    NaN   \n",
       "...                        ...                    ...   \n",
       "3967230                    NaN                    NaN   \n",
       "3967231                    NaN                    NaN   \n",
       "3967232                    NaN                    NaN   \n",
       "3967233                    NaN                    NaN   \n",
       "3967234                    NaN                    NaN   \n",
       "\n",
       "         air_temperature_cs500_air1  air_temperature_cs500_air2  ...  \\\n",
       "0                               NaN                         NaN  ...   \n",
       "1                               NaN                         NaN  ...   \n",
       "2                               NaN                         NaN  ...   \n",
       "3                               NaN                         NaN  ...   \n",
       "4                               NaN                         NaN  ...   \n",
       "...                             ...                         ...  ...   \n",
       "3967230                         NaN                         NaN  ...   \n",
       "3967231                         NaN                         NaN  ...   \n",
       "3967232                         NaN                         NaN  ...   \n",
       "3967233                         NaN                         NaN  ...   \n",
       "3967234                         NaN                         NaN  ...   \n",
       "\n",
       "         incoming_uv_radiation  incoming_longwave_radiation  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "...                        ...                          ...   \n",
       "3967230                    NaN                          NaN   \n",
       "3967231                    NaN                          NaN   \n",
       "3967232                    NaN                          NaN   \n",
       "3967233                    NaN                          NaN   \n",
       "3967234                    NaN                          NaN   \n",
       "\n",
       "         surface_temperature_1  surface_temperature_2  net_radiation_maximum  \\\n",
       "0                          NaN                    NaN                    NaN   \n",
       "1                          NaN                    NaN                    NaN   \n",
       "2                          NaN                    NaN                    NaN   \n",
       "3                          NaN                    NaN                    NaN   \n",
       "4                          NaN                    NaN                    NaN   \n",
       "...                        ...                    ...                    ...   \n",
       "3967230                    NaN                    NaN                    NaN   \n",
       "3967231                    NaN                    NaN                    NaN   \n",
       "3967232                    NaN                    NaN                    NaN   \n",
       "3967233                    NaN                    NaN                    NaN   \n",
       "3967234                    NaN                    NaN                    NaN   \n",
       "\n",
       "         year    month  season  DayOfYear  DayOfCentury  \n",
       "0        1990     June  Summer        152        726137  \n",
       "1        1990     June  Summer        152        726137  \n",
       "2        1990     June  Summer        152        726137  \n",
       "3        1990     June  Summer        152        726137  \n",
       "4        1990     June  Summer        152        726137  \n",
       "...       ...      ...     ...        ...           ...  \n",
       "3967230  2022  October  Autumn        289        737954  \n",
       "3967231  2022  October  Autumn        289        737954  \n",
       "3967232  2022  October  Autumn        290        737955  \n",
       "3967233  2022  October  Autumn        290        737955  \n",
       "3967234  2022  October  Autumn        290        737955  \n",
       "\n",
       "[3967235 rows x 65 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_gcnet_daily()\n",
    "process_gcnet_hourly()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Promice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def process_hourly_data(csv_directory=\"../../PROMICE-AWS-toolbox/out/test\", add_meta_data=False):\n",
    "        # List all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Combine all CSV files into a single DataFrame\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(os.path.join(csv_directory, f), index_col=False)\n",
    "        df.insert(0, 'stid', f[:-7])\n",
    "        dfs.append(df)\n",
    "    df_hourly = pd.concat(dfs)\n",
    "\n",
    "    #read metadata from promice repository\n",
    "    station = pd.read_csv('../../PROMICE-AWS-toolbox/metadata/AWS_station_locations.csv', index_col=False)\n",
    "    station.to_csv('../data/promice/new_promice/AWS_station_locations.csv', index=False)\n",
    "\n",
    "\n",
    "    #display(output station and columns summary)\n",
    "    print('Stations loaded:')\n",
    "    display(df_hourly['stid'].unique())\n",
    "    print('columns in dataset:')\n",
    "    print(list(df_hourly.columns))\n",
    "    print(\"FINISHED LOADING CSV's\")\n",
    "\n",
    "    # Add year column to dataframe\n",
    "    df_hourly[\"Datetime\"] = pd.to_datetime(df_hourly.time)\n",
    "    df_hourly[\"Datetime\"] = pd.to_datetime(df_hourly[\"Datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    #Rename Index Column to Datetime\n",
    "    df_hourly = df_hourly.reset_index(inplace=False)\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        add_nice_to_haves(df_hourly, 'hourly')\n",
    "\n",
    "    header = pd.read_csv('../metadata/promice_header.csv', sep = \";\")\n",
    "    df_hourly = df_hourly.rename(columns = header.set_index('standard_name')['units']) #Units because headers are shifted\n",
    "    print(df_hourly.columns)\n",
    "    \n",
    "    # This bit to detect Ablation\n",
    "    ablation = pd.DataFrame() # Create an empty dataframe to store the filtered and modified dataframes\n",
    "    \n",
    "    # The next lines to format Datetime Correctly, assign it as index while still keeping it as a column\n",
    "    df_hourly['index'] = df_hourly.index\n",
    "    df_hourly[\"Datetime\"] = pd.to_datetime(df_hourly[\"Datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_hourly = df_hourly.rename(columns={'Datetime': 'Index_Datetime'})\n",
    "    df_hourly = df_hourly.set_index(['Index_Datetime'])\n",
    "    df_hourly['Datetime'] = df_hourly.index\n",
    "    df_hourly[\"Datetime\"] = pd.to_datetime(df_hourly[\"Datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    for station in df_hourly[\"stid\"].unique():\n",
    "        print(\"Calculating Ablation\" , station)\n",
    "        df_stid = df_hourly[df_hourly['stid'] == station].copy()\n",
    "\n",
    "        # defining ice ablation period   \n",
    "        smoothed_PT =  df_stid['Depth of pressure transducer in ice - corrected'].interpolate(limit=72).rolling('14D', min_periods=1).mean().shift(-7*14/2, freq='h')\n",
    "        smoothed_PT = smoothed_PT.rolling('14D', min_periods=1).mean().shift(-7*14/2, freq='h')\n",
    "\n",
    "        threshold_ablation = -0.0002 # Modify value if needed\n",
    "        ind_ablation = np.logical_and(smoothed_PT.diff().values < threshold_ablation, \n",
    "                                      np.isin(smoothed_PT.diff().index.month, [6, 7, 8, 9]))\n",
    "\n",
    "        ind_ablation = np.concatenate((ind_ablation[4*24:], np.repeat(ind_ablation[-(4*24):-(4*24-1)], 4*24)))\n",
    "\n",
    "        df_stid['Ablation'] = ind_ablation.tolist()\n",
    "        df_stid['stid'] = station # Add a column for the current station\n",
    "        ablation = ablation.append(df_stid[['Datetime', 'stid', 'Ablation']])\n",
    "    \n",
    "    \n",
    "    df_hourly = df_hourly.set_index(['index'])\n",
    "    \n",
    "    # Merge the new column with the original dataframe based on datetime and station\n",
    "    df_hourly = pd.merge(df_hourly, ablation, on=['Datetime', 'stid'], how='left')\n",
    "        \n",
    "    # This bit to add a beginning, middle & end of melting season attribute\n",
    "    beginning = 0.15\n",
    "    end = 0.85\n",
    "\n",
    "    melting_season = pd.DataFrame() # Create an empty dataframe to store the filtered and modified dataframes\n",
    "\n",
    "\n",
    "    for station in df_hourly[\"stid\"].unique():\n",
    "\n",
    "        print(\"Calculating Melting Season\" , station)\n",
    "        df_stid = df_hourly[df_hourly['stid'] == station].copy() \n",
    "\n",
    "        # Iterate over each year\n",
    "        for year in df_stid['Datetime'].dt.year.unique():\n",
    "\n",
    "            print(\"   Calculating\" , year)\n",
    "\n",
    "            # Get the dataframe for the current year\n",
    "            year_df = df_stid[df_stid['Datetime'].dt.year == year]\n",
    "\n",
    "            # Determine the threshold values for the beginning and end of the melting season\n",
    "            ablation_df = year_df[year_df['Ablation'] == True]\n",
    "            lower_threshold = ablation_df['Datetime'].dt.dayofyear.quantile(beginning)\n",
    "            upper_threshold = ablation_df['Datetime'].dt.dayofyear.quantile(end)\n",
    "\n",
    "            # Set the Melting Season value for each row in the current year\n",
    "            for index, row in year_df.iterrows():\n",
    "\n",
    "                if row['Ablation'] == True:\n",
    "                    if row['Datetime'].dayofyear <= lower_threshold:\n",
    "                        df_stid.loc[index, 'Melting Season'] = 'beginning'\n",
    "                    elif row['Datetime'].dayofyear >= upper_threshold:\n",
    "                        df_stid.loc[index, 'Melting Season'] = 'end'\n",
    "                    else:\n",
    "                        df_stid.loc[index, 'Melting Season'] = 'middle'\n",
    "                        \n",
    "                elif (row['Datetime'] - pd.Timedelta(days=30)).dayofyear <= lower_threshold:\n",
    "                    df_stid.loc[index, 'Melting Season'] = 'pre'\n",
    "                else:\n",
    "                    df_stid.loc[index, 'Melting Season'] = ''\n",
    "\n",
    "        df_stid['stid'] = station # Add a column for the current station\n",
    "        melting_season = melting_season.append(df_stid[['Datetime', 'stid', 'Melting Season']])\n",
    "\n",
    "    # Merge the new column with the original dataframe based on datetime and station\n",
    "    df_hourly = pd.merge(df_hourly, melting_season, on=['Datetime', 'stid'], how='left')\n",
    "    \n",
    "    # Fill NAN values\n",
    "    df_hourly['Melting Season'] = df_hourly['Melting Season'].fillna('')\n",
    "    \n",
    "    #This bit to add delta column \n",
    "    delta = pd.DataFrame() # Create an empty dataframe to store the filtered and modified dataframes\n",
    "    df_hourly['Date'] = df_hourly['Datetime'].dt.date # Date to merge on\n",
    "    \n",
    "    for station in df_hourly[\"stid\"].unique():\n",
    "        print(\"Calculating\", station)\n",
    "        df_stid = df_hourly[df_hourly['stid'] == station].copy()\n",
    "        df_stid['Date'] = df_stid['Datetime'].dt.date # extract date component\n",
    "        df_stid['Surface height from combined measurements DELTA'] = df_stid.groupby('Date')['Surface height from combined measurements'].transform(lambda x: (x.iloc[-1] - x.iloc[0]))\n",
    "        #df_stid['Surface height from combined measurements DELTA'] = df_stid['Surface height from combined measurements'].diff()\n",
    "        df_stid['stid'] = station # Add a column for the current station\n",
    "        delta = delta.append(df_stid[['Datetime', 'stid', 'Surface height from combined measurements DELTA']])\n",
    "\n",
    "    # Merge the new column with the original dataframe based on datetime and station\n",
    "    df_hourly = pd.merge(df_hourly, delta, on=['Datetime', 'stid'], how='left')\n",
    "\n",
    "    \n",
    "    # Remove not needed columns\n",
    "    df_hourly = df_hourly[[column for column in df_hourly.columns if column not in exclude_list]]\n",
    "\n",
    "    counter = 1\n",
    "    for station in df_hourly['stid'].unique():\n",
    "        station_csv = df_hourly.loc[df_hourly['stid'] == station]\n",
    "        station_csv.to_csv('../data/promice/preprocessed/hourly/{station}.csv'.format(station=station))\n",
    "        counter += 1 \n",
    "\n",
    "    print(\"FINISHED PROCESSING HOURLY DATA\")\n",
    "    return df_hourly\n",
    "\n",
    "def process_daily_data(dataframe=None, directory=\"../data/promice/preprocessed/hourly\", add_meta_data=False):\n",
    "    \n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        # List all CSV files in the directory\n",
    "        csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "        # Combine all CSV files into a single DataFrame\n",
    "        dfs = []\n",
    "        for f in csv_files:\n",
    "            df = pd.read_csv(os.path.join(directory, f), index_col=False)\n",
    "            dfs.append(df)\n",
    "            df = pd.concat(dfs)\n",
    "        print('finished loading csvs')\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "    \n",
    "    # remove the Unnamed columns...\n",
    "    df = df[[column for column in df.columns if column not in exclude_list]]\n",
    "    \n",
    "        #display(output station and columns summary)\n",
    "    print('Stations loaded:')\n",
    "    display(df['stid'].unique())\n",
    "    print('columns in dataset:')\n",
    "    print(list(df.columns))\n",
    "    print(\"FINISHED LOADING CSV's\")\n",
    "\n",
    "    \n",
    "    # Define the date column that you want to group by (replace \"date_column\" with the name of your column)\n",
    "    min_values_per_day = 20\n",
    "    \n",
    "    # Group the data by weather station and date\n",
    "    #grouped = df.groupby(['stid','date'])\n",
    "    df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], format=\"%Y-%m-%d\")\n",
    "    df['DayOfYear'] = df['Datetime'].dt.dayofyear\n",
    "    df['DayOfCentury'] = ((df['Datetime'].dt.year - 1) * 365 + (df['Datetime'].dt.year - 1) // 4 + df['Datetime'].dt.dayofyear) % 365\n",
    "    \n",
    "    \n",
    "    grouped = df.groupby(['stid','Datetime'])\n",
    "    \n",
    "\n",
    "    # Specify columns containing numerical values to be averaged\n",
    "    columns_to_average = (df\n",
    "                .select_dtypes(exclude=['object'])\n",
    "                #.drop(columns=['index', 'DayOfYear', 'DayOfCentury'])\n",
    "                .drop(columns=['DayOfYear', 'DayOfCentury', 'Datetime'])\n",
    "                .columns\n",
    "    )\n",
    "    \n",
    "    print(\"Columns to average:\")\n",
    "    print(columns_to_average)\n",
    "    \n",
    "    # Calculate the number of non-NaN values for each variable within each group\n",
    "    valid_date_observations = grouped[columns_to_average].apply(lambda x: x.notna().sum() <= min_values_per_day)\n",
    "    filter_excluded_columns = ['Albedo','Depth of pressure transducer in ice - corrected']\n",
    "    valid_date_observations[filter_excluded_columns] = False\n",
    "\n",
    "\n",
    "    # Calculate average per day per station\n",
    "    df_filtered = grouped[columns_to_average].mean()\n",
    "\n",
    "    #Remove means with less than 20 observations per day\n",
    "    df_masked = df_filtered.mask(valid_date_observations, np.nan)\n",
    "\n",
    "    df_daily = df_masked.reset_index().copy()\n",
    "    \n",
    "    print('columns in df_daily:')\n",
    "    print(list(df_daily.columns))\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        df_daily = add_nice_to_haves(df_daily, 'daily')\n",
    "\n",
    "        \n",
    "    #This bit to add delta column \n",
    "    delta = pd.DataFrame() # Create an empty dataframe to store the filtered and modified dataframes\n",
    "\n",
    "    for station in df_daily[\"stid\"].unique():\n",
    "        print(\"Calculating\", station)\n",
    "        df_stid = df_daily[df_daily['stid'] == station].copy()\n",
    "        df_stid['Surface height from combined measurements DELTA'] = df_stid['Surface height from combined measurements'].diff()\n",
    "        df_stid['stid'] = station # Add a column for the current station\n",
    "        delta = delta.append(df_stid[['Datetime', 'stid', 'Surface height from combined measurements DELTA']])\n",
    "\n",
    "    # Merge the new column with the original dataframe based on datetime and station\n",
    "    df_daily = pd.merge(df_daily, delta, on=['Datetime', 'stid'], how='left')\n",
    "    \n",
    "    df_daily = df_daily[[column for column in df_daily.columns if column not in exclude_list]]\n",
    "    \n",
    "    counter = 1\n",
    "    for station in df_daily['stid'].unique():\n",
    "        station_csv = df_daily.loc[df_daily['stid'] == station]\n",
    "        station_csv.to_csv('../data/promice/preprocessed/daily/{station}.csv'.format(station=station))\n",
    "        counter += 1 \n",
    "\n",
    "    print(\"FINISHED PROCESSING HOURLY DATA\")\n",
    "    return df_daily\n",
    "\n",
    "###########################################################################################\n",
    "### Helper functions\n",
    "\n",
    "def add_station_info(df):\n",
    "    station = pd.read_csv('../data/promice/new_promice/AWS_station_locations.csv')\n",
    "    station.rename(columns={'timestamp':'station_location_timestamp'})\n",
    "    df.merge(station, how='left',on=['stid','stid'])\n",
    "    return df\n",
    "\n",
    "# helper functions for adding metadata like season, day, month, year, date, DayOfYear and DayOfCentury\n",
    "def add_nice_to_haves(df, period):\n",
    "    def add_season(df):\n",
    "        seasons = {\n",
    "            1: \"Winter\",\n",
    "            2: \"Winter\",\n",
    "            3: \"Spring\",\n",
    "            4: \"Spring\",\n",
    "            5: \"Spring\",\n",
    "            6: \"Summer\",\n",
    "            7: \"Summer\",\n",
    "            8: \"Summer\",\n",
    "            9: \"Autumn\",\n",
    "            10: \"Autumn\",\n",
    "            11: \"Autumn\",\n",
    "            12: \"Winter\",\n",
    "        }\n",
    "\n",
    "        # Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "        df[\"season\"] = df['Datetime'].dt.month.map(seasons)\n",
    "        return df\n",
    "\n",
    "    def add_common(df):\n",
    "        df[\"year\"] = df['Datetime'].dt.strftime(\"%Y\")\n",
    "        df[\"month\"] = df['Datetime'].dt.strftime(\"%B\")\n",
    "        df[\"date\"] = df['Datetime'].dt.date\n",
    "        df['DayOfYear'] = df['Datetime'].dt.dayofyear \n",
    "        df['DayOfCentury'] = df['Datetime'].dt.dayofyear+365*(df['Datetime'].dt.year-1)\n",
    "        df = add_season(df)\n",
    "        return df\n",
    "        \n",
    "    def add_day(df):\n",
    "        # Add day column to dataframe\n",
    "        df[\"day\"] = df['Datetime'].dt.strftime(\"%d\")\n",
    "        return df\n",
    "\n",
    "    def add_hour(df):\n",
    "        # Add hour column to dataframe\n",
    "        df[\"hour\"] = df['Datetime'].dt.strftime(\"%h\")\n",
    "        return df\n",
    "\n",
    "    if period == 'hourly':\n",
    "        df = add_hour(\n",
    "            add_day(\n",
    "            add_common(df)))\n",
    "\n",
    "    elif period == 'daily':\n",
    "        df = add_day(\n",
    "            add_common(df))\n",
    "    elif period == 'monthly':\n",
    "        df = add_common(df)\n",
    "    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_daily_data(dataframe=None, directory=\"../data/promice/preprocessed/hourly\", add_meta_data=False):\n",
    "    \n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        # List all CSV files in the directory\n",
    "        csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "        # Combine all CSV files into a single DataFrame\n",
    "        dfs = []\n",
    "        for f in csv_files:\n",
    "            df = pd.read_csv(os.path.join(directory, f), index_col=False)\n",
    "            dfs.append(df)\n",
    "            df = pd.concat(dfs)\n",
    "        print('finished loading csvs')\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "    \n",
    "    # remove the Unnamed columns...\n",
    "    df = df[[column for column in df.columns if column not in exclude_list]]\n",
    "    \n",
    "        #display(output station and columns summary)\n",
    "    print('Stations loaded:')\n",
    "    display(df['stid'].unique())\n",
    "    print('columns in dataset:')\n",
    "    print(list(df.columns))\n",
    "    print(\"FINISHED LOADING CSV's\")\n",
    "\n",
    "    # Convert 'Datetime' column to datetime format\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "    # Group by 'stid' and daily frequency\n",
    "    daily_groups = df.groupby(['stid', pd.Grouper(key='Datetime', freq='D')])\n",
    "\n",
    "    # Define aggregation functions\n",
    "    agg_functions = {'Air pressure (upper boom)': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Air temperature (upper boom)': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Relative humidity (upper boom) - corrected': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Specific humidity (upper boom)': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Wind speed (upper boom)': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Wind from direction (upper boom)': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Downwelling shortwave radiation - corrected': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Upwelling shortwave radiation - corrected': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Downwelling longwave radiation': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Upwelling longwave radiation': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Surface temperature': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Latent heat flux (upper boom)': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Sensible heat flux (upper boom)': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Surface height from combined measurements': 'mean',\n",
    "                     'Surface height from combined measurements DELTA': 'mean',\n",
    "                     'Albedo': 'mean',\n",
    "                     'Cloud cover': lambda x: x.mean() if len(x) > 20 else np.nan,\n",
    "                     'Ablation': lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan,\n",
    "                     'Melting Season': lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan}\n",
    "\n",
    "    # Apply aggregation functions to each group\n",
    "    df_daily = daily_groups.agg(agg_functions)\n",
    "\n",
    "    # Remove the multi-level index and reset the index\n",
    "    df_daily.reset_index(inplace=True)\n",
    "\n",
    "    \n",
    "    print('columns in df_daily:')\n",
    "    print(list(df_daily.columns))\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        df_daily = add_nice_to_haves(df_daily, 'daily')\n",
    "\n",
    "    \n",
    "    df_daily = df_daily[[column for column in df_daily.columns if column not in exclude_list]]\n",
    "    \n",
    "    counter = 1\n",
    "    for station in df_daily['stid'].unique():\n",
    "        station_csv = df_daily.loc[df_daily['stid'] == station]\n",
    "        station_csv.to_csv('../data/promice/preprocessed/daily/{station}.csv'.format(station=station))\n",
    "        counter += 1 \n",
    "\n",
    "    print(\"FINISHED PROCESSING HOURLY DATA\")\n",
    "    return df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to include only relevant features\n",
    "exclude_list = [                                                   # 'index', # excluded because of unimportant information\n",
    "                                                                   #  'stid', # excluded because of unimportant information\n",
    "#                                                'Air pressure (upper boom)',\n",
    "#                                             'Air temperature (upper boom)',\n",
    "                                            'Relative humidity (upper boom)', # excluded because of corrected feature\n",
    "#                               'Relative humidity (upper boom) - corrected',\n",
    "#                                           'Specific humidity (upper boom)',\n",
    "#                                                  'Wind speed (upper boom)',\n",
    "#                                         'Wind from direction (upper boom)',\n",
    "                                          'Downwelling shortwave radiation', # excluded because of corrected feature\n",
    "#                              'Downwelling shortwave radiation - corrected',\n",
    "                                            'Upwelling shortwave radiation', # excluded because of corrected feature\n",
    "#                                'Upwelling shortwave radiation - corrected',\n",
    "#                                                                   'Albedo', \n",
    "#                                           'Downwelling longwave radiation',\n",
    "#                                             'Upwelling longwave radiation',\n",
    "#                                                              'Cloud cover',\n",
    "#                                                      'Surface temperature',\n",
    "#                                            'Latent heat flux (upper boom)', \n",
    "#                                          'Sensible heat flux (upper boom)', \n",
    "                                                        'Upper boom height', # excluded because of unimportant information\n",
    "                                                             'Stake height', # excluded due to missing values\n",
    "                                      'Depth of pressure transducer in ice', # excluded due to derived correlation with y\n",
    "                          'Depth of pressure transducer in ice - corrected', # excluded due to derived correlation with y\n",
    "                   'Precipitation (upper boom) (cumulative solid & liquid)', # excluded because of corrected feature\n",
    "       'Precipitation (upper boom) (cumulative solid & liquid) â€“ corrected', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 1', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 2', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 3', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 4', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 5', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 6', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 7', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 8', # excluded due to missing values\n",
    "                                                             'Tilt to east', # excluded because of unimportant information \n",
    "                                                            'Tilt to north', # excluded because of unimportant information\n",
    "                                         'Station rotation from true North', # excluded because of unimportant information\n",
    "                                                                 'Latitude', # excluded because of unimportant information\n",
    "                                                                'Longitude', # excluded because of unimportant information\n",
    "                                                                 'Altitude', # excluded because of unimportant information\n",
    "                                                                 'GPS time', # excluded because of unimportant information \n",
    "                               'Height of EGM96 geoid over WGS84 ellipsoid', # excluded because of unimportant information\n",
    "                                                                  'GeoUnit', # excluded because of unimportant information\n",
    "                             'GPS horizontal dillution of precision (HDOP)', # excluded because of unimportant information\n",
    "                                                 'GPS number of satellites', # excluded because of unimportant information\n",
    "                                                                  'Quality', # excluded because of unimportant information\n",
    "                                                          'Battery voltage', # excluded because of unimportant information\n",
    "                                                                      'nan',\n",
    "                                           'Battery voltage (sample start)', # excluded because of unimportant information\n",
    "                                                 'Fan current (upper boom)', # excluded because of unimportant information\n",
    "                       'Frequency of vibrating wire in precipitation gauge', # excluded because of unimportant information\n",
    "                                                       'Logger temperature', # excluded because of unimportant information\n",
    "                                             'Radiation sensor temperature', # excluded because of unimportant information\n",
    "                                         'latitude from modem (email text)', # excluded because of unimportant information\n",
    "                                        'longitude from modem (email text)', # excluded because of unimportant information\n",
    "                                                         'Surface height 1', # excluded due to derived correlation with y\n",
    "                                                         'Surface height 1', # excluded due to derived correlation with y\n",
    "                                                        'z_surf_1_adj_flag', # excluded due to derived correlation with y\n",
    "                                                        'z_surf_2_adj_flag', # excluded due to derived correlation with y\n",
    "#                                'Surface height from combined measurements', \n",
    "                                   'Depth of ice temperature measurement 1', # excluded because of unimportant information\n",
    "                                   'Depth of ice temperature measurement 2', # excluded because of unimportant information\n",
    "                                   'Depth of ice temperature measurement 3', # excluded because of unimportant information\n",
    "                                   'Depth of ice temperature measurement 4', # excluded because of unimportant information\n",
    "                                   'Depth of ice temperature measurement 5', # excluded because of unimportant information\n",
    "                                   'Depth of ice temperature measurement 6', # excluded because of unimportant information\n",
    "                                   'Depth of ice temperature measurement 7', # excluded because of unimportant information\n",
    "                                   'Depth of ice temperature measurement 8', # excluded because of unimportant information\n",
    "                               'Ice temperature interpolated at 10 m depth', # excluded due to missing values\n",
    "                                                'Air pressure (lower boom)', # excluded due to missing values\n",
    "                                             'Air temperature (lower boom)', # excluded due to missing values\n",
    "                                           'Relative humidity (lower boom)', # excluded due to missing values\n",
    "                               'Relative humidity (lower boom) - corrected', # excluded due to missing values\n",
    "                                           'Specific humidity (lower boom)', # excluded due to missing values\n",
    "                                                  'Wind speed (lower boom)', # excluded due to missing values\n",
    "                                         'Wind from direction (lower boom)', # excluded due to missing values\n",
    "                                            'Latent heat flux (lower boom)', # excluded due to missing values\n",
    "                                          'Sensible heat flux (lower boom)', # excluded due to missing values\n",
    "                                                        'Lower boom height', # excluded due to missing values\n",
    "                   'Precipitation (lower boom) (cumulative solid & liquid)', # excluded due to missing values\n",
    "       'Precipitation (lower boom) (cumulative solid & liquid) â€“ corrected', # excluded due to missing values\n",
    "                                              'Ice temperature at sensor 9', # excluded due to missing values\n",
    "                                             'Ice temperature at sensor 10', # excluded due to missing values\n",
    "                                             'Ice temperature at sensor 11', # excluded due to missing values\n",
    "                                                 'Fan current (lower boom)', # excluded due to missing values\n",
    "                                   'Depth of ice temperature measurement 9', # excluded due to missing values\n",
    "                                  'Depth of ice temperature measurement 10', # excluded due to missing values\n",
    "                                  'Depth of ice temperature measurement 11', # excluded due to missing values\n",
    "                                                        'z_pt_cor_adj_flag', # excluded due to derived correlation with y\n",
    "                              'Ice surface height adjusted for maintenance', # excluded due to derived correlation with y\n",
    "                                'Surface height adjusted for maintenance 1', # excluded due to derived correlation with y\n",
    "                                  'Surface height adjusted for maintenance', # excluded due to derived correlation with y\n",
    "#                                                                 'Datetime',\n",
    "#                                                                 'Ablation',  \n",
    "#                          'Surface height from combined measurements DELTA'\n",
    "                                                              'Unnamed: 18',# excluded because we do not know what it is\n",
    "                                                                'month_num',# excluded because we do not know what it is\n",
    "                                                                     'date',# excluded because we do not know what it is\n",
    "                                                       'Surface height 1.1',# excluded because we do not know what it is\n",
    "                                                              'Unnamed: 48',# excluded because we do not know what it is\n",
    "                                                                     'Time',# excluded because we do not know what it is\n",
    "                                                               'Unnamed: 0',\n",
    "                                                              'Unnamed: 17',\n",
    "                                                                     'Date',# excluded because we do not know what it is\n",
    "                                                       \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations loaded:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['KAN_L', 'SCO_L'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in dataset:\n",
      "['stid', 'time', 'p_u', 't_u', 'rh_u', 'rh_u_cor', 'qh_u', 'wspd_u', 'wdir_u', 'dsr', 'dsr_cor', 'usr', 'usr_cor', 'albedo', 'dlr', 'ulr', 'cc', 't_surf', 'dlhf_u', 'dshf_u', 'z_boom_u', 'z_stake', 'z_pt', 'z_pt_cor', 'precip_u', 'precip_u_cor', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7', 't_i_8', 'tilt_x', 'tilt_y', 'rot', 'gps_lat', 'gps_lon', 'gps_alt', 'gps_time', 'gps_geoid', 'gps_geounit', 'gps_hdop', 'gps_numsat', 'gps_q', 'batt_v', 'batt_v_ini', 'batt_v_ss', 'fan_dc_u', 'freq_vw', 't_log', 't_rad', 'msg_lat', 'msg_lon', 'z_pt_cor_adj_flag', 'z_surf_1', 'z_surf_2', 'z_surf_2_adj_flag', 'z_pt_cor_adj', 'z_surf_1_adj', 'z_surf_2_adj', 'z_surf_combined', 'depth_t_i_1', 'depth_t_i_2', 'depth_t_i_3', 'depth_t_i_4', 'depth_t_i_5', 'depth_t_i_6', 'depth_t_i_7', 'depth_t_i_8', 't_i_10m']\n",
      "FINISHED LOADING CSV's\n",
      "Index([                                                             'index',\n",
      "                                                                     'stid',\n",
      "                                                                     'Time',\n",
      "                                                'Air pressure (upper boom)',\n",
      "                                             'Air temperature (upper boom)',\n",
      "                                           'Relative humidity (upper boom)',\n",
      "                               'Relative humidity (upper boom) - corrected',\n",
      "                                           'Specific humidity (upper boom)',\n",
      "                                                  'Wind speed (upper boom)',\n",
      "                                         'Wind from direction (upper boom)',\n",
      "                                          'Downwelling shortwave radiation',\n",
      "                              'Downwelling shortwave radiation - corrected',\n",
      "                                            'Upwelling shortwave radiation',\n",
      "                                'Upwelling shortwave radiation - corrected',\n",
      "                                                                   'Albedo',\n",
      "                                           'Downwelling longwave radiation',\n",
      "                                             'Upwelling longwave radiation',\n",
      "                                                              'Cloud cover',\n",
      "                                                      'Surface temperature',\n",
      "                                            'Latent heat flux (upper boom)',\n",
      "                                          'Sensible heat flux (upper boom)',\n",
      "                                                        'Upper boom height',\n",
      "                                                             'Stake height',\n",
      "                                      'Depth of pressure transducer in ice',\n",
      "                          'Depth of pressure transducer in ice - corrected',\n",
      "                   'Precipitation (upper boom) (cumulative solid & liquid)',\n",
      "       'Precipitation (upper boom) (cumulative solid & liquid) â€“ corrected',\n",
      "                                              'Ice temperature at sensor 1',\n",
      "                                              'Ice temperature at sensor 2',\n",
      "                                              'Ice temperature at sensor 3',\n",
      "                                              'Ice temperature at sensor 4',\n",
      "                                              'Ice temperature at sensor 5',\n",
      "                                              'Ice temperature at sensor 6',\n",
      "                                              'Ice temperature at sensor 7',\n",
      "                                              'Ice temperature at sensor 8',\n",
      "                                                             'Tilt to east',\n",
      "                                                            'Tilt to north',\n",
      "                                         'Station rotation from true North',\n",
      "                                                                 'Latitude',\n",
      "                                                                'Longitude',\n",
      "                                                                 'Altitude',\n",
      "                                                                 'GPS time',\n",
      "                               'Height of EGM96 geoid over WGS84 ellipsoid',\n",
      "                                                                  'GeoUnit',\n",
      "                             'GPS horizontal dillution of precision (HDOP)',\n",
      "                                                 'GPS number of satellites',\n",
      "                                                                  'Quality',\n",
      "                                                          'Battery voltage',\n",
      "                                                                        nan,\n",
      "                                           'Battery voltage (sample start)',\n",
      "                                                 'Fan current (upper boom)',\n",
      "                       'Frequency of vibrating wire in precipitation gauge',\n",
      "                                                       'Logger temperature',\n",
      "                                             'Radiation sensor temperature',\n",
      "                                         'latitude from modem (email text)',\n",
      "                                        'longitude from modem (email text)',\n",
      "                                                        'z_pt_cor_adj_flag',\n",
      "                                                         'Surface height 1',\n",
      "                                                         'Surface height 1',\n",
      "                                                        'z_surf_2_adj_flag',\n",
      "                              'Ice surface height adjusted for maintenance',\n",
      "                                'Surface height adjusted for maintenance 1',\n",
      "                                  'Surface height adjusted for maintenance',\n",
      "                                'Surface height from combined measurements',\n",
      "                                   'Depth of ice temperature measurement 1',\n",
      "                                   'Depth of ice temperature measurement 2',\n",
      "                                   'Depth of ice temperature measurement 3',\n",
      "                                   'Depth of ice temperature measurement 4',\n",
      "                                   'Depth of ice temperature measurement 5',\n",
      "                                   'Depth of ice temperature measurement 6',\n",
      "                                   'Depth of ice temperature measurement 7',\n",
      "                                   'Depth of ice temperature measurement 8',\n",
      "                               'Ice temperature interpolated at 10 m depth',\n",
      "                                                                 'Datetime'],\n",
      "      dtype='object')\n",
      "Calculating Ablation KAN_L\n",
      "Calculating Ablation SCO_L\n",
      "Calculating Melting Season KAN_L\n",
      "   Calculating 2008\n",
      "   Calculating 2009\n",
      "   Calculating 2010\n",
      "   Calculating 2011\n",
      "   Calculating 2012\n",
      "   Calculating 2013\n",
      "   Calculating 2014\n",
      "   Calculating 2015\n",
      "   Calculating 2016\n",
      "   Calculating 2017\n",
      "   Calculating 2018\n",
      "   Calculating 2019\n",
      "   Calculating 2020\n",
      "   Calculating 2021\n",
      "   Calculating 2022\n",
      "   Calculating 2023\n",
      "Calculating Melting Season SCO_L\n",
      "   Calculating 2008\n",
      "   Calculating 2009\n",
      "   Calculating 2010\n",
      "   Calculating 2011\n",
      "   Calculating 2012\n",
      "   Calculating 2013\n",
      "   Calculating 2014\n",
      "   Calculating 2015\n",
      "   Calculating 2016\n",
      "   Calculating 2017\n",
      "   Calculating 2018\n",
      "   Calculating 2019\n",
      "   Calculating 2020\n",
      "   Calculating 2021\n",
      "   Calculating 2022\n",
      "   Calculating 2023\n",
      "Calculating KAN_L\n",
      "Calculating SCO_L\n",
      "FINISHED PROCESSING HOURLY DATA\n",
      "finished loading csvs\n",
      "Stations loaded:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['SCO_L', 'KAN_L'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in dataset:\n",
      "['stid', 'Air pressure (upper boom)', 'Air temperature (upper boom)', 'Relative humidity (upper boom) - corrected', 'Specific humidity (upper boom)', 'Wind speed (upper boom)', 'Wind from direction (upper boom)', 'Downwelling shortwave radiation - corrected', 'Upwelling shortwave radiation - corrected', 'Albedo', 'Downwelling longwave radiation', 'Upwelling longwave radiation', 'Cloud cover', 'Surface temperature', 'Latent heat flux (upper boom)', 'Sensible heat flux (upper boom)', 'Surface height from combined measurements', 'Datetime', 'Ablation', 'Melting Season', 'Surface height from combined measurements DELTA']\n",
      "FINISHED LOADING CSV's\n",
      "columns in df_daily:\n",
      "['stid', 'Datetime', 'Air pressure (upper boom)', 'Air temperature (upper boom)', 'Relative humidity (upper boom) - corrected', 'Specific humidity (upper boom)', 'Wind speed (upper boom)', 'Wind from direction (upper boom)', 'Downwelling shortwave radiation - corrected', 'Upwelling shortwave radiation - corrected', 'Downwelling longwave radiation', 'Upwelling longwave radiation', 'Surface temperature', 'Latent heat flux (upper boom)', 'Sensible heat flux (upper boom)', 'Surface height from combined measurements', 'Surface height from combined measurements DELTA', 'Albedo', 'Cloud cover', 'Ablation', 'Melting Season']\n",
      "FINISHED PROCESSING HOURLY DATA\n"
     ]
    }
   ],
   "source": [
    "#os.chdir('c:\\\\Users\\\\mabj16ac\\\\Desktop\\\\Thesis\\\\GEUS-Master-Thesis\\\\scripts')\n",
    "os.chdir('/Users/nilsfulde/Desktop/Master_Thesis/scripts')\n",
    "\n",
    "#header = pd.read_csv('../metadata/promice_header.csv', sep = \";\")\n",
    "#df_hourly = df_hourly.rename(columns = header.set_index('standard_name')['units'])\n",
    "df_hourly = process_hourly_data(add_meta_data=False)\n",
    "#df_hourly = df_hourly.rename(columns = header.set_index('standard_name')['units'])\n",
    "\n",
    "#df_hourly = process_hourly_data(directory=\"../data/new_promice/all_promice_data.parquet.gzip\")\n",
    "############################################\n",
    "\n",
    "df_daily = process_daily_data(add_meta_data=False)\n",
    "#df_daily = df_daily.rename(columns = header.set_index('standard_name')['units'])\n",
    "\n",
    "#df_daily = process_daily_data(directory=\"../data/new_promice/all_promice_data_hourly.parquet.gzip\", add_meta_data=True)\n",
    "\n",
    "############################################\n",
    "#df_monthly = process_monthly_data(dataframe=df_daily, add_meta_data=True)\n",
    "#df_monthly = df_monthly.rename(columns = header.set_index('standard_name')['units'])\n",
    "\n",
    "#df_monthly = process_monthly_data(directory='..\\\\Data\\\\new_promice\\\\all_promice_data_daily.parquet.gzip', add_meta_data=True)\n",
    "\n",
    "### helper function to add station information such as # of booms, location and classification\n",
    "\n",
    "# df_hourly = add_station_info(df_hourly)\n",
    "# df_daily = add_station_info(df_daily)\n",
    "# df_monthly = add_station_info(df_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promice_pull(directory='/data/promice/new_promice/daily_data'):\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Combine all CSV files into a single DataFrame\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(os.path.join(directory, f), index_col=False)\n",
    "        df.insert(0, 'stid', f[:-7])\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\mabj16ac\\Desktop\\Thesis\\GEUS-Master-Thesis\\data\\promice\\new_promice\\hourly_data\n",
    "null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "print(\"null columns:\\n\", null_columns)\n",
    "print(\"flag_columns:\\n\", flag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zyy-tj5MR42i"
   },
   "source": [
    "Promice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BkQ-3yLP-37W"
   },
   "outputs": [],
   "source": [
    "!wget -r -e robots=off -nH --cut-dirs=3 --content-disposition \"https://dataverse.geus.dk/api/datasets/:persistentId/dirindex?persistentId=doi:10.22008/FK2/8SS7EW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CWscWKUJxkCA"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Redundant\n",
    "\n",
    "def process_monthly_data(dataframe=None, directory=\"\", add_meta_data=False):\n",
    "\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        df = pd.read_parquet(directory)\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "\n",
    "    df['month_year'] = df['Datetime'].dt.to_period('M')\n",
    "    # Define the date column that you want to group by (replace \"date_column\" with the name of your column)\n",
    "    min_values_per_month = 24\n",
    "\n",
    "    # Create a new column with the month and year of the date column\n",
    "\n",
    "    # Group the data by weather station and date\n",
    "    grouped = df.groupby(['stid','month_year'])\n",
    "\n",
    "    # Specify columns containing numerical values to be averaged\n",
    "    columns_to_average = (df\n",
    "                .select_dtypes(exclude=['object'])\n",
    "                .drop(columns=['Datetime', 'DayOfYear', 'DayOfCentury'])\n",
    "                .columns\n",
    "    )\n",
    "\n",
    "    # Calculate the number of non-NaN values for each variable within each group\n",
    "    valid_date_observations = grouped[columns_to_average].apply(lambda x: x.notna().sum() <= min_values_per_month)\n",
    "\n",
    "    # Calculate average per day per station\n",
    "    df_filtered = grouped[columns_to_average].mean()\n",
    "\n",
    "    #Remove means with less than 20 observations per day\n",
    "    df_masked = df_filtered.mask(valid_date_observations, np.nan)\n",
    "\n",
    "    df_monthly = df_masked.reset_index().copy()\n",
    "\n",
    "    df_monthly[\"Datetime\"] = pd.to_datetime(df_monthly['month_year'].astype(str) + '-01')\n",
    "\n",
    "    if add_meta_data==True:\n",
    "        df_monthly = add_nice_to_haves(df_monthly, 'monthly')\n",
    "\n",
    "    #display(df_monthly)\n",
    "\n",
    "    # Convert the DataFrame to a compressed Parquet file\n",
    "    output_file = \"..\\\\Data\\\\new_promice\\\\all_promice_data_monthly.parquet.gzip\"\n",
    "    df_monthly.to_parquet(output_file, compression='gzip', engine='pyarrow')\n",
    "\n",
    "    print('FINISHED PROCESSING MONTHLY DATA')\n",
    "    return df_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vwZWPw1I_YPn"
   },
   "outputs": [],
   "source": [
    "# save daily data \n",
    "# Get the files from the path provided in the OP\n",
    "files = Path(path).glob('*_day_v03.txt')  # .rglob to get subdirectories\n",
    "\n",
    "dfs = list()\n",
    "for f in files:\n",
    "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
    "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
    "    data['file'] = f.stem\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = df.replace(-999, np.nan)\n",
    "\n",
    "df[\"Month\"] = df[\"MonthOfYear\"]\n",
    "df[\"Day\"] = df[\"DayOfMonth\"]\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]], format='%Y%m%d')\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df.to_parquet(\"data/promice_daily.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_daily.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bDnLS8bd_a3y"
   },
   "outputs": [],
   "source": [
    "# save monthly data \n",
    "# Get the files from the path provided in the OP\n",
    "files = Path(path).glob('*_month_v03.txt')  # .rglob to get subdirectories\n",
    "\n",
    "dfs = list()\n",
    "for f in files:\n",
    "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
    "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
    "    data['file'] = f.stem\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = df.replace(-999, np.nan)\n",
    "\n",
    "df[\"Month\"] = df[\"MonthOfYear\"]\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(['{}-{}-01'.format(y, m) for y, m in zip(df.Year, df.Month)])\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df.to_parquet(\"data/promice_monthly.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_monthly.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
