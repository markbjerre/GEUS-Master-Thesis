{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jWCm8I5T0Dy",
        "outputId": "2bf088c2-7ef3-41bb-b164-63ff8906515d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GC Net Data"
      ],
      "metadata": {
        "id": "cLqu3qB_6MQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/GEUS-PROMICE/pyNEAD.git"
      ],
      "metadata": {
        "id": "iFn7hTObSpj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env bash\n",
        "\n",
        "# Latest L1 data: https://github.com/GEUS-Glaciology-and-Climate/GC-Net-level-1-data-processing/tree/main/L1\n",
        "# API contents of latest L1 data (raw URLs etc.): https://api.github.com/repositories/319306521/contents/L1\n",
        "\n",
        "import os\n",
        "#os.chdir('/content/drive/MyDrive/Master_Thesis')\n",
        "os.chdir(\"../\")\n",
        "print(os.getcwd())\n",
        "\n",
        "\n",
        "try:\n",
        "    os.mkdir(\"data\")\n",
        "    os.mkdir(\"data/data_daily\")\n",
        "    os.mkdir(\"data/data_hourly\")\n",
        "except:\n",
        "    print('Overwritting existing data in \"/data\"')\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Download data\n",
        "print(\"Downloading daily data...\\r\")\n",
        "\n",
        "\n",
        "# xargs -n 1 curl --silent -O --output-dir data_daily < ../metadata/urls_1.txt\n",
        "for url in open(\"metadata/urls_1.txt\"):\n",
        "    # Split on the rightmost / and take everything on the right side of that\n",
        "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
        "    # Strip /n at the end of filename\n",
        "    name = name.strip()\n",
        "    # Combine the name and the downloads directory to get the local filename\n",
        "    filename = os.path.join(\"data/data_daily\", name)\n",
        "\n",
        "    # Download the file if it does not exist\n",
        "    if not os.path.isfile(filename):\n",
        "        urllib.request.urlretrieve(url, filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba5Hy-lHR2Jr",
        "outputId": "22868e77-0da1-4934-9b33-e66e47e507c3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Master_Thesis\n",
            "Overwritting existing data in \"/data\"\n",
            "Downloading daily data...\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading hourly data...\\r\")\n",
        "# xargs -n 1 curl --silent -O --output-dir data_hourly < ../metadata/urls_2.txt\n",
        "for url in open(\"metadata/urls_2.txt\"):\n",
        "    # Split on the rightmost / and take everything on the right side of that\n",
        "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
        "    # Strip /n at the end of filename\n",
        "    name = name.strip()\n",
        "    # Combine the name and the downloads directory to get the local filename\n",
        "    filename = os.path.join(\"data/data_hourly\", name)\n",
        "\n",
        "    # Download the file if it does not exist\n",
        "    if not os.path.isfile(filename):\n",
        "        urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exKNv5IVbMx-",
        "outputId": "3f0f0b01-578c-4d40-940c-ecfd2547209b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading hourly data...\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data\n",
        "# echo -ne 'Processing dairly data...\\r'\n",
        "# python scripts/process_data_daily.py\n",
        "# echo -ne 'Processing hourly data...\\r'\n",
        "# python scripts/process_data_hourly.py\n",
        "\n",
        "# Delete unprocessed data\n",
        "# rm -r data_daily data_hourly\n",
        "\n",
        "# %% process data daily\n",
        "import pandas as pd\n",
        "import nead\n",
        "\n",
        "# Convert NEAD files to Pandas dataframes\n",
        "station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
        "dfs_daily = []\n",
        "\n",
        "for name, ID in zip(station.Name, station.ID):\n",
        "    format_name = name.replace(\" \", \"\")\n",
        "    files = \"data/data_daily/\" + str(ID).zfill(2) + \"-\" + format_name + \"_daily.csv\"\n",
        "    ds_daily = nead.read(files, index_col=0)\n",
        "    df_daily = ds_daily.to_dataframe()\n",
        "    df_daily.insert(\n",
        "        loc=0, column=\"station_name\", value=name\n",
        "    )  # Add station_name column to each dataframe\n",
        "    dfs_daily.append(df_daily)\n",
        "\n",
        "# Concatenate dataframes\n",
        "df_daily = pd.concat(dfs_daily).sort_index()\n",
        "\n",
        "# Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
        "# null_columns = df_daily.columns[df_daily.isnull().all()]\n",
        "# flag_columns = df_daily.filter(regex=\"flag$\").columns\n",
        "# print(null_columns)\n",
        "# print(flag_columns)\n",
        "\n",
        "df_daily = df_daily.drop(\n",
        "    columns=[\n",
        "        \"OSWR_max\",\n",
        "        \"HW2_adj_flag\",\n",
        "        \"P_adj_flag\",\n",
        "        \"HW1_adj_flag\",\n",
        "        \"OSWR_adj_flag\",\n",
        "        \"HS1_adj_flag\",\n",
        "        \"HS2_adj_flag\",\n",
        "        \"TA3_adj_flag\",\n",
        "        \"TA4_adj_flag\",\n",
        "        \"DW1_adj_flag\",\n",
        "    ]\n",
        ")\n",
        "# Add season column to dataframe\n",
        "seasons = {\n",
        "    1: \"Winter\",\n",
        "    2: \"Winter\",\n",
        "    3: \"Spring\",\n",
        "    4: \"Spring\",\n",
        "    5: \"Spring\",\n",
        "    6: \"Summer\",\n",
        "    7: \"Summer\",\n",
        "    8: \"Summer\",\n",
        "    9: \"Autumn\",\n",
        "    10: \"Autumn\",\n",
        "    11: \"Autumn\",\n",
        "    12: \"Winter\",\n",
        "}\n",
        "\n",
        "# Extract the month from the index and use the dictionary to map it to the corresponding season\n",
        "df_daily[\"season\"] = df_daily.index.month.map(seasons)\n",
        "\n",
        "# Add year column to dataframe\n",
        "df_daily[\"year\"] = df_daily.index.strftime(\"%Y\")\n",
        "\n",
        "# Add month column to dataframe\n",
        "df_daily[\"month\"] = df_daily.index.strftime(\"%B\")\n",
        "\n",
        "# Rename Index Column to Datetime\n",
        "df_daily = df_daily.reset_index(inplace=False)\n",
        "df_daily = df_daily.rename(columns={'timestamp': 'Datetime'}, inplace=False) \n",
        "\n",
        "# Rename 'station_name' to file\n",
        "df_daily = df_daily.rename(columns={'station_name': 'file'}, inplace=False)\n",
        "\n",
        "# Add Day of Year & Day of Century \n",
        "df_daily['DayOfYear'] = df_daily['Datetime'].dt.dayofyear \n",
        "df_daily['DayOfCentury'] = df_daily['Datetime'].dt.dayofyear+365*(df_daily['Datetime'].dt.year-1)\n",
        "\n",
        "\n",
        "# # Add day column to dataframe\n",
        "# df_daily[\"day\"] = df_daily.index.strftime(\"%d\")\n",
        "\n",
        "# Add hour column to dataframe\n",
        "# df_daily[\"hour\"] = df_daily.index.strftime(\"%h\")\n",
        "\n",
        "# Change column headers\n",
        "#header = pd.read_csv('/content/drive/MyDrive/Master_Thesis/metadata/Masterdata_GCNET.csv', sep = \";\")\n",
        "header = pd.read_csv('metadata/Masterdata_GCNET.csv', sep = \";\")\n",
        "df_daily = df_daily.rename(columns = header.set_index('fields')['display_description'])\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df_daily.to_parquet(\"data/df_daily.gzip\", compression=\"gzip\")\n",
        "\n",
        "# %% process data hourly\n",
        "# Convert NEAD files to Pandas dataframes\n",
        "station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
        "dfs_hourly = []\n",
        "\n",
        "for name, ID in zip(station.Name, station.ID):\n",
        "    format_name = name.replace(\" \", \"\")\n",
        "    files = \"data/data_hourly/\" + str(ID).zfill(2) + \"-\" + format_name + \".csv\"\n",
        "    ds_hourly = nead.read(files, index_col=0)\n",
        "    df_hourly = ds_hourly.to_dataframe()\n",
        "    df_hourly.insert(\n",
        "        loc=0, column=\"station_name\", value=name\n",
        "    )  # Add station_name column to each dataframe\n",
        "    dfs_hourly.append(df_hourly)\n",
        "\n",
        "# Concatenate dataframes\n",
        "df_hourly = pd.concat(dfs_hourly).sort_index()\n",
        "\n",
        "# Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
        "# null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
        "# flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
        "# print(null_columns)\n",
        "# print(flag_columns)\n",
        "\n",
        "df_hourly = df_hourly.drop(\n",
        "    columns=[\n",
        "        \"OSWR_max\",\n",
        "        \"HW2_adj_flag\",\n",
        "        \"P_adj_flag\",\n",
        "        \"HW1_adj_flag\",\n",
        "        \"OSWR_adj_flag\",\n",
        "        \"HS1_adj_flag\",\n",
        "        \"HS2_adj_flag\",\n",
        "        \"TA3_adj_flag\",\n",
        "        \"TA4_adj_flag\",\n",
        "        \"DW1_adj_flag\",\n",
        "    ]\n",
        ")\n",
        "# Add year column to dataframe\n",
        "df_hourly[\"year\"] = df_hourly.index.strftime(\"%Y\")\n",
        "\n",
        "# Add month column to dataframe\n",
        "df_hourly[\"month\"] = df_hourly.index.strftime(\"%B\")\n",
        "\n",
        "# Add season column to dataframe\n",
        "seasons = {\n",
        "    1: \"Winter\",\n",
        "    2: \"Winter\",\n",
        "    3: \"Spring\",\n",
        "    4: \"Spring\",\n",
        "    5: \"Spring\",\n",
        "    6: \"Summer\",\n",
        "    7: \"Summer\",\n",
        "    8: \"Summer\",\n",
        "    9: \"Autumn\",\n",
        "    10: \"Autumn\",\n",
        "    11: \"Autumn\",\n",
        "    12: \"Winter\",\n",
        "}\n",
        "\n",
        "# Extract the month from the index and use the dictionary to map it to the corresponding season\n",
        "df_hourly[\"season\"] = df_hourly.index.month.map(seasons)\n",
        "\n",
        "\n",
        "#Rename Index Column to Datetime\n",
        "df_hourly = df_hourly.reset_index(inplace=False)\n",
        "df_hourly = df_hourly.rename(columns={'timestamp': 'Datetime'}, inplace=False) \n",
        "\n",
        "# Rename 'station_name' to file\n",
        "df_hourly = df_hourly.rename(columns={'station_name': 'file'}, inplace=False)\n",
        "\n",
        "# Add Day of Year & Day of Century \n",
        "df_hourly['DayOfYear'] = df_hourly['Datetime'].dt.dayofyear \n",
        "df_hourly['DayOfCentury'] = df_hourly['Datetime'].dt.dayofyear+365*(df_hourly['Datetime'].dt.year-1)\n",
        "\n",
        "\n",
        "# # Add day column to dataframe\n",
        "# df_hourly[\"day\"] = df_hourly.index.strftime(\"%d\")\n",
        "\n",
        "# # Add hour column to dataframe\n",
        "# df_hourly[\"hour\"] = df_hourly.index.strftime(\"%h\")\n",
        "\n",
        "\n",
        "#header = pd.read_csv('/content/drive/MyDrive/Master_Thesis/metadata/Masterdata_GCNET.csv', sep = \";\")\n",
        "header = pd.read_csv('metadata/Masterdata_GCNET.csv', sep = \";\")\n",
        "df_hourly = df_hourly.rename(columns = header.set_index('fields')['display_description'])\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df_hourly.to_parquet(\"data/df_hourly.gzip\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "QqbmK6FqeRkP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promice Data"
      ],
      "metadata": {
        "id": "Zyy-tj5MR42i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkQ-3yLP-37W"
      },
      "outputs": [],
      "source": [
        "!wget -r -e robots=off -nH --cut-dirs=3 --content-disposition \"https://dataverse.geus.dk/api/datasets/:persistentId/dirindex?persistentId=doi:10.22008/FK2/8SS7EW\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save hourly data\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "path = r'/content'  # or unix / linux / mac path\n",
        "\n",
        "# Get the files from the path provided in the OP\n",
        "files = Path(path).glob('*_hour_v03.txt')  # .rglob to get subdirectories\n",
        "\n",
        "dfs = list()\n",
        "for f in files:\n",
        "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
        "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
        "    data['file'] = f.stem\n",
        "    dfs.append(data)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df = df.replace(-999, np.nan)\n",
        "\n",
        "df[\"Month\"] = df[\"MonthOfYear\"]\n",
        "df[\"Day\"] = df[\"DayOfMonth\"]\n",
        "df[\"Hour\"] = df[\"HourOfDay(UTC)\"]\n",
        "\n",
        "df[\"Datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\", \"Hour\"]], format='%Y%m%d%h')\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df.to_parquet(\"data/promice_hourly.gzip\", compression=\"gzip\")\n",
        "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_hourly.gzip\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "CWscWKUJxkCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save daily data \n",
        "# Get the files from the path provided in the OP\n",
        "files = Path(path).glob('*_day_v03.txt')  # .rglob to get subdirectories\n",
        "\n",
        "dfs = list()\n",
        "for f in files:\n",
        "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
        "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
        "    data['file'] = f.stem\n",
        "    dfs.append(data)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df = df.replace(-999, np.nan)\n",
        "\n",
        "df[\"Month\"] = df[\"MonthOfYear\"]\n",
        "df[\"Day\"] = df[\"DayOfMonth\"]\n",
        "\n",
        "df[\"Datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]], format='%Y%m%d')\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df.to_parquet(\"data/promice_daily.gzip\", compression=\"gzip\")\n",
        "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_daily.gzip\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "vwZWPw1I_YPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save monthly data \n",
        "# Get the files from the path provided in the OP\n",
        "files = Path(path).glob('*_month_v03.txt')  # .rglob to get subdirectories\n",
        "\n",
        "dfs = list()\n",
        "for f in files:\n",
        "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
        "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
        "    data['file'] = f.stem\n",
        "    dfs.append(data)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df = df.replace(-999, np.nan)\n",
        "\n",
        "df[\"Month\"] = df[\"MonthOfYear\"]\n",
        "\n",
        "df[\"Datetime\"] = pd.to_datetime(['{}-{}-01'.format(y, m) for y, m in zip(df.Year, df.Month)])\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df.to_parquet(\"data/promice_monthly.gzip\", compression=\"gzip\")\n",
        "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_monthly.gzip\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "bDnLS8bd_a3y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}