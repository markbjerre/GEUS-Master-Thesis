{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jWCm8I5T0Dy",
    "outputId": "2bf088c2-7ef3-41bb-b164-63ff8906515d"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLqu3qB_6MQ0"
   },
   "source": [
    "GC Net Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-11.0.0-cp38-cp38-macosx_10_14_x86_64.whl (24.4 MB)\n",
      "     |████████████████████████████████| 24.4 MB 342 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from pyarrow) (1.24.2)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-11.0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/Users/nilsfulde/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iFn7hTObSpj6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/GEUS-PROMICE/pyNEAD.git\n",
      "  Cloning https://github.com/GEUS-PROMICE/pyNEAD.git to /private/var/folders/_r/2f7t_cz14232jqp3576_p04m0000gn/T/pip-req-build-s3nb1h_s\n",
      "  Running command git clone --filter=blob:none -q https://github.com/GEUS-PROMICE/pyNEAD.git /private/var/folders/_r/2f7t_cz14232jqp3576_p04m0000gn/T/pip-req-build-s3nb1h_s\n",
      "  Resolved https://github.com/GEUS-PROMICE/pyNEAD.git to commit d811220c2d048d84d4612b045880d6ca914c39a4\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from nead==0.0.0) (1.24.2)\n",
      "Requirement already satisfied: pandas in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from nead==0.0.0) (1.5.3)\n",
      "Requirement already satisfied: xarray in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from nead==0.0.0) (2023.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from pandas->nead==0.0.0) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from pandas->nead==0.0.0) (2.8.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from xarray->nead==0.0.0) (23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->nead==0.0.0) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/Users/nilsfulde/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/GEUS-PROMICE/pyNEAD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba5Hy-lHR2Jr",
    "outputId": "22868e77-0da1-4934-9b33-e66e47e507c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nilsfulde/Documents/University/University/CPH/Master/2_Semester/Machine_Learning/Exam/FairFace/GEUS-Master-Thesis\n",
      "Overwritting existing data in \"/data\"\n",
      "Downloading daily data...\r\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "# Latest L1 data: https://github.com/GEUS-Glaciology-and-Climate/GC-Net-level-1-data-processing/tree/main/L1\n",
    "# API contents of latest L1 data (raw URLs etc.): https://api.github.com/repositories/319306521/contents/L1\n",
    "\n",
    "import os\n",
    "#os.chdir('/content/drive/MyDrive/Master_Thesis')\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"data\")\n",
    "    os.mkdir(\"data/data_daily\")\n",
    "    os.mkdir(\"data/data_hourly\")\n",
    "except:\n",
    "    print('Overwritting existing data in \"/data\"')\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading daily data...\\r\")\n",
    "\n",
    "\n",
    "# xargs -n 1 curl --silent -O --output-dir data_daily < ../metadata/urls_1.txt\n",
    "for url in open(\"metadata/urls_1.txt\"):\n",
    "    # Split on the rightmost / and take everything on the right side of that\n",
    "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
    "    # Strip /n at the end of filename\n",
    "    name = name.strip()\n",
    "    # Combine the name and the downloads directory to get the local filename\n",
    "    filename = os.path.join(\"data/data_daily\", name)\n",
    "\n",
    "    # Download the file if it does not exist\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exKNv5IVbMx-",
    "outputId": "3f0f0b01-578c-4d40-940c-ecfd2547209b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hourly data...\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading hourly data...\\r\")\n",
    "# xargs -n 1 curl --silent -O --output-dir data_hourly < ../metadata/urls_2.txt\n",
    "for url in open(\"metadata/urls_2.txt\"):\n",
    "    # Split on the rightmost / and take everything on the right side of that\n",
    "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
    "    # Strip /n at the end of filename\n",
    "    name = name.strip()\n",
    "    # Combine the name and the downloads directory to get the local filename\n",
    "    filename = os.path.join(\"data/data_hourly\", name)\n",
    "\n",
    "    # Download the file if it does not exist\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QqbmK6FqeRkP"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/compat/_optional.py:141\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:973\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m df_daily \u001b[38;5;241m=\u001b[39m df_daily\u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m header\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfields\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay_description\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Save dataframe as parquet file\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mdf_daily\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/df_daily.gzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# %% process data hourly\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Convert NEAD files to Pandas dataframes\u001b[39;00m\n\u001b[1;32m    104\u001b[0m station \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata/station_info.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:2976\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2891\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2974\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py:426\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    425\u001b[0m     partition_cols \u001b[38;5;241m=\u001b[39m [partition_cols]\n\u001b[0;32m--> 426\u001b[0m impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m    430\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    431\u001b[0m     df,\n\u001b[1;32m    432\u001b[0m     path_or_buf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py:63\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyArrowImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py:148\u001b[0m, in \u001b[0;36mPyArrowImpl.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow is required for parquet support.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/compat/_optional.py:144\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "# echo -ne 'Processing dairly data...\\r'\n",
    "# python scripts/process_data_daily.py\n",
    "# echo -ne 'Processing hourly data...\\r'\n",
    "# python scripts/process_data_hourly.py\n",
    "\n",
    "# Delete unprocessed data\n",
    "# rm -r data_daily data_hourly\n",
    "\n",
    "# %% process data daily\n",
    "import pandas as pd\n",
    "import nead\n",
    "\n",
    "# Convert NEAD files to Pandas dataframes\n",
    "station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
    "dfs_daily = []\n",
    "\n",
    "for name, ID in zip(station.Name, station.ID):\n",
    "    format_name = name.replace(\" \", \"\")\n",
    "    files = \"data/data_daily/\" + str(ID).zfill(2) + \"-\" + format_name + \"_daily.csv\"\n",
    "    ds_daily = nead.read(files, index_col=0)\n",
    "    df_daily = ds_daily.to_dataframe()\n",
    "    df_daily.insert(\n",
    "        loc=0, column=\"station_name\", value=name\n",
    "    )  # Add station_name column to each dataframe\n",
    "    dfs_daily.append(df_daily)\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_daily = pd.concat(dfs_daily).sort_index()\n",
    "\n",
    "# Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
    "# null_columns = df_daily.columns[df_daily.isnull().all()]\n",
    "# flag_columns = df_daily.filter(regex=\"flag$\").columns\n",
    "# print(null_columns)\n",
    "# print(flag_columns)\n",
    "\n",
    "df_daily = df_daily.drop(\n",
    "    columns=[\n",
    "        \"OSWR_max\",\n",
    "        \"HW2_adj_flag\",\n",
    "        \"P_adj_flag\",\n",
    "        \"HW1_adj_flag\",\n",
    "        \"OSWR_adj_flag\",\n",
    "        \"HS1_adj_flag\",\n",
    "        \"HS2_adj_flag\",\n",
    "        \"TA3_adj_flag\",\n",
    "        \"TA4_adj_flag\",\n",
    "        \"DW1_adj_flag\",\n",
    "    ]\n",
    ")\n",
    "# Add season column to dataframe\n",
    "seasons = {\n",
    "    1: \"Winter\",\n",
    "    2: \"Winter\",\n",
    "    3: \"Spring\",\n",
    "    4: \"Spring\",\n",
    "    5: \"Spring\",\n",
    "    6: \"Summer\",\n",
    "    7: \"Summer\",\n",
    "    8: \"Summer\",\n",
    "    9: \"Autumn\",\n",
    "    10: \"Autumn\",\n",
    "    11: \"Autumn\",\n",
    "    12: \"Winter\",\n",
    "}\n",
    "\n",
    "# Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "df_daily[\"season\"] = df_daily.index.month.map(seasons)\n",
    "\n",
    "# Add year column to dataframe\n",
    "df_daily[\"year\"] = df_daily.index.strftime(\"%Y\")\n",
    "\n",
    "# Add month column to dataframe\n",
    "df_daily[\"month\"] = df_daily.index.strftime(\"%B\")\n",
    "\n",
    "# Rename Index Column to Datetime\n",
    "df_daily = df_daily.reset_index(inplace=False)\n",
    "df_daily = df_daily.rename(columns={'timestamp': 'Datetime'}, inplace=False) \n",
    "\n",
    "# Rename 'station_name' to file\n",
    "df_daily = df_daily.rename(columns={'station_name': 'file'}, inplace=False)\n",
    "\n",
    "# Add Day of Year & Day of Century \n",
    "df_daily['DayOfYear'] = df_daily['Datetime'].dt.dayofyear \n",
    "df_daily['DayOfCentury'] = df_daily['Datetime'].dt.dayofyear+365*(df_daily['Datetime'].dt.year-1)\n",
    "\n",
    "\n",
    "# # Add day column to dataframe\n",
    "# df_daily[\"day\"] = df_daily.index.strftime(\"%d\")\n",
    "\n",
    "# Add hour column to dataframe\n",
    "# df_daily[\"hour\"] = df_daily.index.strftime(\"%h\")\n",
    "\n",
    "# Change column headers\n",
    "#header = pd.read_csv('/content/drive/MyDrive/Master_Thesis/metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "header = pd.read_csv('metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "df_daily = df_daily.rename(columns = header.set_index('fields')['display_description'])\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df_daily.to_parquet(\"data/df_daily.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "\n",
    "# %% process data hourly\n",
    "# Convert NEAD files to Pandas dataframes\n",
    "station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
    "dfs_hourly = []\n",
    "\n",
    "for name, ID in zip(station.Name, station.ID):\n",
    "    format_name = name.replace(\" \", \"\")\n",
    "    files = \"data/data_hourly/\" + str(ID).zfill(2) + \"-\" + format_name + \".csv\"\n",
    "    ds_hourly = nead.read(files, index_col=0)\n",
    "    df_hourly = ds_hourly.to_dataframe()\n",
    "    df_hourly.insert(\n",
    "        loc=0, column=\"station_name\", value=name\n",
    "    )  # Add station_name column to each dataframe\n",
    "    dfs_hourly.append(df_hourly)\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_hourly = pd.concat(dfs_hourly).sort_index()\n",
    "\n",
    "# Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
    "# null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
    "# flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
    "# print(null_columns)\n",
    "# print(flag_columns)\n",
    "\n",
    "df_hourly = df_hourly.drop(\n",
    "    columns=[\n",
    "        \"OSWR_max\",\n",
    "        \"HW2_adj_flag\",\n",
    "        \"P_adj_flag\",\n",
    "        \"HW1_adj_flag\",\n",
    "        \"OSWR_adj_flag\",\n",
    "        \"HS1_adj_flag\",\n",
    "        \"HS2_adj_flag\",\n",
    "        \"TA3_adj_flag\",\n",
    "        \"TA4_adj_flag\",\n",
    "        \"DW1_adj_flag\",\n",
    "    ]\n",
    ")\n",
    "# Add year column to dataframe\n",
    "df_hourly[\"year\"] = df_hourly.index.strftime(\"%Y\")\n",
    "\n",
    "# Add month column to dataframe\n",
    "df_hourly[\"month\"] = df_hourly.index.strftime(\"%B\")\n",
    "\n",
    "# Add season column to dataframe\n",
    "seasons = {\n",
    "    1: \"Winter\",\n",
    "    2: \"Winter\",\n",
    "    3: \"Spring\",\n",
    "    4: \"Spring\",\n",
    "    5: \"Spring\",\n",
    "    6: \"Summer\",\n",
    "    7: \"Summer\",\n",
    "    8: \"Summer\",\n",
    "    9: \"Autumn\",\n",
    "    10: \"Autumn\",\n",
    "    11: \"Autumn\",\n",
    "    12: \"Winter\",\n",
    "}\n",
    "\n",
    "# Extract the month from the index and use the dictionary to map it to the corresponding season\n",
    "df_hourly[\"season\"] = df_hourly.index.month.map(seasons)\n",
    "\n",
    "\n",
    "#Rename Index Column to Datetime\n",
    "df_hourly = df_hourly.reset_index(inplace=False)\n",
    "df_hourly = df_hourly.rename(columns={'timestamp': 'Datetime'}, inplace=False) \n",
    "\n",
    "# Rename 'station_name' to file\n",
    "df_hourly = df_hourly.rename(columns={'station_name': 'file'}, inplace=False)\n",
    "\n",
    "# Add Day of Year & Day of Century \n",
    "df_hourly['DayOfYear'] = df_hourly['Datetime'].dt.dayofyear \n",
    "df_hourly['DayOfCentury'] = df_hourly['Datetime'].dt.dayofyear+365*(df_hourly['Datetime'].dt.year-1)\n",
    "\n",
    "\n",
    "# # Add day column to dataframe\n",
    "# df_hourly[\"day\"] = df_hourly.index.strftime(\"%d\")\n",
    "\n",
    "# # Add hour column to dataframe\n",
    "# df_hourly[\"hour\"] = df_hourly.index.strftime(\"%h\")\n",
    "\n",
    "\n",
    "#header = pd.read_csv('/content/drive/MyDrive/Master_Thesis/metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "header = pd.read_csv('metadata/Masterdata_GCNET.csv', sep = \";\")\n",
    "df_hourly = df_hourly.rename(columns = header.set_index('fields')['display_description'])\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df_hourly.to_parquet(\"data/df_hourly.gzip\", compression=\"gzip\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zyy-tj5MR42i"
   },
   "source": [
    "Promice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkQ-3yLP-37W"
   },
   "outputs": [],
   "source": [
    "!wget -r -e robots=off -nH --cut-dirs=3 --content-disposition \"https://dataverse.geus.dk/api/datasets/:persistentId/dirindex?persistentId=doi:10.22008/FK2/8SS7EW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWscWKUJxkCA"
   },
   "outputs": [],
   "source": [
    "# save hourly data\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = r'/content'  # or unix / linux / mac path\n",
    "\n",
    "# Get the files from the path provided in the OP\n",
    "files = Path(path).glob('*_hour_v03.txt')  # .rglob to get subdirectories\n",
    "\n",
    "dfs = list()\n",
    "for f in files:\n",
    "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
    "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
    "    data['file'] = f.stem\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = df.replace(-999, np.nan)\n",
    "\n",
    "df[\"Month\"] = df[\"MonthOfYear\"]\n",
    "df[\"Day\"] = df[\"DayOfMonth\"]\n",
    "df[\"Hour\"] = df[\"HourOfDay(UTC)\"]\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\", \"Hour\"]], format='%Y%m%d%h')\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df.to_parquet(\"data/promice_hourly.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_hourly.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwZWPw1I_YPn"
   },
   "outputs": [],
   "source": [
    "# save daily data \n",
    "# Get the files from the path provided in the OP\n",
    "files = Path(path).glob('*_day_v03.txt')  # .rglob to get subdirectories\n",
    "\n",
    "dfs = list()\n",
    "for f in files:\n",
    "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
    "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
    "    data['file'] = f.stem\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = df.replace(-999, np.nan)\n",
    "\n",
    "df[\"Month\"] = df[\"MonthOfYear\"]\n",
    "df[\"Day\"] = df[\"DayOfMonth\"]\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]], format='%Y%m%d')\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df.to_parquet(\"data/promice_daily.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_daily.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDnLS8bd_a3y"
   },
   "outputs": [],
   "source": [
    "# save monthly data \n",
    "# Get the files from the path provided in the OP\n",
    "files = Path(path).glob('*_month_v03.txt')  # .rglob to get subdirectories\n",
    "\n",
    "dfs = list()\n",
    "for f in files:\n",
    "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
    "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
    "    data['file'] = f.stem\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = df.replace(-999, np.nan)\n",
    "\n",
    "df[\"Month\"] = df[\"MonthOfYear\"]\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(['{}-{}-01'.format(y, m) for y, m in zip(df.Year, df.Month)])\n",
    "\n",
    "# Save dataframe as parquet file\n",
    "df.to_parquet(\"data/promice_monthly.gzip\", compression=\"gzip\", engine='pyarrow')\n",
    "#df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_monthly.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
