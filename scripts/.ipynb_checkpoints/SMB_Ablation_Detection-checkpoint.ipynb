{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface Mass Balance  - Ablation Period Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTbsuMgO3ery"
   },
   "source": [
    "Import relevant Libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from pyarrow) (1.24.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/nilsfulde/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (5.6.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in /Users/nilsfulde/opt/anaconda3/lib/python3.8/site-packages (from plotly) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/nilsfulde/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ERROR: unknown command \"update\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow\n",
    "%pip install plotly\n",
    "%pip update pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yJLqdDNx3j-Y"
   },
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math as math\n",
    "import datetime\n",
    "from scipy import stats\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ODOhsaMP7u_"
   },
   "source": [
    "Set WD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyrsistent import v\n",
    "os.getcwd()\n",
    "#os.chdir('/Users/asgerlyngeholst-hansen/Desktop/GEUS-Master-Thesis/')\n",
    "#os.chdir('C:\\\\Users\\\\nifu18ab\\\\Desktop\\\\GEUS-Master-Thesis')\n",
    "#os.chdir('/Users/nilsfulde/Desktop/GEUS-Master-Thesis')\n",
    "os.chdir('/Users/nilsfulde/Desktop/Master_Thesis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load New Promice Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for loading data and converting to datetime \n",
    "def load_hourly(path_to_file):\n",
    "    '''\n",
    "    Loading PROMICE data for a given path into a DataFrame.\n",
    "    + adding time index\n",
    "    \n",
    "    INTPUTS:\n",
    "        path_to_file: Path to the desired file containing PROMICE data [string]\n",
    "    \n",
    "    OUTPUTS:\n",
    "        df: Dataframe containing PROMICE data for the desired settings [DataFrame]\n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv(path_to_file)\n",
    "    df['time'] = pd.to_datetime(df.time, utc=True)\n",
    "    df = df.set_index('time')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for loading data and converting to datetime \n",
    "def load_daily(path_to_file):\n",
    "    '''\n",
    "    Loading PROMICE data for a given path into a DataFrame.\n",
    "    + adding time index\n",
    "    \n",
    "    INTPUTS:\n",
    "        path_to_file: Path to the desired file containing PROMICE data [string]\n",
    "    \n",
    "    OUTPUTS:\n",
    "        df: Dataframe containing PROMICE data for the desired settings [DataFrame]\n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv(path_to_file)\n",
    "    df['date'] = pd.to_datetime(df.date, utc=True)\n",
    "    df = df.set_index('date')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Unique Stations\n",
    "station_list = pd.read_csv('data/promice/stations.csv')\n",
    "\n",
    "# Load Header\n",
    "header = pd.read_csv('metadata/promice_header.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For All Stations - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to define the data that is to be loaded\n",
    "\n",
    "import glob\n",
    "\n",
    "# Define the directory path where the CSV files are located\n",
    "directory_path = 'data/promice/new_promice/'\n",
    "\n",
    "# Define a pattern to match the filenames of the CSV files\n",
    "pattern = '*_daily.csv'\n",
    "\n",
    "# Use glob to get a list of all files that match the pattern\n",
    "filenames = glob.glob(output_path + pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit to define the dataname that is to be saved\n",
    "\n",
    "# Define the directory path where the CSV files are to be saved\n",
    "output_path = 'data/promice/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'z_pt_cor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'z_pt_cor'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m load_daily(file)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# defining ice ablation period   \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m smoothed_PT \u001b[38;5;241m=\u001b[39m  \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz_pt_cor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39minterpolate(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m72\u001b[39m)\u001b[38;5;241m.\u001b[39mrolling(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m14D\u001b[39m\u001b[38;5;124m'\u001b[39m, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m14\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m smoothed_PT \u001b[38;5;241m=\u001b[39m smoothed_PT\u001b[38;5;241m.\u001b[39mrolling(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m14D\u001b[39m\u001b[38;5;124m'\u001b[39m, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m14\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m threshold_ablation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0002\u001b[39m \u001b[38;5;66;03m# Modify value if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'z_pt_cor'"
     ]
    }
   ],
   "source": [
    "# Percentiles for beginning & end of Ablation Period\n",
    "beginning = 0.15\n",
    "end = 0.85\n",
    "\n",
    "for file in filenames:\n",
    "    \n",
    "    # Load one station\n",
    "    df = load_daily(file)\n",
    "    \n",
    "    # defining ice ablation period   \n",
    "    smoothed_PT =  df['z_pt_cor'].interpolate(limit=72).rolling('14D', min_periods=1).mean().shift(-7*14/2, freq='h')\n",
    "    smoothed_PT = smoothed_PT.rolling('14D', min_periods=1).mean().shift(-7*14/2, freq='h')\n",
    "\n",
    "    threshold_ablation = -0.0002 # Modify value if needed\n",
    "    ind_ablation = np.logical_and(smoothed_PT.diff().values < threshold_ablation, \n",
    "                                  np.isin(smoothed_PT.diff().index.month, [6, 7, 8, 9]))\n",
    "\n",
    "    ind_ablation = np.concatenate((ind_ablation[4*24:], np.repeat(ind_ablation[-(4*24):-(4*24-1)], 4*24)))\n",
    "\n",
    "    df['Ablation'] = ind_ablation.tolist()\n",
    "\n",
    "    # add headers\n",
    "    df = df.rename(columns = header.set_index('standard_name')['units']) # Headers are shifted in csv file\n",
    "\n",
    "\n",
    "    # add delta column\n",
    "    df['Surface height from combined measurements DELTA'] = df['Surface height from combined measurements'].diff()\n",
    "\n",
    "\n",
    "    # This bit to add a beginning, middle & end of melting season attribute\n",
    "\n",
    "    # Iterate over each year\n",
    "    #df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"Datetime\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "    for year in df['Datetime'].dt.year.unique():\n",
    "\n",
    "        print(\"Calculating\" , year)\n",
    "\n",
    "        # Get the dataframe for the current year\n",
    "        year_df = df[df['Datetime'].dt.year == year]\n",
    "\n",
    "        # Determine the threshold values for the beginning and end of the melting season\n",
    "        ablation_df = year_df[year_df['Ablation'] == True]\n",
    "        lower_threshold = ablation_df['Datetime'].dt.dayofyear.quantile(beginning)\n",
    "        upper_threshold = ablation_df['Datetime'].dt.dayofyear.quantile(end)\n",
    "\n",
    "        # Set the Melting Season value for each row in the current year\n",
    "        for index, row in year_df.iterrows():\n",
    "\n",
    "            if row['Ablation'] == True:\n",
    "                if row['Datetime'].dayofyear <= lower_threshold:\n",
    "                    df.loc[index, 'Melting Season'] = 'beginning'\n",
    "                elif row['Datetime'].dayofyear >= upper_threshold:\n",
    "                    df.loc[index, 'Melting Season'] = 'end'\n",
    "                else:\n",
    "                    df.loc[index, 'Melting Season'] = 'middle'\n",
    "            else:\n",
    "                df.loc[index, 'Melting Season'] = ''\n",
    "\n",
    "    df.to_csv(output_path + file, index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For All Stations - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise redo the steps just for KAN_L:\n",
    "\n",
    "os.chdir('/Users/nilsfulde/Desktop/GEUS-Master-Thesis')\n",
    "\n",
    "df = load_promice('data/new_promice/all_promice_data_daily.parquet.gzip')\n",
    "\n",
    "os.chdir('/Users/nilsfulde/Desktop/Master_Thesis')\n",
    "\n",
    "df = df[df[\"stid\"] == \"KAN_L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating 2008\n",
      "Calculating 2009\n",
      "Calculating 2010\n",
      "Calculating 2011\n",
      "Calculating 2012\n",
      "Calculating 2013\n",
      "Calculating 2014\n",
      "Calculating 2015\n",
      "Calculating 2016\n",
      "Calculating 2017\n",
      "Calculating 2018\n",
      "Calculating 2019\n",
      "Calculating 2020\n",
      "Calculating 2021\n",
      "Calculating 2022\n",
      "Calculating 2023\n"
     ]
    }
   ],
   "source": [
    "# defining ice ablation period   \n",
    "smoothed_PT =  df['z_pt_cor'].interpolate(limit=72).rolling('14D', min_periods=1).mean().shift(-7*14/2, freq='h')\n",
    "smoothed_PT = smoothed_PT.rolling('14D', min_periods=1).mean().shift(-7*14/2, freq='h')\n",
    "\n",
    "threshold_ablation = -0.0002 # Modify value if needed\n",
    "ind_ablation = np.logical_and(smoothed_PT.diff().values < threshold_ablation, \n",
    "                              np.isin(smoothed_PT.diff().index.month, [6, 7, 8, 9]))\n",
    "\n",
    "ind_ablation = np.concatenate((ind_ablation[4*24:], np.repeat(ind_ablation[-(4*24):-(4*24-1)], 4*24)))\n",
    "\n",
    "df['Ablation'] = ind_ablation.tolist()\n",
    "\n",
    "# add headers\n",
    "header = pd.read_csv('metadata/promice_header.csv', sep = \";\")\n",
    "df = df.rename(columns = header.set_index('standard_name')['units']) # Headers are shifted in csv file\n",
    "\n",
    "\n",
    "# add delta column (Modification Needed)\n",
    "df['Surface height from combined measurements DELTA'] = df['Surface height from combined measurements'].diff()\n",
    "\n",
    "\n",
    "\n",
    "# This bit to add a beginning, middle & end of melting season attribute\n",
    "beginning = 0.15\n",
    "end = 0.85\n",
    "\n",
    "# Iterate over each year\n",
    "#df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "for year in df['Datetime'].dt.year.unique():\n",
    "    \n",
    "    print(\"Calculating\" , year)\n",
    "    \n",
    "    # Get the dataframe for the current year\n",
    "    year_df = df[df['Datetime'].dt.year == year]\n",
    "    \n",
    "    # Determine the threshold values for the beginning and end of the melting season\n",
    "    ablation_df = year_df[year_df['Ablation'] == True]\n",
    "    lower_threshold = ablation_df['Datetime'].dt.dayofyear.quantile(beginning)\n",
    "    upper_threshold = ablation_df['Datetime'].dt.dayofyear.quantile(end)\n",
    "    \n",
    "    # Set the Melting Season value for each row in the current year\n",
    "    for index, row in year_df.iterrows():\n",
    "        \n",
    "        if row['Ablation'] == True:\n",
    "            if row['Datetime'].dayofyear <= lower_threshold:\n",
    "                df.loc[index, 'Melting Season'] = 'beginning'\n",
    "            elif row['Datetime'].dayofyear >= upper_threshold:\n",
    "                df.loc[index, 'Melting Season'] = 'end'\n",
    "            else:\n",
    "                df.loc[index, 'Melting Season'] = 'middle'\n",
    "        else:\n",
    "            df.loc[index, 'Melting Season'] = ''\n",
    "            \n",
    "df.to_csv('data/KAN_L_daily.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
