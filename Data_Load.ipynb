{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Mount Google Drive"
      ],
      "metadata": {
        "id": "-ODOhsaMP7u_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GC Net Data"
      ],
      "metadata": {
        "id": "cLqu3qB_6MQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/GEUS-PROMICE/pyNEAD.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFn7hTObSpj6",
        "outputId": "631d5a59-23cd-4a21-8a31-44578a4ea06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/GEUS-PROMICE/pyNEAD.git\n",
            "  Cloning https://github.com/GEUS-PROMICE/pyNEAD.git to /tmp/pip-req-build-dyn4goe3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/GEUS-PROMICE/pyNEAD.git /tmp/pip-req-build-dyn4goe3\n",
            "  Resolved https://github.com/GEUS-PROMICE/pyNEAD.git to commit d42c1016ab9f5abf2eb0a001168a1201fdaeb20e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from nead==0.0.0) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from nead==0.0.0) (1.3.5)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.8/dist-packages (from nead==0.0.0) (2022.12.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->nead==0.0.0) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->nead==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from xarray->nead==0.0.0) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=21.3->xarray->nead==0.0.0) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->nead==0.0.0) (1.15.0)\n",
            "Building wheels for collected packages: nead\n",
            "  Building wheel for nead (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nead: filename=nead-0.0.0-py2.py3-none-any.whl size=10593 sha256=c3eeb59fe8d6a363a5f5dbbe5d8a4e6914adae4a8c49f156d079b20c79929058\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-89k7td5y/wheels/1e/33/13/ab589de906001bcc80afb596cb13a49ca1b3ba90e94ea6b697\n",
            "Successfully built nead\n",
            "Installing collected packages: nead\n",
            "Successfully installed nead-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env bash\n",
        "\n",
        "# Latest L1 data: https://github.com/GEUS-Glaciology-and-Climate/GC-Net-level-1-data-processing/tree/main/L1\n",
        "# API contents of latest L1 data (raw URLs etc.): https://api.github.com/repositories/319306521/contents/L1\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Master_Thesis')\n",
        "#os.chdir(\"../\")\n",
        "print(os.getcwd())\n",
        "\n",
        "try:\n",
        "    os.mkdir(\"data\")\n",
        "    os.mkdir(\"data/data_daily\")\n",
        "    os.mkdir(\"data/data_hourly\")\n",
        "except:\n",
        "    print('Overwritting existing data in \"/data\"')\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Download data\n",
        "print(\"Downloading daily data...\\r\")\n",
        "\n",
        "\n",
        "# xargs -n 1 curl --silent -O --output-dir data_daily < ../metadata/urls_1.txt\n",
        "for url in open(\"metadata/urls_1.txt\"):\n",
        "    # Split on the rightmost / and take everything on the right side of that\n",
        "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
        "    # Strip /n at the end of filename\n",
        "    name = name.strip()\n",
        "    # Combine the name and the downloads directory to get the local filename\n",
        "    filename = os.path.join(\"data/data_daily\", name)\n",
        "\n",
        "    # Download the file if it does not exist\n",
        "    if not os.path.isfile(filename):\n",
        "        urllib.request.urlretrieve(url, filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba5Hy-lHR2Jr",
        "outputId": "6adc7fce-130e-4d8a-d9df-66dd2c5f9992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Master_Thesis\n",
            "Overwritting existing data in \"/data\"\n",
            "Downloading daily data...\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading hourly data...\\r\")\n",
        "# xargs -n 1 curl --silent -O --output-dir data_hourly < ../metadata/urls_2.txt\n",
        "for url in open(\"metadata/urls_2.txt\"):\n",
        "    # Split on the rightmost / and take everything on the right side of that\n",
        "    name = url.rsplit(\"/\", 1)[-1].replace(\"\\r\", \"\")\n",
        "    # Strip /n at the end of filename\n",
        "    name = name.strip()\n",
        "    # Combine the name and the downloads directory to get the local filename\n",
        "    filename = os.path.join(\"data/data_hourly\", name)\n",
        "\n",
        "    # Download the file if it does not exist\n",
        "    if not os.path.isfile(filename):\n",
        "        urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exKNv5IVbMx-",
        "outputId": "ba7ba835-b0b8-4f37-fa59-380f032d45f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading hourly data...\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data\n",
        "# echo -ne 'Processing dairly data...\\r'\n",
        "# python scripts/process_data_daily.py\n",
        "# echo -ne 'Processing hourly data...\\r'\n",
        "# python scripts/process_data_hourly.py\n",
        "\n",
        "# Delete unprocessed data\n",
        "# rm -r data_daily data_hourly\n",
        "\n",
        "# %% process data daily\n",
        "import pandas as pd\n",
        "import nead\n",
        "\n",
        "# Convert NEAD files to Pandas dataframes\n",
        "station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
        "dfs_daily = []\n",
        "\n",
        "for name, ID in zip(station.Name, station.ID):\n",
        "    format_name = name.replace(\" \", \"\")\n",
        "    files = \"data/data_daily/\" + str(ID).zfill(2) + \"-\" + format_name + \"_daily.csv\"\n",
        "    ds_daily = nead.read(files, index_col=0)\n",
        "    df_daily = ds_daily.to_dataframe()\n",
        "    df_daily.insert(\n",
        "        loc=0, column=\"station_name\", value=name\n",
        "    )  # Add station_name column to each dataframe\n",
        "    dfs_daily.append(df_daily)\n",
        "\n",
        "# Concatenate dataframes\n",
        "df_daily = pd.concat(dfs_daily).sort_index()\n",
        "\n",
        "# Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
        "# null_columns = df_daily.columns[df_daily.isnull().all()]\n",
        "# flag_columns = df_daily.filter(regex=\"flag$\").columns\n",
        "# print(null_columns)\n",
        "# print(flag_columns)\n",
        "\n",
        "df_daily = df_daily.drop(\n",
        "    columns=[\n",
        "        \"OSWR_max\",\n",
        "        \"HW2_adj_flag\",\n",
        "        \"P_adj_flag\",\n",
        "        \"HW1_adj_flag\",\n",
        "        \"OSWR_adj_flag\",\n",
        "        \"HS1_adj_flag\",\n",
        "        \"HS2_adj_flag\",\n",
        "        \"TA3_adj_flag\",\n",
        "        \"TA4_adj_flag\",\n",
        "        \"DW1_adj_flag\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Add year column to dataframe\n",
        "df_daily[\"year\"] = df_daily.index.strftime(\"%Y\")\n",
        "\n",
        "# Add month column to dataframe\n",
        "df_daily[\"month\"] = df_daily.index.strftime(\"%B\")\n",
        "\n",
        "# # Add day column to dataframe\n",
        "# df_daily[\"day\"] = df_daily.index.strftime(\"%d\")\n",
        "\n",
        "# Add hour column to dataframe\n",
        "# df_daily[\"hour\"] = df_daily.index.strftime(\"%h\")\n",
        "\n",
        "# Add season column to dataframe\n",
        "seasons = {\n",
        "    1: \"Winter\",\n",
        "    2: \"Winter\",\n",
        "    3: \"Spring\",\n",
        "    4: \"Spring\",\n",
        "    5: \"Spring\",\n",
        "    6: \"Summer\",\n",
        "    7: \"Summer\",\n",
        "    8: \"Summer\",\n",
        "    9: \"Autumn\",\n",
        "    10: \"Autumn\",\n",
        "    11: \"Autumn\",\n",
        "    12: \"Winter\",\n",
        "}\n",
        "\n",
        "# Extract the month from the index and use the dictionary to map it to the corresponding season\n",
        "df_daily[\"season\"] = df_daily.index.month.map(seasons)\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df_daily.to_parquet(\"data/df_daily.gzip\", compression=\"gzip\")\n",
        "\n",
        "# %% process data hourly\n",
        "# Convert NEAD files to Pandas dataframes\n",
        "station = pd.read_csv(\"metadata/station_info.csv\", header=0)\n",
        "dfs_hourly = []\n",
        "\n",
        "for name, ID in zip(station.Name, station.ID):\n",
        "    format_name = name.replace(\" \", \"\")\n",
        "    files = \"data/data_hourly/\" + str(ID).zfill(2) + \"-\" + format_name + \".csv\"\n",
        "    ds_hourly = nead.read(files, index_col=0)\n",
        "    df_hourly = ds_hourly.to_dataframe()\n",
        "    df_hourly.insert(\n",
        "        loc=0, column=\"station_name\", value=name\n",
        "    )  # Add station_name column to each dataframe\n",
        "    dfs_hourly.append(df_hourly)\n",
        "\n",
        "# Concatenate dataframes\n",
        "df_hourly = pd.concat(dfs_hourly).sort_index()\n",
        "\n",
        "# Delete irrelevant columns from dataframe (i.e. null columns and flag columns)\n",
        "# null_columns = df_hourly.columns[df_hourly.isnull().all()]\n",
        "# flag_columns = df_hourly.filter(regex=\"flag$\").columns\n",
        "# print(null_columns)\n",
        "# print(flag_columns)\n",
        "\n",
        "df_hourly = df_hourly.drop(\n",
        "    columns=[\n",
        "        \"OSWR_max\",\n",
        "        \"HW2_adj_flag\",\n",
        "        \"P_adj_flag\",\n",
        "        \"HW1_adj_flag\",\n",
        "        \"OSWR_adj_flag\",\n",
        "        \"HS1_adj_flag\",\n",
        "        \"HS2_adj_flag\",\n",
        "        \"TA3_adj_flag\",\n",
        "        \"TA4_adj_flag\",\n",
        "        \"DW1_adj_flag\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Add year column to dataframe\n",
        "df_hourly[\"year\"] = df_hourly.index.strftime(\"%Y\")\n",
        "\n",
        "# Add month column to dataframe\n",
        "df_hourly[\"month\"] = df_hourly.index.strftime(\"%B\")\n",
        "\n",
        "# # Add day column to dataframe\n",
        "# df_hourly[\"day\"] = df_hourly.index.strftime(\"%d\")\n",
        "\n",
        "# # Add hour column to dataframe\n",
        "# df_hourly[\"hour\"] = df_hourly.index.strftime(\"%h\")\n",
        "\n",
        "# Add season column to dataframe\n",
        "seasons = {\n",
        "    1: \"Winter\",\n",
        "    2: \"Winter\",\n",
        "    3: \"Spring\",\n",
        "    4: \"Spring\",\n",
        "    5: \"Spring\",\n",
        "    6: \"Summer\",\n",
        "    7: \"Summer\",\n",
        "    8: \"Summer\",\n",
        "    9: \"Autumn\",\n",
        "    10: \"Autumn\",\n",
        "    11: \"Autumn\",\n",
        "    12: \"Winter\",\n",
        "}\n",
        "\n",
        "# Extract the month from the index and use the dictionary to map it to the corresponding season\n",
        "df_hourly[\"season\"] = df_hourly.index.month.map(seasons)\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df_hourly.to_parquet(\"data/df_hourly.gzip\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "QqbmK6FqeRkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promice Data"
      ],
      "metadata": {
        "id": "Zyy-tj5MR42i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkQ-3yLP-37W"
      },
      "outputs": [],
      "source": [
        "!wget -r -e robots=off -nH --cut-dirs=3 --content-disposition \"https://dataverse.geus.dk/api/datasets/:persistentId/dirindex?persistentId=doi:10.22008/FK2/8SS7EW\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save hourly data\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "path = r'/content'  # or unix / linux / mac path\n",
        "\n",
        "# Get the files from the path provided in the OP\n",
        "files = Path(path).glob('*_hour_v03.txt')  # .rglob to get subdirectories\n",
        "\n",
        "dfs = list()\n",
        "for f in files:\n",
        "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
        "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
        "    data['file'] = f.stem\n",
        "    dfs.append(data)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df = df.replace(-999, np.nan)\n",
        "\n",
        "df[\"Month\"] = df[\"MonthOfYear\"]\n",
        "df[\"Day\"] = df[\"DayOfMonth\"]\n",
        "df[\"Hour\"] = df[\"HourOfDay(UTC)\"]\n",
        "\n",
        "df[\"Datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\", \"Hour\"]], format='%Y%m%d%h')\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_hourly.gzip\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "CWscWKUJxkCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save daily data \n",
        "# Get the files from the path provided in the OP\n",
        "files = Path(path).glob('*_day_v03.txt')  # .rglob to get subdirectories\n",
        "\n",
        "dfs = list()\n",
        "for f in files:\n",
        "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
        "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
        "    data['file'] = f.stem\n",
        "    dfs.append(data)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df = df.replace(-999, np.nan)\n",
        "\n",
        "df[\"Month\"] = df[\"MonthOfYear\"]\n",
        "df[\"Day\"] = df[\"DayOfMonth\"]\n",
        "\n",
        "df[\"Datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]], format='%Y%m%d')\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_daily.gzip\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "vwZWPw1I_YPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save monthly data \n",
        "# Get the files from the path provided in the OP\n",
        "files = Path(path).glob('*_month_v03.txt')  # .rglob to get subdirectories\n",
        "\n",
        "dfs = list()\n",
        "for f in files:\n",
        "    data = pd.read_csv(f, delimiter=r\"\\s+\", engine='python')\n",
        "    # .stem is method for pathlib objects to get the filename w/o the extension\n",
        "    data['file'] = f.stem\n",
        "    dfs.append(data)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df = df.replace(-999, np.nan)\n",
        "\n",
        "df[\"Month\"] = df[\"MonthOfYear\"]\n",
        "\n",
        "df[\"Datetime\"] = pd.to_datetime(['{}-{}-01'.format(y, m) for y, m in zip(df.Year, df.Month)])\n",
        "\n",
        "# Save dataframe as parquet file\n",
        "df.to_parquet(\"/content/drive/MyDrive/Master_Thesis/data/promice_monthly.gzip\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "bDnLS8bd_a3y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}